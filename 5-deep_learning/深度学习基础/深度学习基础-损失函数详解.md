- [一，损失函数概述](#一损失函数概述)
- [二，交叉熵函数-分类损失](#二交叉熵函数-分类损失)
  - [2.1，交叉熵（Cross-Entropy）](#21交叉熵cross-entropy)
  - [2.2，二分类问题的交叉熵](#22二分类问题的交叉熵)
  - [2.3，多分类问题的交叉熵](#23多分类问题的交叉熵)
  - [2.4，为什么不能使用均方差做为分类问题的损失函数？](#24为什么不能使用均方差做为分类问题的损失函数)
- [三，均方差函数-回归损失](#三均方差函数-回归损失)
- [参考资料](#参考资料)

## 一，损失函数概述

大多数深度学习算法都会涉及某种形式的优化，**所谓优化指的是改变 $x$ 以最小化或最大化某个函数 $f(x)$ 的任务**，我们通常以最小化 $f(x)$ 指代大多数最优化问题。

在机器学习中，损失函数是代价函数的一部分，而代价函数是目标函数的一种类型。
- **损失函数**（`loss function`）: 用于定义单个训练样本预测值与真实值之间的误差
- **代价函数**（`cost function`）: 用于定义单个批次/整个训练集样本预测值与真实值之间的累计误差。
- **目标函数**（`objective function`）: 泛指任意可以被优化的函数。

**损失函数定义**：损失函数是深度学习模型训练过程中关键的一个组成部分，其通过前言的内容，我们知道深度学习算法优化的第一步首先是确定目标函数形式。

损失函数大致可分为两种：回归损失（针对连续型变量）和分类损失（针对离散型变量）。

常用的减少损失函数的优化算法是“梯度下降法”（Gradient Descent）。

## 二，交叉熵函数-分类损失

### 2.1，交叉熵（Cross-Entropy）
> 交叉熵损失(`Cross-Entropy Loss`) 又称为对数似然损失(Log-likelihood Loss)、对数损失，二分类时还可称之为逻辑斯谛回归损失(Logistic Loss)。

交叉熵（Cross Entropy）是 Shannon 信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。在信息论中，交叉熵是表示两个概率分布 $p,q$ 的差异，其中 $p$ 表示真实分布（目标分布），$p$ 表示预测分布，那么 $H(p, q)$ 就称为交叉熵：

$$
H(p,q) = -\sum_ip(x_i)\log q(x_i)
$$
> 另一种表达公式（公式表达形式虽然不一样，但是意义相同）:
> $$H(P, Q)  = -\mathbb{E}_{\textrm{x}\sim p}log(q(x))$$
> 在本文中，我们总是用 $\text{log}$ 来表示自然对数，**其底数**为 $e$。

交叉熵可在神经网络中作为损失函数，$p$ 表示真实标记（样本标签）的分布，$q$ 则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量 $p$ 与 $q$ 的相似性。

交叉熵函数常用于逻辑回归(`logistic regression`)，也就是分类(`classification`)。

根据信息论中熵的性质，将熵、相对熵（KL 散度）以及交叉熵的公式放到一起:
$$
\begin{align}
H(p) &= -\sum_{i}p(x_i) \log p(x_i) \\
D_{KL}(p \parallel q) &= \sum_{i}p(x_i)\log \frac{p(x_i)}{q(x_i)} = \sum_i (p(x_i)\log p(x_i) - p(x_i) \log q(x_i)) \\
H(p,q) &=  -\sum_ip(x_i)\log q(x_i)
\end{align}
$$

通过上面三个公式就可以得到:

$$
D_{KL}(p||q) = -H(p) + H(p, q)
$$
> 交叉熵也可以定义为熵 $H(P)$ 和 $P$ 和 $Q$之间的 KL 散度的总和。交叉熵 $H(p, q)$ 也记作 $CE(p, q)$、$H(P, Q)$。

在机器学习中，我们需要评估**标签值 $y$ 和预测值 $a$** 之间的差距熵，使用 KL 散度刚刚好，即 $D_{KL}(y||a)$，因为样本标签值的分布通常是固定的，即 $H(a)$ 不变。因此，在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用**交叉熵**做损失函数来评估模型。

$$
loss = \sum_{j = 1}^{n}y_{j}\text{log}(a_{j})
$$

上式是单个样本的情况，$n$ **并不是样本个数，而是分类个数**。所以，对于批量样本的交叉熵损失计算公式是：

$$
J = -\frac{1}{m}\sum_{i=1}^m \sum_{j=1}^n y_{ij} \log a_{ij}
$$

其中，$m$ 是样本数，$n$ 是分类数。

有一类特殊问题，就是事件只有两种情况发生的可能，比如“是狗”和“不是狗”，称为 $0/1$ 分类或**二分类**。对于这类问题，由于 $n=2，y_1=1-y_2，a_1=1-a_2$，所以**二分类问题的单个样本的交叉熵**可以简化为：

$$
loss =-[y \log a + (1-y) \log (1-a)]
$$

**二分类对于批量样本的交叉熵**计算公式是：

$$
J= -\frac{1}{m} \sum_{i=1}^m [y_i \log a_i + (1-y_i) \log (1-a_i)] \tag{4}
$$
> 为什么交叉熵的代价函数是求均值而不是求和?
>  Cross entropy loss is defined as the “expectation” of the probability distribution of a random variable 𝑋, and that’s why we use mean instead of sum. 参见[这里](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html#cross-entropy)。
### 2.2，二分类问题的交叉熵

把二分类的交叉熵公式 4 分解开两种情况：
- 当 $y=1$ 时，即标签值是 $1$ ，是个正例，加号后面的项为: $loss = -\log(a)$
- 当 $y=0$ 时，即标签值是 $0$，是个反例，加号前面的项为 $0$: $loss = -\log (1-a)$

横坐标是预测输出，纵坐标是损失函数值。$y=1$ 意味着当前样本标签值是1，当预测输出越接近1时，损失函数值越小，训练结果越准确。当预测输出越接近0时，损失函数值越大，训练结果越糟糕。此时，损失函数值如下图所示。

![二分类交叉熵损失函数图](./images/loss/二分类交叉熵损失函数图.png)

### 2.3，多分类问题的交叉熵

当标签值不是非0即1的情况时，就是多分类了。

假设希望根据图片动物的轮廓、颜色等特征，来预测动物的类别，有三种可预测类别：猫、狗、猪。假设我们训练了两个分类模型，其预测结果如下:

**模型1**:

|预测值|标签值|是否正确|
|-----|-----|-------|
|0.3 0.3 0.4|0 0 1（猪）|正确|
|0.3 0.4 0.4|0 1 0（狗）|正确|
|0.1 0.2 0.7|1 0 0（猫）|错误|

每行表示不同样本的预测情况，公共 3 个样本。可以看出，模型 1 对于样本 1 和样本 2 以非常微弱的优势判断正确，对于样本 3 的判断则彻底错误。

**模型2**:

|预测值|标签值|是否正确|
|-----|-----|-------|
|0.1 0.2 0.7|0 0 1（猪）|正确|
|0.1 0.7 0.2|0 1 0（狗）|正确|
|0.3 0.4 0.4|1 0 0（猫）|错误|

可以看出，模型 2 对于样本 1 和样本 2 判断非常准确（预测概率值更趋近于 1），对于样本 3 虽然判断错误，但是相对来说没有错得太离谱（预测概率值远小于 1）。

结合多分类的交叉熵损失函数公式可得，模型 1 的交叉熵为:

$$\text{sample}\ 1\ \text{loss} = -(0\times log(0.3) + 0\times log(0.3) + 1\times log(0.4) = 0.91 \\
\text{sample}\ 1\ \text{loss} = -(0\times log(0.3) + 1\times log(0.4) + 0\times log(0.4) = 0.91 \\
\text{sample}\ 1\ \text{loss} = -(1\times log(0.1) + 0\times log(0.2) + 0\times log(0.7) = 2.30$$

对所有样本的 `loss` 求平均:

$$
L = \frac{0.91 + 0.91 + 2.3}{3} = 1.37
$$

模型 2 的交叉熵为:

$$\text{sample}\ 1\ \text{loss} = -(0\times log(0.1) + 0\times log(0.2) + 1\times log(0.7) = 0.35 \\
\text{sample}\ 1\ \text{loss} = -(0\times log(0.1) + 1\times log(0.7) + 0\times log(0.2) = 0.35 \\
\text{sample}\ 1\ \text{loss} = -(1\times log(0.3) + 0\times log(0.4) + 0\times log(0.4) = 1.20$$

对所有样本的 `loss` 求平均:

$$
L = \frac{0.35 + 0.35 + 1.2}{3} = 0.63
$$

可以看到，0.63 比 1.37 的损失值小很多，这说明预测值越接近真实标签值，即交叉熵损失函数可以较好的捕捉到模型 1 和模型 2 预测效果的差异。**交叉熵损失函数值越小，反向传播的力度越小**。
> 多分类问题计算交叉熵的实例来源于知乎文章-[损失函数｜交叉熵损失函数](https://zhuanlan.zhihu.com/p/35709485)。

### 2.4，为什么不能使用均方差做为分类问题的损失函数？

回归问题通常用均方差损失函数，可以保证损失函数是个凸函数，即可以得到最优解。而分类问题如果用均方差的话，损失函数的表现不是凸函数，就很难得到最优解。而交叉熵函数可以保证区间内单调。

分类问题的最后一层网络，需要分类函数，`Sigmoid` 或者 `Softmax`，如果再接均方差函数的话，其求导结果复杂，运算量比较大。用交叉熵函数的话，可以得到比较简单的计算结果，一个简单的减法就可以得到反向误差。

## 三，均方差函数-回归损失

首先介绍一个回归问题的基本概念: **残差**或称为**预测误差**，用于衡量模型预测值与真实标记的靠近程度。假设回归问题中对应于第 $i$ 个输入特征 $x_i$ 的真实标记为 $y^i = (y_1,y_2,...,y_M)^{\top}$，$M$ 为标记向量总维度，则 $l_{t}^{i}$ 即表示样本 $i$ 上神经网络的回归预测值 ($y^i$)与其真实标记在第 $t$ 维的预测误差(亦称残差):

$$
l_{t}^{i} = y_{t}^{i} - \hat{y}_{t}^{i}
$$

## 参考资料

1. [《动手学深度学习-22.11. Information Theory》](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html#cross-entropy)
2. [损失函数｜交叉熵损失函数](https://zhuanlan.zhihu.com/p/35709485)
3. [AI-EDU: 交叉熵损失函数](https://microsoft.github.io/ai-edu/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/A2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/%E7%AC%AC1%E6%AD%A5%20-%20%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/03.2-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html)