{"./":{"url":"./","title":"Introduction","keywords":"","body":" CV 算法工程师成长之路 前言 目录 学习路线 面试题 可投递公司 我的公众号前言 本项目最初是当作 cv 算法工程师实习内推表、校招可投递公司汇总以及个人面经的汇总，后面逐步转变为个人 cv 算法工程师成长之路所记录的技术栈笔记、以及少部分面经等内容。 项目部分内容参考自 github 项目/网络博客/书籍和 个人博客 等，由于时间和精力有限，有些知识点还没有没有完成，请见谅。 GitHub 已经支持直接显示 latex 公式，部分公式如果显示不全，也可在谷歌浏览器安装 MathJax Plugin for Github 插件访问(需要翻墙下载安装)，或者下载仓库到本地，使用 Typora 软件阅读，也可以使用安装了 Markdown+Math 插件的 VSCode 软件阅读。 目录 作为一个计算机视觉算法工程师，需要掌握的不仅是计算机编程知识，还需要掌握编程开发、机器学习/深度学习、图像识别/目标检测/语义分割、模型压缩、模型部署等知识点，我整理了一个 技术栈思维导图。 强调一下如何从“零”起步，首先确保基础打好。建议完整修完一门国外经典课程（从课程视频、作业到项目），然后完整阅读一本机器学习或者深度学习教科书，同时熟练掌握一门基本的编程语言以及深度学习框架。（参考 中国人民大学赵鑫：AI 科研入坑指南） 计算机基础 编程语言 数据结构与算法 机器学习 深度学习 计算机视觉 模型压缩与量化 高性能计算 模型部署 图像算法岗面经 学习路线 cv算法工程师学习成长路线 面试题 深度学习基础的一些面试题和 Python 编程相关的面试题，部分题目来源网上资料。 深度学习面试题 Python3 编程面试题 可投递公司 鉴于 2019 年写的春招算法实习岗位表绝大部分已经失效，本人也再无精力维护，故将其移除，故不在展示在仓库主页上。 虽然算法工程师可投递的公司是较多的，但是岗位提供的 hc 是不及开发多的，这点需要注意。以下表格侧重于计算机视觉算法和算法优化/部署工程师岗位。 top级公司 互联网公司 AI独角兽公司 其他大公司 百度 美团 地平线机器人 顺丰科技 阿里巴巴 滴滴出行 图森未来 招银网络科技 腾讯 拼多多/菜鸟网络 momenta 平安科技 字节跳动 京东 小马智行 cvte 微软 网易 蔚来汽车 海康威视 谷歌 快手 小鹏汽车 虹软科技 商汤 爱奇艺 科大讯飞 传音手机 英伟达 小米 寒武纪/依图 大华 博世 陌陌 旷视 荣耀手机 大疆无人机 美图MTlab 文行知远 联想 蚂蚁金服 360安全 云天励飞 汇顶科技 Intel/亚马逊 搜狗 摩尔线程 美的中央研究院 华为 猿辅导 思必驰 锐明技术 无 新浪/搜狐/金山 奥比中光 联发科 无 YY/虎牙/BIGO/斗鱼 优必选 联影医疗 无 oppo/vivo/一加 度小满金融 戴尔 无 贝壳找房 深睿医疗 TP-LINK 无 携程/去哪儿/途家 镁佳科技 ZOOM 无 瓜子二手车 猎豹移动 广联达 无 作业帮/VIPKID/好未来 京东数科 深信服 无 阅文集团/58集团 追一科技 中国电信云计算 无 B站 深兰科技 三星电子研究所 无 小红书/英语流利说 明略科技 苏宁 无 趣头条/一点资讯 数美科技 微众银行 无 知乎 驭势科技 中国移动成研院 无 蘑菇街 随手科技 远景智能 无 转转 智加科技 牧原智能科技 无 同花顺/老虎证券 壁仞科技 便利蜂 无 乐信/有赞 趋势科技 中兴 无 金蝶软件(中国) 云从科技 航天二院706所 无 汽车之家 第四范式 吉利汽车 无 珍爱网/酷狗音乐 黑芝麻智能 碧桂园机器人 无 巨人网络/盛大游戏 格灵深瞳 华米/极米 无 无 最右/快看漫画 码隆科技 无 无 猫眼娱乐/多牛传媒 轻舟智航 无 无 我的公众号 更多知识和最新博客，欢迎扫码关注我的公众号-嵌入式视觉，记录 CV 算法工程师成长之路，分享技术总结、读书笔记和个人感悟。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"cv算法工程师成长路线.html":{"url":"cv算法工程师成长路线.html","title":"cv算法工程师成长路线","keywords":"","body":" 前言 一，计算机系统 1.1，计算机系统书籍 1.2，设计模式教程 二，编程语言 2.1，C++ 学习资料 2.2，Python 学习资料 三，数据结构与算法 3.1，数据结构与算法课程 3.2，算法题解 四，机器学习 4.1，机器学习课程 五，深度学习 5.1，深度学习课程 5.2，深度学习基础文章 5.3，经典CNN分析文章 5.4，PyTorch 框架学习文章 5.5，PyTorch/Caffe 框架分析文章 六，计算机视觉 6.1，数字图像处理教程 6.2，计算机视觉基础课程 6.3，深度学习模型和资源库 6.4，目标检测网络文章 6.5，语义分割文章 6.6，3D 视觉技术文章 6.7，深度学习的评价指标文章 七，模型压缩与量化 7.1，轻量级网络设计 7.2，模型压缩文章 7.3，神经网络量化文章 7.4，推理框架剖析文章 八，高性能计算 8.1，CPU/GPU/AI 芯片科普 8.2，指令集(ISA)学习资料 8.3，矩阵乘优化文章 九，模型部署(算法SDK开发) 9.1，模型部署文章 效率工具 markdown/latex 写作 博客阅读后的知识点总结 参考资料 文章同步发于 github 仓库 和 csdn 博客，最新版以 github 为主。如果看完文章有所收获，一定要先点赞后收藏。毕竟，赠人玫瑰，手有余香. 本文内容为 cv 算法工程师成长子路上的经典学习教材汇总，对于一些新兴领域则给出了较好的博客文章链接。本文列出的知识点目录是成系统且由浅至深的，可作为 cv 算法工程师的常备学习路线资料。 文章所涉知识点和参考资料内容很多也很广，建议先看目录，心中有个大概知识点结构，后面由浅入深，慢慢学习各个知识点，由浅入深，切忌浮躁。 部分学习资料存在离线 PDF 电子版，其可在 github仓库-cv_books 中下载。如果仓库失效，可以关注我的公众号-嵌入式视觉，后台回复对应关键字下载高清 PDF 电子书。 前言 课程学习方法，三句话总结： 看授课视频形成概念，发现个人感兴趣方向。 读课程笔记理解细节，夯实工程实现的基础。 码课程作业实现算法，积累实验技巧与经验。 再引用一下学习金字塔的图： 图片来源 github 仓库 DeepLearning Tutorial 关于科研和研发的思考，可参考文章-中国人民大学赵鑫：AI 科研入坑指南。 一，计算机系统 1.1，计算机系统书籍 《深入理解计算机系统第三版》： 网上有电子版，PDF 电子书下载方式在文章首页。 1.2，设计模式教程 设计模式： 内容很全，存在 C++ 示例代码。 二，编程语言 2.1，C++ 学习资料 cpp reference: C++ 库接口参考标准文档，官方文档，包含各个函数定义及使用 example。 http://www.cplusplus.com/reference/stl/ Cpp Primer 学习: 《C++ Primer 中文版（第 5 版）》学习仓库，包括笔记和课后练习答案。 C++ Tips of the Week: 谷歌出品的 C++ 编程技巧。 2.2，Python 学习资料 《廖雪峰-Python3教程》: 内容很全且通俗易懂，适合初学者，但代码示例不够丰富。描述的知识点有：Python 基础、函数、高级特性、函数式编程、模块、面向对象编程、面向对象高级编程、错误、调试和测试、IO 编程、进程和线程、正则表达式、常用内建模块、常用第三方模块、图形界面、网络编程、异步IO 等内容。电子书可在github仓库-cv_books 中下载。 Python 工匠系列文章: 很适合深入理解 Python 面向对象编程、装饰器、模块、异常处理等内容。三，数据结构与算法 3.1，数据结构与算法课程 《图解算法》：存在 PDF 电子版，内容较为基础且通俗易懂，适合快速了解数据结构与算法的基础知识，但深度不够，示例代码为 Python。 专栏-数据结构与算法之美: 学习数据结构与算法的知识点课程，内容全且深度足够。官方例子为 java 代码，同时 github 仓库提供 C/C++/GO/Python 等代码。 3.2，算法题解 《剑指Offer》面试题: Python实现: 题目为《剑指Offer》书籍原题，代码实现为 Python，仓库简洁，阅读体验不错，无任何广告，适合刚学完数据结构与算法基础知识的同学。 力扣++-算法图解: leetcode 高频题图解，题解分析很多，部分题目有动画分析，提供 Python/Java/C++ 实现，但也存在部分题解分析废话较多，不够精简的问题。 小浩算法: 一部图解算法题典，讲解 105 道高频面试算法题目，go 代码实现。 LeetCode题解: leetcode 高频题题解，全书代码默认使用 C++11 语法编写，题解为文字性描述，题解分析较短且不够通俗易懂。本书的目标读者是准备去硅谷找工作的码农，也适用于在国内找工作的码农，以及刚接触 ACM 算法竞赛的新手。 四，机器学习 4.1，机器学习课程 《机器学习》-周志华（西瓜书）：存在 PDF 电子版，内容很全，很适合打下扎实的基础。 《李宏毅-机器学习课程》: 机器学习经典视频教程啊，非常适合初学者观看。 李宏毅机器学习笔记(LeeML-Notes): 可以在线阅读，很方便，内容完成度高。 《南瓜书PumpkinBook》: 南瓜书，是西瓜书的补充资料，包含了西瓜书的公式的详细推导，建议先看西瓜书，部分公式不会推导的情况下，可以查看南瓜书的对应内容。 机器学习数学基础: 黄海广博士翻译的 CS229 机器学习课程的线性代数基础材料，英文好的建议看原版。 五，深度学习 想要快速入门神经网络（深度学习）或者重新复习基础的同学，推荐看这个文章合集Neural Networks From Scratch。文章内容由浅入深，既有公式推导，也有对应代码实现。 5.1，深度学习课程 《深度学习》（花书），存在英文和中文 PDF 电子版，内容成系统，覆盖了深度学习的方方面面，强烈建议至少看完跟自己方向相关的章节，有利于打好扎实的基础。 《李宏毅-深度学习课程》： 经典视频教程，实例有趣（皮卡丘），内容讲解由浅至深，李宏毅老师个人官网也提供了视频链接、 PPT 课件、代码资料。 5.2，深度学习基础文章 CNN中参数解释及计算 深度学习推理时融合BN，轻松获得约5%的提速 动图形象理解深度学习卷积 5.3，经典CNN分析文章 深度可分离卷积（Xception 与 MobileNet 的点滴） [DL-架构-ResNet系] 002 ResNet-v2 ResNet及其变种的结构梳理、有效性分析与代码解读 1，VGGNet 拥有 5 段 卷积，每一段有 2~3 个卷积层，同时每段尾部会连接一个最大池化层用来缩小图片尺寸，每段内的卷积核数量相同，越靠后的段的卷积核数量越多：64-128-256-512-512。ResNet 网络拥有 4 段卷积， 每段卷积代表一个 残差学习 Blocks，根据网络层数的不同， Blocks 的单元数量不同，例如 ResNet18 的 Blocks 单元数量分别为2、2、2 和 2。越靠后的段的卷积核数量越多：64-128-256-512，残差学习 Blocks 内的卷积核通道数是相同的。 2，ResNet v2 创新点在于通过理论分析和实验证明恒等映射对于残差块的重要性，根据激活函数与相加操作的位置关系，我们称之前的组合方式（ResNet）为“后激活（post-activation）”，现在新的组合方式（ResNet v2）称之为“预激活（pre-activation）”。使用预激活有两个方面的优点：1)f 变为恒等映射，使得网络更易于优化；2)使用 BN 作为预激活可以加强对模型的正则化。 5.4，PyTorch 框架学习文章 PyTorch中文文档；PyTorch官方教程中文版；PyTorch 官方教程。 PyTorchtutorial_0.0.5余霆嵩: 存在开源 PDF 电子版，且提供较为清晰的代码，适合快速入门，教程目录结构清晰明了。 5.5，PyTorch/Caffe 框架分析文章 pytorch自定义层如何实现？超简单！ 【PyTorch】torch.nn.Module 源码分析 详解Pytorch中的网络构造，模型save和load，.pth权重文件解析 半小时学会 PyTorch Hook 详解Pytorch中的网络构造 深度学习与Pytorch入门实战（九）卷积神经网络&Batch Norm Pytorch 里 nn.AdaptiveAvgPool2d(output_size) 原理是什么? caffe源码解析-开篇 《Caffe官方教程中译本》：存在开源 PDF 电子版。六，计算机视觉 6.1，数字图像处理教程 《数字图像处理第四版》：存在开源 PDF 电子版。成系统的介绍了数字图像的原理及应用，内容多且全、深度也足够，非常适合深入理解数学图像原理，可挑重点看。 桔子code-OpenCV-Python教程 6.2，计算机视觉基础课程 《CS231 课程》-李飞飞。b 站视频教程；CS231n官方笔记授权翻译总集。课程非常经典，内容深入浅出，每节课都有课后作业和对应学习笔记。 《动手学深度学习》-李沐，存在开源 PDF 电子书，官方代码为 MXNet 框架实现，github 上有开源的《动⼿手学深度学习 PYTORCH 版》。 《解析卷积神经网络-深度学习实践手册》-魏秀参：对 CNN 对基础部件做了深入描述，本书内容全且成系统，适合想深入学习 CNN 的同学，唯一的缺点没有项目案例以供实践。本书提供开源 PDF 电子版。 6.3，深度学习模型和资源库 Papers With Code Jetson Zoo ModelZOO MediaPipe 框架 Deci's Hardware Aware Model Papers with code 是由 Meta AI Research 团队主导的一个开放资源的社区，汇集了深度学习论文、数据集、算法代码、模型以及评估表。 Jetson Zoo，是一个开源目录，其中包含在 NVIDIA Jetson 硬件平台上开发指南以及参考案例分享汇总。模型库资源里包括图像分类、目标检测、语义分割和姿势估计等方向的实践分享，提供开源代码和开发指南文章的链接。 Model Zoo 包含了机器学习各领域的算法框架及预训练模型资源汇总，其中包括 TensorFlow、PyTorch、Keras、Caffe等框架，作者是 Google 的机器学习研究员的Jing Yu Koh构建。 MediaPipe 是一个为直播和流媒体提供跨平台、可定制的机器学习解决方案的框架。MediaPipe 提供了包括人脸检测、人脸网格、虹膜识别、手部关键点检测、人体姿态估计、人体+人脸+手部组合整体、头发分割、目标检测、Box 跟踪、即时运动追踪、3D 目标检测等解决方案。 Deci 旨在使用 AI 构建更好的 AI，使深度学习能够发挥其真正的潜力。借助该公司的端到端深度学习加速平台，人工智能开发人员可以为任何环境（包括云、边缘或移动）构建、优化和部署更快、更准确的模型。借助 Deci 的平台，开发人员可以在任何硬件上将深度学习模型推理性能提高 3 到 15 倍，同时仍然保持准确性。平台除了能够显示每个模型的准确性之外，还可以轻松选择目标推理硬件并查看模型的运行时性能结果，例如各种硬件的吞吐量、延迟、模型大小和内存占用。但是模型加速模块的 demo 是需要注册账户和购买的。 6.4，目标检测网络文章 一文读懂Faster RCNN 从编程实现角度学习Faster R-CNN（附极简实现） Mask RCNN学习笔记 Mask RCNN 源代码解析 (1) - 整体思路 物体检测之Focal Loss及RetinaNet CVPR18 Detection文章选介（下） 2020首届海洋目标智能感知国际挑战赛 冠军方案分享 目标检测中的样本不平衡处理方法——OHEM, Focal Loss, GHM, PISA 6.5，语义分割文章 2019年最新基于深度学习的语义分割技术讲解 U-Net 论文笔记 6.6，3D 视觉技术文章 3D成像方法 汇总（原理解析）--- 双目视觉、激光三角、结构光、ToF、光场、全息 关于双目立体视觉的三大基本算法及发展现状的总结 3D视觉CV界的终极体现形式，计算机如何「看」这个三维世界 6.7，深度学习的评价指标文章 ROC和AUC介绍以及如何计算AUC COCO目标检测测评指标 如何评测AI系统？ PLASTER：一个与深度学习性能有关的框架 The Correct Way to Measure Inference Time of Deep Neural Networks 七，模型压缩与量化 7.1，轻量级网络设计 轻量卷积神经网络的设计 网络结构碎片化更多是指网络中的多路径连接，类似于 short-cut，bottle neck 等不同层特征融合，还有如 FPN 等结构。拖慢并行的一个很主要因素是，运算快的模块总是要等待运算慢的模块执行完毕。 ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design ShufflenetV2_高效网络的4条实用准则 轻量级神经网络：ShuffleNetV2解读 7.2，模型压缩文章 解读模型压缩3：高效模型设计的自动机器学习流水线 Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding 韩松Deep compression论文讲解——PPT加说明文字 论文总结 - 模型剪枝 Model Pruning 编译器与IR的思考: LLVM IR，SPIR-V到MLIR 7.3，神经网络量化文章 神经网络量化简介 线性量化 Int8量化-介绍（一） Int8量化-ncnn社区Int8重构之路（三） ncnn源码学习（六）：模型量化原理笔记 神经网络推理加速之模型量化 NNIE 量化感知训练 1，量化是指用于执行计算并以低于浮点精度的位宽存储张量的技术，或者说量化就是将神经网络的浮点算法转换为定点。 量化模型对张量使用整数而不是浮点值执行部分或全部运算。 2，量化简单来说就是将浮点存储（运算）转换为整型存储（运算）的一种模型压缩技术。 3，虽然精心设计的 MobileNet 能在保持较小的体积时仍然具有与 GoogleNet 相当的准确度，不同大小的 MobileNet 本身就表明——也许一个好的模型设计可以改进准确度，但同类模型中仍然是更大的网络，更好的效果! 4，权重值域调整是另一个机器学习过程，学习的目标是一对能在量化后更准确地运行网络的超参数 min/max。 7.4，推理框架剖析文章 优化 TensorFlow Lite 推理运行环境内存占用 ncnn源码解析（五）：执行器Extractor 八，高性能计算 8.1，CPU/GPU/AI 芯片科普 一文读懂 GPU 的发展历程 CPU、GPU、NPU等芯片架构、特点研究 什么是异构并行计算？CPU与GPU的区别是什么？ 看懂芯片原来这么简单（二）：AI为什么聪明？什么是华为自研架构NPU？ 【专利解密】如何提高AI资源利用率？ 华为卷积运算芯片 嵌入式系统 内存模块设计 8.2，指令集(ISA)学习资料 Intel® Intrinsics Guide Neon Intrinsics Reference ARM Neon Intrinsics 学习指北：从入门、进阶到学个通透 Neon 是 ARM 平台的向量化计算指令集，通过一条指令完成多个数据的运算达到加速的目的，或者说 Neon 是 ARM 平台的 SIMD（Single Instruction Multiple Data，单指令多数据流）指令集实现。常用于AI、多媒体等计算密集型任务。 8.3，矩阵乘优化文章 移动端arm cpu优化学习笔记----一步步优化盒子滤波（Box Filter） OpenBLAS gemm从零入门 通用矩阵乘（GEMM）优化算法 卷积神经网络中的Winograd快速卷积算法 知乎专栏-深入浅出GPU优化 CUDA GEMM 理论性能分析与 kernel 优化 OpenPPL 中的卷积优化技巧：概述总结类文章，无代码，非专注时刻也能阅读。 【张先轶】BLISlab学习优化矩阵乘。第一课 矩阵乘法与 SIMD Winograd 是一种快速卷积算法，适用于小卷积核，可以减少浮点乘法的次数。 九，模型部署(算法SDK开发) 9.1，模型部署文章 海思AI芯片(Hi3519A/3559A)方案学习（二十五）初识 mapper_quant 和mapper_param 部署PyTorch模型到终端 多场景适配，TNN如何优化模型部署的存储与计算 模型转换、模型压缩、模型加速工具汇总 深度学习模型转换与部署那些事(含ONNX格式详细分析) ONNX初探 效率工具 markdown/latex 写作 markdown语法大全: 这篇文章对 markdown 语法整理得很好，文章排版也做的好，读完很容易就掌握 markdown 语法。 通用 LaTeX 数学公式语法手册: 文章排版很好，目录结构清晰明了，阅读起来很舒服，推荐用来学习 latex 语法内容。 https://latex.codecogs.com/eqneditor/editor.php: 在线 latex 语法。 博客阅读后的知识点总结 1，为了尽可能地提高 MAC阵列 的利用率以及卷积运算效率，阵列控制模块会根据第一卷积参数矩阵的行数和第一卷积数据阵列的行数来确定第一乘法累加窗口的列数。 2，SNPE 开发流程： 3，目标检测模型效果提升方法： 以 Cascade RCNN 作为 baseline，以 Res2Net101 作为 Backbone； Albumentation 库做数据集增强-用在模型训练中； 多尺度训练(MST Multi-scale training/testing)的升级版-SNIP方法(Scale Normalization for Image Pyramids)，用在 baseline 模型训练和测试中：解决模板大小尺度不一的问题； DCN 可变性卷积网络-用在 baseline 模型的 backone 中； soft-NMS：解决目标互相重叠的问题； HTC 模型预训练， Adam 优化算法可以较好的适应陌生数据集，学习率热身(warm-up)来稳定训练过程。 4，SNIP 论文解读： SNIP 非常 solid 地证明了就算是数据相对充足的情况下，CNN 仍然很难使用所有 scale 的物体。个人猜测是由于 CNN 中没有对于 scale invariant 的结构，CNN 能检测不同 scale 的“假象”，更多是通过CNN 来通过 capacity 来强行 memorize 不同 scale 的物体来达到的，这其实浪费了大量的 capacity，而 SNIP 这样只学习同样的 scale 可以保障有限的 capacity 用于学习语义信息。论文的关键贡献：发现现在的 CNN 网络无法很好的解决 scale invariance 的问题，提出了一个治标不治本的方法。 5，高效模型设计（模型压缩）方法： 一般而言，高效模型的设计有 6 大基本思路：1）轻量级架构、2）模型裁剪、3）AutoML 和 NAS 模型搜索、4）低精度量化、5）知识蒸馏、6）高效实现。 来源旷世学术分享-张祥雨：高效轻量级深度模型的研究和实践。 6，网络深度与宽度的理解及意义 更多理解参考知乎网络宽度对深度学习模型性能有什么影响？ 在一定的程度上，网络越深越宽，性能越好。宽度，即通道(channel)的数量，网络深度，及 layer 的层数，如 resnet18 有 18 层网络。注意我们这里说的和宽度学习一类的模型没有关系，而是特指深度卷积神经网络的(通道)宽度。 网络深度的意义：CNN 的网络层能够对输入图像数据进行逐层抽象，比如第一层学习到了图像边缘特征，第二层学习到了简单形状特征，第三层学习到了目标形状的特征，网络深度增加也提高了模型的抽象能力。 网络宽度的意义：网络的宽度（通道数）代表了滤波器（3 维）的数量，滤波器越多，对目标特征的提取能力越强，即让每一层网络学习到更加丰富的特征，比如不同方向、不同频率的纹理特征等。 7，所有 Inception 模型都具有一个重要的性质——都是遵循 拆分-变换-合并（split-transform-merge） 的设计策略。 8，对于某种指令，延迟 latency 主要关注单条该指令的最小执行时间，吞吐量 throughout 主要关注单位时间内系统（一个CPU核）最多执行多少条该指令。因为 AI 计算的数据量比较大，所以更关注吞吐量。 9，CPU 高性能通用优化方法包括： 编译选项优化 内存性能和耗电优化：内存复用原则，小块快跑是内存设计的重要原则。 循环展开：循环的每次迭代都有一定的性能损失（分支指令）。但是现代 ARM 处理器具有分支预测的能力，它可以在执行条件之前预测是否将进入分支，从而降低性能损耗，这种情况下全部循环展开的的优势就减弱了。 并行优化和流水线重排：并行优化分为多线程核与核之间数据处理，以及单核心内部并行处理。从本质上讲，流水线重排也是一种并行优化。 10，卷积性能优化方式：卷积的计算方式有很多种，通用矩阵运算（GEMM）方式有良好的通用性，但是仅使用 GEMM 无法实现性能最优。除 GEMM 外，常用的优化方法还包括滑窗（Sliding Window）、快速傅里叶变换（Fast Fourier Transform, FFT）、Winograd 等。不同的方法适合不同的输入输出场景，最佳的办法就是对算子加入逻辑判断，将不同大小的输入分别导向不同的计算方法，以最合适的方法进行卷积计算。 大多数情况下，使用滑窗方法的计算性能还是无法和 GEMM 方法比较，但是一般当输入小于 $32\\times 32$ 时，可以考虑采用滑窗的优化方式。 Winograd 是存在已久的性能优化算法，在大多数场景中，Winograd 算法都显示了较大的优势，其用更多的加法运算代替部分乘法运算，因为乘法运算耗时远高于加法运算。Winograd 适用于乘法计算消耗的时钟周期数大于加法运算消耗的时钟周期数的场景，且常用于 $3\\times 3$ 卷积计算中。对于 CPU，一般来说，一次乘法计算消耗的时间是一次加法计算消耗时间的 6 倍。 FFT 方法不适合卷积核较小的 CNN 模型。 11，下图展示了如何在英伟达 GPU 架构发展史以及单块 GPU 上纵向扩展以满足深度学习的需求（截止2020年）。 12，Deep compression 论文阅读总结 deep compression 是解决存储问题，对于速度问题几乎没获得改善； 权值剪枝还得看另外一篇论文：learning both weights and connection for efficient neural network CNN 模型的存储空间问题，主要还是在全连接层，若要改善 inference 速度，需要在卷积层下功夫。 13，Deep Compression 论文介绍的神经网络压缩方法，可分为三步： 剪枝：舍弃权重绝对值较小的权重，并将剩余权重以稀疏矩阵表示。 量化：将剪枝结果进行进一步量化，具体的是构建一组权值码本，使模型中的权值共享码本中的其中一个权重值，以减少每个权重保存所需的比特数。 霍夫曼编码（可选）：通过霍夫曼编码，进一步地压缩索引值以及权重数值地存储空间。 参考资料 DeepLearning Tutorial Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/":{"url":"1-computer_basics/","title":"1. 计算机基础","keywords":"","body":"前言 本目录内容旨在分享常用编程开发效率工具、操作系统、Linux 系统等方向知识总结和笔记。 效率工具 git命令学习笔记 ubuntu16.04安装mmdetection库 Docker基础和常用命令 计算机系统 深入理解计算机系统-第1章计算机系统漫游笔记 深入理解计算机系统-第2章信息的表示和处理 深入理解计算机系统-第3章程序的机器级表示 Linux 系统 Linux 基础 Linux 基础-学会使用命令帮助 Linux 基础-新手必备命令 Linux 基础-文件权限与属性 Linux 基础-文件及目录管理 Linux 基础-文本处理命令 Linux 基础-查看cpu、内存和环境等信息 Linux 基础-查看进程命令ps和top Linux 基础-查看和设置环境变量 Linux 进阶 [ ] 正则表达式与文件格式处理 [ ] shell scripts 学习 参考资料 《鸟哥私房菜-基础篇》 《深入理解计算机系统第三版》 Docker-从入门到实践 Docker教程 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/效率工具/Docker基础和常用命令.html":{"url":"1-computer_basics/效率工具/Docker基础和常用命令.html","title":"Docker基础和常用命令","keywords":"","body":" 一，Docker 简介 1.1，什么是 Docker 1.2，Docker 与虚拟机的区别 1.3，Docker 架构 1.4，为什么用 Docker 二，Docker 基本概念 2.1，镜像 2.2，容器 2.3，仓库 三，Docker 使用 3.1，Docker 服务 3.2，下载与使用Docker公共镜像(Images) 四，Docker 镜像命令 五，Docker 容器命令 5.1，docker run 命令 六，参考资料 一，Docker 简介 1.1，什么是 Docker Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 OverlayFS 类的 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。Docker容器与虚拟机类似，但二者在原理上不同。容器是将操作系统层虚拟化，虚拟机则是虚拟化硬件，因此容器更具有便携性、能更高效地利用服务器。 专业名词 Docker 有两个意思： 代指整个 Docker 项目。 代指 Docker 引擎。 Docker 引擎(Docker Engine)是指一个服务端-客户端结构的应用，主要有这些部分：Docker 守护进程、Docker Engine API（页面存档备份，存于互联网档案馆）、Docker 客户端。 1.2，Docker 与虚拟机的区别 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程。 Docker 容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 特性 Docker 虚拟机 启动 秒级 分钟级 硬盘使用 一般为 MB 一般为 GB 性能 接近原生 弱于 系统支持量 单机支持上千个容器 一般几十个 1.3，Docker 架构 runc 是一个 Linux 命令行工具，用于根据 OCI 容器运行时规范 创建和运行容器。 containerd 是一个守护程序，它管理容器生命周期，提供了在一个节点上执行容器和管理镜像的最小功能集。 1.4，为什么用 Docker Docker 作为一种新的虚拟化技术，跟传统的虚拟化技术相比具有众多的优势： 更高效的利用系统资源：不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker 对系统资源的利用率更高。 更快速的启动时间：Docker 容器应用直接运行于宿主内核，不需要启动完整的操作系统，所以启动时间可做到秒级的启动时间。 一致的运行环境：Docker 镜像提供了除内核外完整的运行时环境，确保开发环境、测试环境、生产环境的一致性。 持续交付和部署：开发人员可以通过 Dockerfile 来进行镜像构建，并结合持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合持续部署(Continuous Delivery/Deployment) 系统进行自动部署。 更轻松的迁移：Docker 可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的。 更轻松的维护和扩展。 二，Docker 基本概念 Docker 三个基本概念： 镜像（Image） 容器（Container） 仓库（Repository） 2.1，镜像 操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统。比如官方镜像 ubuntu:18.04 就包含了完整的一套 Ubuntu 18.04 最小系统的 root 文件系统。 Docker 镜像 是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像 不包含 任何动态数据，其内容在构建之后也不会被改变。 Docker 镜像并非是像一个 ISO 那样的打包文件，镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。其被设计为分层存储的架构，镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 2.2，容器 镜像（Image）和容器（Container）的关系，类似面向对象程序设计中的类和实例的关系。可以把 Docker容器(Container) 看做是一个简易版的 Linux 环境（包括 root 用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。它可以被启动、开始、停止、 删除。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。 容器和镜像一样都是使用分层存储，每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。 2.3，仓库 镜像构建完成后，可以很容器的在当前宿主主机上运行，但是如果需要在其他服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，即仓库（Repository）-集中存放镜像的地方。 Docker 仓库(Registry) 分为公开仓库（Public）和私有仓库（Private）两种形式。目前 Docker 官方维护了一个公共仓库 Docker Hub，其中已经包括了数量超过 2,650,000 的镜像。大部分需求都可以通过在 Docker Hub 中直接下载镜像来实现。 有时候使用 Docker Hub 这样的公共仓库可能不方便，用户可以创建一个本地仓库供私人使用。Docker Registry 是官方提供的工具，可以用于构建私有的镜像仓库。 一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 : 的格式来指定具体是这个软件哪个版本的镜像。如：ubuntu: 14.04、ubuntu: 16.04 等等。 $ docker image ls ubuntu REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 18.04 329ed837d508 3 days ago 63.3MB ubuntu bionic 329ed837d508 3 days ago 63.3MB 三，Docker 使用 3.1，Docker 服务 安装 Docker 这里不做介绍。以下是 Linux 系统下，一些 docker 使用命令： 1，查看 Docker 服务状态：使用 systemctl status docker 命令查看 Docker 服务的状态。其中 Active: active (running) 即表示 Docker 服务为正在运行状态。 2，停止 Docker 服务：使用 systemctl stop docker 命令。 3，启动 Docker 服务：使用 systemctl start docker 命令。 4，重启 Docker 服务：使用 systemctl restart docker 命令。 5，测试 Docker 是否安装正确。 $ docker run --rm hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 7050e35b49f5: Pull complete Digest: sha256:13e367d31ae85359f42d637adf6da428f76d75dc9afeb3c21faea0d976f5c651 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (arm64v8) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ 3.2，下载与使用Docker公共镜像(Images) macos 系统环境下操作示例，ubuntu 系统可能略有不同。 1，使用 docker search 命令从 Docker Repo 搜索 Dokcer 可用的镜像。示例命令：docker search ubuntu18.04。 2，使用 docker image pull 命令从 Docker Repo 获取指定的 Dokcer镜像(Images)。示例命令: docker image pull docker.io/hello-world。拉取名为 docker.io/hello-world 的镜像。 3，使用 docker image ls 命令查看本地的 Dokcer 镜像(Images)。 4，使用 docker run 命令运行 Dokcer 镜像(Images)。示例命令：docker run hello-world。 5，使用 docker info 命令，查看当前 docker容器 的所有的信息。 6，使用 docker version 查看容器的版本信息。 $ docker --version # 这个命令查看 docker 版本更简单 Docker version 19.03.13, build 4484c46d9d 四，Docker 镜像命令 Docker 镜像(Images) 也可以理解为是一个用于创建 Docker容器(Container) 的静态模板。一个 Docker 镜像(Images) 可以创建很多 Docker容器(Container)。 Docker 镜像常用命令如下： 命令 描述 docker commit 创建镜像。 docker images 查看镜像信息。 docker load 导入镜像。 docker pull 拉取 Docker 镜像。 docker push 上传镜像。 docker rmi 删除镜像。 docker save 导出镜像。 docker search 在 Docker Hub 搜索镜像。 docker tag 为镜像打标签。 五，Docker 容器命令 5.1，docker run 命令 通过 docker run 命令可以基于镜像新建一个容器并启动，语法如下： docker run [OPTIONS] IMAGE [COMMAND] [ARG...] 其他常用容器管理命令如下： # 新建容器并启动 $ docker run [镜像名/镜像ID] # 启动已终止容器 $ docker start [容器ID] # 列出本机运行的容器 $ docker ps # 停止运行的容器 $ docker stop [容器ID] # 杀死容器进程 $ docker kill [容器ID] # 重启容器 $ docker restart [容器ID] docker run 命令语法 1, docker run 命令常用选项：可通过 docker run --help 命令查看全部内容。 选项 说明 -d, --detach=false 指定容器运行于前台还是后台，默认为 false。 -i, --interactive=false 打开 STDIN，用于控制台交互。 -t, --tty=false 分配 tty 设备，该可以支持终端登录，默认为 false。 -u, --user=\"\" 指定容器的用户。 -a, --attach=[] 登录容器（必须是以 docker run -d 启动的容器）。 -w, --workdir=\"\" 指定容器的工作目录。 -c, --cpu-shares=0 设置容器 CPU 权重，在 CPU 共享场景使用。 -e, --env=[] 指定环境变量，容器中可以使用该环境变量。 -m, --memory=\"\" 指定容器的内存上限。 -P, --publish-all=false 指定容器暴露的端口。 -p, --publish=[] 指定容器暴露的端口。 -h, --hostname=\"\" 指定容器的主机名。 -v, --volume=[] 给容器挂载存储卷，挂载到容器的某个目录。 –volumes-from=[] 给容器挂载其他容器上的卷，挂载到容器的某个目录。 –cap-add=[] 添加权限。 –cap-drop=[] 删除权限。 –cidfile=\"\" 运行容器后，在指定文件中写入容器 PID 值，一种典型的监控系统用法。 –cpuset=\"\" 设置容器可以使用哪些 CPU，此参数可以用来容器独占 CPU。 –device=[] 添加主机设备给容器，相当于设备直通。 –dns=[] 指定容器的 dns 服务器。 –dns-search=[] 指定容器的 dns 搜索域名，写入到容器的 /etc/resolv.conf 文件。 –entrypoint=\"\" 覆盖 image 的入口点。 –env-file=[] 指定环境变量文件，文件格式为每行一个环境变量。 –expose=[] 指定容器暴露的端口，即修改镜像的暴露端口。 –link=[] 指定容器间的关联，使用其他容器的 IP、env 等信息。 –lxc-conf=[] 指定容器的配置文件，只有在指定 --exec-driver=lxc 时使用。 –name=\"\" 指定容器名字，后续可以通过名字进行容器管理，links 特性需要使用名字。 –net=“bridge” 器网络设置：1. bridge 使用 docker daemon 指定的网桥。2. host //容器使用主机的网络。3. container:NAME_or_ID >//使用其他容器的网路，共享 IP 和 PORT 等网络资源。4. none 容器使用自己的网络（类似–net=bridge），但是不进行配置。 –privileged=false 指定容器是否为特权容器，特权容器拥有所有的 capabilities。 –restart=“no” 指定容器停止后的重启策略:1. no：容器退出时不重启。2. on-failure：容器故障退出（返回值非零）时重启。3. always：容器退出时总是重启。 –rm=false 指定容器停止后自动删除容器(不支持以 docker run -d 启动的容器)。 –sig-proxy=true 设置由代理接受并处理信号，但是 SIGCHLD、SIGSTOP 和 SIGKILL 不能被代理。 2，Docker 交互式运行的语法为：docker run -i -t IMAGE [COMMAND] [ARG] 。Docker 交互式运行，即 Docker 启动直接进入 Docker 镜像内部。 3，使用 docker ps 命令，查看正在运行的 docker。 六，参考资料 Docker-从入门到实践 Docker教程 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/效率工具/git命令学习笔记.html":{"url":"1-computer_basics/效率工具/git命令学习笔记.html","title":"git命令学习笔记","keywords":"","body":"git 命令学习笔记 git 创建代码仓库 第一步：刚下载安装的 git 都需要先配置用户名和邮箱： ```Plain Text git config --global user.name \"HarleysZhang\" git config --global user.email \"ZHG5200211@outlook.com\" 第二步：要想从 `github` 或者 `gitlab` 上实现 `clone/pull/push` 等操作，首先就得在本地创建 `SSH Key` 文件，在用户主目录下，查看是否有 `.ssh` 目录，看这个目录下是否有 `id_rsa` 和 `id_rsa.pub` 这两个文件，如果没有，则需要打开 shell（windows 系统打开Git Bash），在命令行中输入: ```bash ssh-keygen -t rsa -C \"youremail@example.com\" SSH 概述： **SSH(Secure Shell) ** 是一种网络协议，用于计算机之间的加密登录。如果一个用户从本地计算机，使用SSH协议登录另一台远程计算机，我们就可以认为，这种登录是安全的，即使被中途截获，密码也不会泄露，原因在于它采用了非对称加密技术(RSA)加密了所有传输的数据。 第三步：登录 Github，打开 \"Account settings”，“SSH Keys”页面，然后，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容，点“Add Key”，就可以看到已经添加的 Key 了。之后你就可以玩转 Git了。 为什么GitHub需要SSH Key呢？ 因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人冒充的，而Git支持SSH协议，所以，GitHub只要知道了你的公钥，就可以确认只有你自己才能推送。 第四步：上传项目到 github 仓库。配置好用户名和密码后，接下来就是将本地项目代码上传到 github/gitlab 仓库了。 在前面的准备工作完成后，你首先可以在 gitlab/github 新建仓库后，这样会得到一个仓库地址，这时候你可以把本地的文件夹上传到这个仓库地址，具体操作步骤命令如下： # 推送现有文件夹到远程仓库地址 cd existing_folder git init git remote add origin \"你的仓库地址\" git add . git commit -m \"Initial commit\" git push -u origin master 其他上传方式命令如下图： git 基本知识 本地仓库由 git 维护的三棵“树\"组成。 第一个是 工作目录，它持有实际文件； 第二个是暂存区(Index)，它像个缓存区域，临时保存仓库做的改动; 最后是 Head，它指向我们的最后一次提交的结果。 对于分支来说，在创建仓库的时候，master 是”默认的“分支。一般在项目中，要先在其他分支上进行开发，完成后再将它们合并到主分支上 master上。 不建议使用pull拉取最新代码，因为pull拉取下来后会自动和本地分支合并。 git 基本操作 git init # 创建新的git仓库 git status # 查看状态 git branch # 查看分支 git branch dev # 创建dev分支 git branch -d dev # 删除 dev 分支 git push origin --delete dev # 删除远程分支 【git push origin --参数远程分支名称】 git branch -a # 查看远程分支 git checkout -b dev # 基于当前分支(master)创建dev分支，并切换到dev分支，dev 分支会关联到 master 分支上 git checkout -f test # 强制切换至 test 分支，丢弃当前分支的修改 git checkout master # 切换到master分支 git add filename # 添加指定文件，把当前文件放入暂存区域 git add . # 表示添加新文件和编辑过的文件不包括删除的文件 git add -A # 表示添加所有内容 git commit # 给暂存区域生成快照并提交 git reset -- files # 用来撤销最后一次 git add files，也可以用 git reset 撤销所有暂存区域文件 git push origin master # 推送改动到master分支（前提是已经clone了现有仓库） git remote add origin # 没有克隆现有仓库，想仓库连接到某个远程服务器 git pull # 更新本地仓库到最新版本（多人合作的项目），以在我们的工作目录中 获取（fetch） 并 合并（merge） 远端的改动 git diff # 查看两个分支差异 git diff # 查看已修改的工作文档但是尚未写入缓冲的改动 git rm # 用于简单的从工作目录中手工删除文件 git rm -f # 删除已经修改过的并且放入暂存区域的文件，必须使用强制删除选项 -f git mv # 用于移动或重命名一个文件、目录、软链接 git log # 列出历史提交记录 git remote -v # 列出所有远程仓库信息, 包括网址 git 操作实例 1，将其他分支更改的操作提交到主分支： git checkout master # 切换回master分支(当前分支为dev) git merge dev # 合并（有合并冲突的话得手动更改文件） 2，git** 如何回退版本**： git log # 查看分支提交历史，确认要回退的历史版本 git reset --hard [commit_id] # 恢复到历史版本 git push -f -u origin branch # 把修改推送到远程仓库 branch 分支 4，拉取远程分支到本地： # 本地已经拉取了仓库代码，想拉取远程某一分支的代码到本地 git checkout -b ac_branch origin/ac_branch # 拉取远程分支到本地(方式一) git fetch origin ac_branch:ac_branch # 拉取远程分支到本地(方式二) 5，查看本地已有分支 # 显示该项目的本地的全部分支，当前分支有 * 号 git branch 6，查看本地分支和原称分支差异 7，回退版本 git 操作的一些问题 git 解决 fatal: 'origin' does not appear to be a git repository问题。 控制台输入： git remote add origin git@github.com:xxse/xx.git 推送本地新创建分支到远程仓库新分支 git push origin local_branch:remote_branch 参考资料 Git 教程 图解Git git documentation Git 使用简明手册 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/效率工具/ubuntu16.04安装mmdetection库.html":{"url":"1-computer_basics/效率工具/ubuntu16.04安装mmdetection库.html","title":"ubuntu16.04安装mmdetection库","keywords":"","body":" 一，更新 pip 和 conda 下载源 1.1，查看 conda 和 pip 版本 1.2，更新 pip 下载源 1.3，更新 conda 下载源 二，MMDetection 简介 三，MMDetection 安装 3.1，依赖环境 3.2，安装过程记录 1，安装操作系统+cuda 2，安装 Anconda3 3，安装 pytorch-gpu 4，安装 mmdetection 5，安装 MMOCR 参考资料一，更新 pip 和 conda 下载源 默认情况下 pip 使用的是国外的镜像，在下载的时候速度非常慢，下面我将介绍如何更新下载源为国内的清华镜像源，其地址为：https://pypi.tuna.tsinghua.edu.cn/simple，阿里云镜像的更新方法一样。 1.1，查看 conda 和 pip 版本 root# conda --version conda 22.9.0 root# pip --version pip 20.2.4 from /opt/miniconda3/lib/python3.8/site-packages/pip (python 3.8) 如果 pip 版本 ，建议升级 pip 到最新的版本 (>=10.0.0) 以方便后面的更新下载源配置： # 更新 pip 版本命令 python -m pip install --upgrade pip 1.2，更新 pip 下载源 在下载安装好 python3+pip 或 anconda3 的基础上，建议更新为清华/阿里镜像源（默认的 pip 和 conda下载源速度很慢）。 1，Linux/Mac 系统，pip 全局更新下载源为清华源和和查看下载源地址的命令如下: # 更新 pip 下载源为清华镜像 pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 查看下载源地址 pip3 config list 2，Windows 系统，需要当前在当前用户目录下手动创建配置文件然后修改。 资源管理器的地址栏输入 %appdata% 后回车，切换到用户路径下的 appdata 目录； 进入到 pip 文件夹中（没有则手动创建），并创建文件 pip.ini，此文件的完整路径为 %APPDATA%/pip/pip.ini； 在 pip.ini 文件中添加以下内容后，再次使用 pip，即会使用新下载源。 [global] timeout = 8000 index-url = https://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.cn/simple Linux/Mac 也可通过直接修改配置文件（可能需要 root 权限）的方式直接更新下载源，vim ~/.pip/pip.conf，修改如下: global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple' 1.3，更新 conda 下载源 1，conda 更新源的方法： 各系统都可以通过修改用户目录下的 .condarc 文件。Windows 用户无法直接创建名为 .condarc 的文件，可先执行 conda config --set show_channel_urls yes 生成该文件。Linux/Mac 系统一般自带 .condarc 文件，文件地址为 ~/.condarc。 2，之后再修改.condarc 文件内容如下: channels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud 二，MMDetection 简介 MMDetection 是一个基于 PyTorch 的目标检测开源工具箱。它是 OpenMMLab 项目的一部分。主分支代码目前支持 PyTorch 1.5 以上的版本。主要特性为： 模块化设计 丰富的即插即用的算法和模型 速度快 性能高 更多详情请参考 MMDetection 仓库 README。 三，MMDetection 安装 3.1，依赖环境 系统：首选 Linux，其次 macOS 和 Windows（理论上支持，实际安装需要踩很多坑） Python 3.6+ 首选 CUDA 11.3+、其次推荐 CUDA 9.2+ 首选 Pytorch 1.9+，其次推荐 PyTorch 1.3+ GCC 5+ MMCV 3.2，安装过程记录 1，安装操作系统+cuda 我是在 docker 容器中安装和进行深度学习算法开发的，其操作系统、cuda、gcc 环境如下： 2，安装 Anconda3 官网下载 Anconda3 linux 安装脚本，并安装 Anconda3（很好装一路 yes 即可），并使用 conda 新建虚拟环境，并激活虚拟环境进入。 conda create -n mmlab python=3.8 -y # 创建 mmlab 的虚拟环境，其中python解释器版本为3.8(python3.9版本不行, 没有pytorch_cuda11.0版本) conda activate mmlab # 激活虚拟环境进入 虚拟环境安装成功后的部分过程截图如下所示： 如果你激活虚拟环境出现如下所示错误。 CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'. To initialize your shell, run $ conda init Currently supported shells are: - bash - fish - tcsh - xonsh - zsh - powershell See 'conda init --help' for more information and options. IMPORTANT: You may need to close and restart your shell after running 'conda init'. 可通过以下命令重新激活 conda 环境，即可解决问题，方法参考自 stack overflow 问题。 source ~/anaconda3/etc/profile.d/conda.sh # anaconda3 的安装路径有可能不一样，自行修改 conda activate mmlab 3，安装 pytorch-gpu 首选安装 pytorch-gpu 版本，使用在线安装命令: conda install pytorch=1.7.1 cudatoolkit=11.0 torchvision=0.8.2 -c pytorch 官网命令的 cuda11.0 的 torchaudio==0.7.2 版本不存在，故去除。 安装过程信息（记得检查 pytorch 版本是 cuda11.0 的）截图如下： 安装成功后，进入 python 解释器环境，运行以下命令，判断 pytorch-gpu 版本是否安装成功。 >>> import torch >>> torch.cuda.is_available() True >>> torch.cuda.device_count() 2 >>> 同时可通过以下命令查看 CUDA 和 PyTorch 的版本 python -c 'import torch;print(torch.__version__);print(torch.version.cuda)' 总的来说，pytorch 等各种 python 包有离线和在线两种方式安装： 在线：conda/pip 方法安装，详细命令参考 pytorch 官网，但是这种方式实际测试下来可能会有问题，需要自己肉眼检查安装的版本是否匹配。 离线：浏览器下载安装包，然后通过 pip 或者 conda 方式离线安装。 pip 可通过此链接 浏览器下载各种 pytorch 版本的二进制安装包，到本地安装（pip install *.whl）。 conda 通过清华源链接，浏览器下载对应版本压缩包，然后 conda install --offline pytorch压缩包的全称（后缀都不能忘记） 不通过浏览器下载 whl 包，而是 pip install https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp39-cp39-linux_x86_64.whl 方式可能会有很多问题，比如网络问题可能会导致安装失败。 WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError(': Failed to establish a new connection: Errno 101 Network is unreachable')': /whl/cu110/torch-1.7.1%2Bcu110-cp39-cp39-linux_x86_64.whl 或者下载到一半的网络连接时常超过限制。 pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='download.pytorch.org', port=443): Read timed out. 4，安装 mmdetection 不建议安装 cpu 版本，因为很多算子不可用，其次截止到2022-11-3日，macos 系统 cpu 环境的 mmdet.apis 是不可用的。 建议使用 MIM 来自动安装 MMDetection 及其相关依赖包-mmcv-full 。 pip install openmim # 或者 pip install -U openmim mim install mmdet 5，安装 MMOCR MMOCR 依赖 PyTorch, MMCV 和 MMDetection。这些依赖环境，我们在前面的步骤中已经安装好了，所以可通过下面命令直接安装 MMOCR。 情况 1: 若你需要直接运行 MMOCR 或在其基础上进行开发，则通过源码安装： git clone https://github.com/open-mmlab/mmocr.git cd mmocr pip3 install -v -e . # \"-v\" 会让安装过程产生更详细的输出 # \"-e\" 会以可编辑的方式安装该代码库，你对该代码库所作的任何更改都会立即生效 情况 2：如果你将 MMOCR 作为一个外置依赖库使用，通过 pip 安装即可： pip install mmocr 参考资料 mmdetection 和 pytorch 官网 https://download.pytorch.org/whl/torch\\_stable.html Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/操作系统/计算机基础知识.html":{"url":"1-computer_basics/操作系统/计算机基础知识.html","title":"计算机基础知识","keywords":"","body":"知识点目录 操作系统 计算机网络 面向对象 数据库 Linux系统开发 常用工具（Cmake/Git/Docker/正则表达式） 操作系统 操作系统中堆和栈的区别 操作系统中堆和栈都是指内存空间，不同的是堆为按需申请、动态分配，例如 C++ 中的 new 操作（当然 C++ 的 new 不仅仅是申请内存这么简单）。堆可以简单理解为当前使用的空闲内存，其申请和释放需要程序员自己写代码管理。 操作系统中的栈是程序运行时自动拥有的一小块内存，大小在编译时由编译器参数决定，是用于局部变量的存放或者函数调用栈的保存。在 C 中声明一个局部变量（如 int a）会存放在栈中，当其离开作用域后，所占用的内存则会被释放，栈用于保存函数调用栈时和数据结构的栈是有关系的。在函数调用过程中，常常会多层甚至递归调用。每一个函数调用都有各自的局部变量值和返回值，每一次函数调用其实是先将当前函数的状态压栈，然后在栈顶开辟新空间用于保存新的函数状态，接下来才是函数执行。当函数执行完毕之后，栈先进后出的特性使得后调用的函数先返回，这样可以保证返回值的有序传递，也保证函数现场可以按顺序恢复。操作系统的栈在内存中高地址向低地址增长，也即低地址为栈顶，高地址为栈底。这就导致了栈的空间有限制，一旦局部变量申请过多（例如开个超大数组），或者函数调用太深（例如递归太多次），那么就会导致栈溢出（Stack Overflow），操作系统这时候就会直接把你的程序杀掉。 参考知乎问答-什么是堆？什么是栈？他们之间有什么区别和联系？ Linux查看cpu和cache信息 1，Linux 查看 cpu 信息命令：cat /proc/cpuinfo。 (base) pc:$ cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 158 model name : Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz stepping : 10 microcode : 0xb4 cpu MHz : 3192.005 cache size : 12288 KB physical id : 0 siblings : 1 core id : 0 cpu cores : 1 apicid : 0 initial apicid : 0 fpu : yes fpu_exception : yes cpuid level : 22 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt xsaveopt xsavec xsaves arat md_clear flush_l1d arch_capabilities bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit srbds bogomips : 6384.01 clflush size : 64 cache_alignment : 64 address sizes : 43 bits physical, 48 bits virtual power management: ... processor : 3 ... 2，Linux 查询 L1/L2/L3 cache大小：cat /sys/devices/system/cpu/cpu0/cache/index*/size(*为 0/1/2/3) (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index0/size 32K (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index0/type Data (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index1/type Instruction (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index1/size 32K (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index2/size 256K (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index3/size 12288K 现代 CPU的 L1 cache 是逻辑核私有的，L1 cache分指令L1 cache和数据L1 cache，大小相等都为 32 KB；目前，L2 cache也是片内私有，所以每个核只有256 KB；而对于L3 cache，一个物理核CPU的所有逻辑核共享，所以在每个逻辑核来看，L3 cache都为12288 KB。本机的虚拟机总共有 1 个物理核，而本机共有 6 核 12 线程，所以可推算得到**本机的 Cache 信息： L1 cache： L1 Data: 192 KB = 32 x 6 KB L1 Instruction: 192 KB = 32 x 6 KB L2 cache：1536 KB = 256 X 6 KB L3 cache：12288 KB Windows查看cpu和cache信息 任务管理器--->性能，即可查看 cpu 和 L1/L2/L3 Cache 大小，如下图所示： 或者下载安装 cpuz 软件，打开即可查看，如下图所示。 并发与并行区别 并发是指宏观上在一段时间内能同时运行多个程序，而并行则指在同一时刻能运行多个指令。 并行需要硬件支持，如多流水线、多核处理器或者分布式计算系统。 操作系统通过引入进程和线程，使得程序能够并发运行。 数据库 什么是事务 事务是指满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以用 Rollback 进行回滚。 并发一致性问题 丢失修改 读脏数据 不可重复读 脏影读 多进程与多线程区别 进程是资源分配的最小单位，线程是CPU调度（独立调度）的最小单位。 多个进程之间相互独立，一个任务就是一个进程，进程内的“子任务”称为线程；线程是进程的一部分，一个进程至少有一个线程。 同一个进程中的所有线程的数据是共享的（进程通讯），线程之间可以直接通信；但是进程之间的数据是独立的，进程之间的交流需要借助中间代理（｀IPC`）来实现。 由于创建或撤销进程时，系统要为之分配资源或回收资源，如内存空间、I/O设备等，所以进程间调用、通讯和切换开销均比多线程大，单个线程的崩溃会导致整个应用的退出。 存在大量IO，网络耗时或者需要和用户交互等操作时，使用多线程（线程开销小、切换速度快）有利于提高系统的并发性和用户界面快速响应从而提高友好性。 进程/线程通信 进程间通信 进程间通信（IPC, InterProcess Communication）：是指在不同进程之间传播或交换信息。IPC 的方式有：管道、消息队列、信号量、共享存储、Socket等。Socket　支持不同主机的两个进程的　IPC。 python Process类参数：target表示调用的对象，就是子进程要执行的任务 Python 多线程通信可以通过导入Process（创建进程）　和Queue(创建队列)：from multiprocessing import Process, Queue，具体通信过程如下： 父进程创建 Queue,并传递给各个子进程：q = Queue() pw = Process(target=write, args=(q,)) 分别启动写入数据和读数据子进程：pw.start() pr.start() 等待进程技术：pw.join() 线程通信 一个进程所含的不同线程是共享内存的，所以线程之间共享数据有一个问题是多个线程共享一个变量，导致内容改乱了。解决办法是，如果不同线程间有共享的变量，其中一个方法就是在修改前给其上一把锁lock，确保一次只有一个线程能修改。Python 创建线程方式如下： t = threading.Thread(target=loop, name='LoopThread') # `target`表示调用的对象(自己定义的任务函数) Python 创建锁可以使用，threading.lock() 方法，实现对一个共享变量的锁定，修改完后 release 供其它线程使用，具体加锁 python 代码如下： balance = 0 lock = threading.Lock() # 创建锁 def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() 并发与并行 多进程与多线程的区别 并行指物理上同时执行，并发指能够让多个任务在逻辑上交织执行的程序设计。 要让单核　CPU 运行多任务，就得用到并发技术，实现并发技术相当复杂，最容易理解的是“时间片轮转进程调度算法”，它的思想简单介绍如下：在操作系统的管理下，所有正在运行的进程轮流使用CPU，每个进程允许占用CPU的时间非常短(比如10毫秒)，这样用户根本感觉不出来CPU是在轮流为多个进程服务，就好象所有的进程都在不间断地运行一样。但实际上在任何一个时间内有且仅有一个进程占有CPU。如果一台计算机有多个CPU，情况就不同了，如果进程数小于CPU数，则不同的进程可以分配给不同的CPU来运行，这样，多个进程就是真正同时运行的，这便是并行。但如果进程数大于CPU数，则仍然需要使用并发技术。因此，我们可以得出以下结论： 总线程数 总线程数 > CPU数量：并发运行 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/操作系统/深入理解计算机系统-第1章计算机系统漫游笔记.html":{"url":"1-computer_basics/操作系统/深入理解计算机系统-第1章计算机系统漫游笔记.html","title":"深入理解计算机系统-第1章计算机系统漫游笔记","keywords":"","body":"深入理解计算机系统-计算机系统漫游笔记.md 1，信息就是位+上下文 计算机系统是由硬件和系统软件组成，它们共同工作来运行应用程序。 C 语言是系统级编程的首选，同时它也非常实用于应用级程序的编写。 2，程序被其他程序翻译成不同格式 以hello 程序为例来解释程序的生命周期，hello.c 文件代码如下: #include int main() { printf(\"hello world\\n\"); return 0; } 为了在系统上运行 hello.c 程序，文件中的每条 C 语言都必须被其他程序转化为一系列的低级机器语言指令。然后这些指令按照一种称为可执行目标程序的格式打包好，并以二进制磁盘文件的形式保存。目标程序也称为可执行目标文件。 在 Linux 系统中，GCC 编译器将源程序文件 hello.c 翻译（编译）为目标文件 hello 的过程分为四个阶段，如下图所示。执行这四个阶段（预处理器、编译器、汇编器和链接器）的程序一起构成了编译器系统（compilation system）。 3，了解编译器如何工作是有大有益处的 优化程序性能。了解一些机器代码（汇编代码）以及编译器将不同的 C/C++语句转化为机器代码的方式。比如一个函数调用的开销有多大？while 循环比 for 循环更有效吗？指针引用比数组索引更有效吗等？ 理解链接时出现的错误。比如静态库和动态库的区别，静态变量和全局变量的区别等。 避免安全漏洞。 4，处理器读取存在内存中的指令 典型系统的硬件组织构成: 总线、I/O 设备、主存（动态随机存取存取器 DRAM）、处理器。 主存是临时存储设备，在处理器执行程序时，用来存放程序和程序处理的数据。从逻辑上说，存储器是一个线性的字节数组，每个字节都有其唯一的地址（数组索引），这些地址是从零开始的。 将 hello 程序输出的字符串从存储器写到显示器的过程如下图所示。 5，高速缓冲至关重要 针对处理器与主存之间读取数据的差异，处理器系统设计者采用了更小更快的存储设备，称为高速缓冲器（cache memory，简称 cache 或高速缓冲），作为暂时的集结区域，存放处理器近期可能会需要的信息。 6，存储设备形成层次结构 在处理器和一个较大较慢的设备（例如主存）之间插入一个更小更快的存储设备（例如高速缓冲）的想法已经成为一个普遍的观念。实际上，每个计算机系统中的存储设备都被组织成了一个存储器层次结构，如图1-9所示。在这个层次结构中，从上至下，设备的访问速度越来越慢、容量越来越大，并且每字节的成本也越来越低。 7，操作系统管理硬件 所有应用程序对硬件的操作尝试都必须通过操作系统，操作系统有两个基本功能： 防止硬件被失控的应用程序滥用； 向应用程序提供简单一致的机制来控制复杂而又通常大不相同的低级硬件设备。 操作系统通过几个基本的抽象概念（进程、虚拟内存和文件）来实现这两个功能。文件是对 I/O 设备的抽象表示，虚拟内存是对主存和磁盘 I/O 设备的抽象表示，进程则是对处理器、主存和 I/O 设备的抽象表示。 7.1，进程 进程是操作系统对一个正在运行的程序的一种抽象。在一个系统上可以同时运行多个进程，每个进程都好像在独占地使用硬件。而并发运行，则是说一个进程的指令和另一个进程的指令是交错之行的。在大多数系统中，需要运行的进程数是可以多于它们的 CPU 个数的。无论在单核还是多核系统中，一个 CPU 看上去都像是在并发地执行多个进程，这实际是通过处理器在进程间切换来实现的，操作系统实现这种交错执行的机制成为上下文切换。 操作系统会保持跟踪进程运行所需的所有状态信息，这种状态信息称为上下文，其包括多种信息，比如 PC 和寄存器文件的当前值，以及主存的内容。在任何一个时刻，但处理器系统都只能执行一个进程的代码。当操作系统决定要把控制权从当前进程转移到某个新进程时，就会进行上下文切换，即保存当前进程的上下文、恢复进程的上下文，然后将控制权传递到新进程。图1-12展示了示例 hello 程序运行场景的基本理念。 7.2，线程 尽管我们直观认为一个进程只有单一的控制流，但在现代计算机系统中，一个进程实际上可以由多个称为线程的执行单元组成，每个线程都运行在进程的上下文中，并共享同样的代码和全局数据。由于网络服务器对并行处理的需求，线程成为越来越重要的编程模型，因为多线程之间比多进程之间更容易共享数据且更高效。 7.3，虚拟内存 虚拟内存是一个抽象概念，它为每个进程提供了一个假象，即每个进程都在独占地使用主存，每个进程看到的内存都是一致的，称为虚拟地址空间。图 1-13 所示的是 Linux 进程的虚拟地址空间。在 Linux 中，地址空间最上面的区域是保留给操作系统中代码和数据的，这对所有进程来说都是一样。地址空间的底部区域存放用户进程定义的代码和数据。注意，图中的地址从下往上增大的。 每个进程看到的虚拟地址空间由大量准确定义的区构成，每个区都有专门的功能，进程的虚拟地址空间意义如下。 程序代码和数据。对所有进程来说，代码是从同一固定地址开始，进接着的是和全局变量和相对应的数据位置，代码和数据区是直接按照可执行目标文件的内容初始化的。 堆。代码和数据区后紧随着的是运行时堆。代码和数据区在进程一开始运行时就被指定了大小，与此不同，当调用 malloc 和 free 这样的 C 标准库函数时，堆可以在运行时动态地拓展和收缩。 共享库。地址空间的中间部分是一块用来存放像 C 标准库和数学库这样的共享库的代码和数据的区域。 栈。位于用户虚拟地址空间顶部的是用户栈，编译器用它来实现函数调用。和堆类似，用户栈在程序执行期间可以动态地拓展和收缩，调用函数栈则增长，从一个函数返回，栈则收缩。 内核虚拟内存。地址空间顶部的区域是为操作系统内核保留的。 虚拟内存的运作需要硬件和操作系统软件之间精密复杂的交互，包括对处理器生成的每个地址的硬件翻译，基本思想是把一个进程虚拟内存的内容存在在磁盘上，然后用主存作为磁盘的高速缓冲。 7.4，文件 文件就是字节序列，每个 I/O 设备包括磁盘、键盘、显示器，甚至网络都可以看成文件，系统中所有输入输出都是通过使用一小组称为 Unix I/O 的系统函数调用读写文件来实现的。 8，系统之间利用网络通信 现代系统通过网络将单个计算机系统连接在一起。 9，重要主题 9.1，Amdahl 定律 该定律的主要思想就是，当我们对系统的某个部分加速时，其对系统整体性能的影响取决于该部分的重要性和加速程度。 9.2，并发与并行 我们用术语并发（concurrency）是一个通用的概念，指一个同时具有多个活动的系统；而术语并行（parallelism）指的是用并发来是一个系统运行得更快。并行可以在计算机系统的多个抽象层次上运用，这里按照系统层次结构中由高到低的顺序描述三个层次。 1，线程级并发 超线程，有时称为同时多线程（simultaneous multi-threading），是一项允许一个 CPU 执行多个控制流的技术。它涉及 CPU 某些硬件有多个备份，比如程序计数器和寄存器文件，而其他硬件部分只有一份，比如执行浮点算术运算的单元。常规的处理器需要大约 20000 个时钟周期做不同线程间的转换，而超线程的处理器可以在单个周期的基础上决定要执行哪一个线程。 多处理器的使用可以从两方面提高系统性能，首先，它减少了在执行多个任务时模拟并发的需要；其次，它可以使应用程序运行得更快，前提是程序以多线程方式编写。 2，指令级并行 在较低的抽象层次上，现代处理器可以同时执行多条指令的属性称为指令级并行。如果处理器可以达到比一个周期一条指令更快的执行速率，就称为超标量（super-scalar）处理器，大多数现代处理器都支持超标量操作。 3，单指令、多数据并行 在最低层次中，许多现代处理器拥有特殊的硬件，允许一条指令产生多个可以并行执行的操作，这种方式称为单指令、多数据，即 SIMD 并行。 9.4，计算机系统中抽象的重要性 前面我们介绍了计算机系统使用的几个抽象，如图 1-18 所示，在处理器里，指令集架构提供了对实际处理器的抽象。 操作系统中有四个抽象：文件是对 I/O 设备的抽象，虚拟内存时对程序存储器的抽象，进程是对一个正在运行的程序的抽象，虚拟机提供对整个计算机的抽象，包括操作系统、处理器和程序。 参考资料 《深入理解操作系统第三版-第1章》 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/操作系统/深入理解计算机系统-第2章信息的表示和处理.html":{"url":"1-computer_basics/操作系统/深入理解计算机系统-第2章信息的表示和处理.html","title":"深入理解计算机系统-第2章信息的表示和处理","keywords":"","body":" 关于程序的结构和执行，我们需要考虑机器指令如何操作整数和实数数据，以及编译器如何将 C 程序翻译成这样的指令。 现代计算机存储和处理信息是用二进制（二值信号）表示的，因为二值信号更容易表示、存储和传输，如导线上的高电压和低电压。 1，信息存储 大多数计算机使用字节（byte），作为最小的可寻址的内存单位，而不是访问内存中单独的位。机器级程序将内存视为一个非常大的字节数组，称为虚拟内存（virtual memory），内存中的每个字节都由一个唯一的数字来表示，称为它的地址（address），所有可能地址的集合称为虚拟地址空间（virtual address space）。简而言之，这个虚拟地址空间只是一个展现给机器级程序的概念性映像，实际的实现是将动态随机访问存储器（DRAM）、闪存、磁盘存储器、特殊硬件和操作系统软件结合起来，为程序提供一个看上去统一的字节数组。 1.1，十六进制表示法 一个字节由 8 位组成，在二进制表示法中，它的值域是 $000000002\\sim 11111111{2}$，如果看成十进制整数，它的值域则是 $0{10}\\sim 255{10}$。在 C 语言中，以 0x 或 0X 开头的数字常量被认为是十六进制的值。 1.2，字数据大小 每台计算机都有一个字长（word size），说明指针数据的标称大小（nominal size），字长决定的最重要的系统参数就是虚拟地址空间的最大大小，即对于一个字长$w$位的机器而言，虚拟地址的范围为 $0\\sim 2^{w}-1$，程序最多访问 $2^{w}$个字节。 1.3，寻址和字节顺序 对于跨越多个字节的程序对象，我们必须建立两个规则: 这个对象的地址是什么，以及内存中如何排列这些字节。对象的存储有两个通用的规则: 小端法（little endian），最低有效字节在最前面的方式。 大端法（big endian），最高有效字节在最前面的方式。 2，整数表示 略 3，整数运算 略 4，浮点数 IEEE 754 标准同来表示计算机系统中的浮点数定义，即浮点数规则都遵从 IEEE 754 标准。 4.1，二进制小数 理解浮点数的第一步是考虑含有小数值的二进制数字，其表示方法如下: $$b = \\sum{i=-n}^{m} 2^{i} \\times b{i}$$ 符号 . 现在变成了二进制的点，点左边的位的权是 2 的正幂，点右边的位的权是 2 的负幂。小数的二进制幂表示如下图所示。 4.2，IEEE 浮点表示 定点表示法不能有效地表示非常大的数字。IEEE 标准 754，浮点表示用 $V = (-1)^s \\times M \\times 2^E$表示一个数: 在 IEEE 754 标准中浮点数由三部分组成：符号位（sign bit），有偏指数（biased exponent），小数（fraction）。浮点数分为两种，单精度浮点数（single precision）和双精度浮点数（double precision），它们两个所占的位数不同。 在单精度浮点格式（C 语言的 float）中，符号位，8 位指数，23 位有效数。 在双精度浮点格式（C 语言的 double）中，符号位，11 位指数，52 位有效数。 给定位表示，根据 exp 的值，被编码的浮点数可以分成三种不同的情况（最后一种有两个变种）。图2-33说明了对单精度格式的情况。 规格化的值产生的指数的取值范围，对于单精度是 -126～127，而对于双精度是 -1022~1023。 4.3，浮点数的规格化 若不对浮点数的表示作出明确规定，同一个浮点数的表示就不是唯一的。例如 $(1.75)_{10}$可以表示成 $1.11\\times 2^0$，$0.111\\times 2^1$，$0.0111\\times 2^2$等多种形式。当尾数不为 0 时，尾数域的最高有效位为 1，这称为浮点数的规格化。否则，以修改阶码同时左右移动小数点位置的办法，使其成为规格化数的形式。 1，单精度浮点数真值 对于浮点数的规格化的值，其指数的偏差 Bias 是其可能值的一半: $2^{k-1}-1$(单精度是127，双精度是 1023)。 也就是说，用存储的指数减去此偏差就得到了实际指数。 如果存储的指数小于此偏差，则实际为负指数。 IEEE754 标准中，一个规格化的 32 位浮点数 $x$ 的真值表示为： $$x = (-1)^{S}\\times (1.M)\\times 2^{e}, e = E-127, e\\in [-126, 127]$$ 其中尾数域值是 1.M。因为规格化的浮点数的尾数域最左位总是 1，所以这一位不予存储，默认其隐藏在小数点的左边。在计算指数 e 时，对阶码 E 的计算采用原码的计算方式，因此 32 位浮点数的 8 bits 的阶码 E 的取值范围是 0 到 255。其中当 E 为全 0 或者全 1 时，是 IEEE754 规定的特殊情况。去除 0 和 255 这两种特殊情况，那么指数 $e$ 的取值范围就是 $1-127=-126$ 到 $254-127=127$。 因为 $2^{127}$ 大约等于 $10^{38}$，所以单精度的实际极限为 $10^{38}$ ($e^{38}$)。 2，双精度浮点数真值 64 位的浮点数中符号为 1 位，阶码域为 11 位，尾数域为 52 位，指数偏移值是 1023（指数偏差）。因此规格化的 64 位浮点数 x 的真值是： $$x = (-1)^{S}\\times (1.M)\\times 2^{e}, e = E-1023, e\\in [-1022,1023]$$ 因为 $2^{1023}$ 大约等于 $10^{308}$，所以单精度的实际极限为 $10^{308}$（$e^{308}$）。 4.4，数字示例 图 2-34 展示了一组数值，它们可以用假定的 6 位格式来表示，有 $k=3$的阶码位和 $n=2$的尾数位，偏置位是 $2^{3-1}-1 = 3$。 4.5，浮点数的数值范围 下图 2-36 展示了单精度和双精度的浮点数取值范围。 参考资料 《深入理解操作系统第三版-第2章》 IEEE 浮点表示形式 IEEE Standard 754 Floating Point Numbers Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/操作系统/深入理解计算机系统-第3章程序的机器级表示.html":{"url":"1-computer_basics/操作系统/深入理解计算机系统-第3章程序的机器级表示.html","title":"深入理解计算机系统-第3章程序的机器级表示","keywords":"","body":"计算机执行机器代码，用字节序列编码低级的操作，包括处理数据、管理内存、读写存储设备上的数据，以及利用网络通信。编译器基于编程语言的规则、目标机器的指令集和操作系统遵循的惯例，经过一系列阶段生成机器代码。 在本章中，我们将详细学习一种特别的汇编语言，了解如何将 C 程序编译成这种形式的机器代码。阅读编译器产生的汇编代码，需要具备的技能不同于手工编写汇编代码，我们必须了解典型的编译器在将 C 程序结构变换成机器代码时所做的转换。相对于 C 代码表示的计算操作，优化编译器能够重新排列执行顺序，消除不必要的计算，用快速操作替换慢速操作，甚至将递归计算变换成迭代计算。但是源代码与对应的汇编代码的对应关系通常不太容易理解，因为这是一种逆向工程（reverse engineering）-通过研究系统和逆向工作。 本章内容会涉及到 x86-64 汇编级指令代码。 1，历史观点 Intel 处理器系列俗称 x86，经历了一个长期的、不断进化的发展过程。 2，程序编码 Linux 系统默认的编译器时 GCC C 编译器。编译器选项 -Og 会指示编译器使用会生成符合原始 C 代码整体结构的机器代码的优化等级，通常使用 -O1 或 -O2 选项。 x86-64 的机器代码和原始的 C 代码差别非常大，一些通常对 C 语言程序员隐藏的处理器状态都是可见的: 程序计数器（PC，在 x86-64 中用 %rip 表示）给出将要执行的下一条指令在内存中的地址。 整数寄存器文件包含 16 个命名的位置，分别存储 64 位的值。这些寄存器可以存储地址（对应于C 语言的指针）或整数数据。有的寄存器被用来记录默写重要的程序状态，有的寄存器保存临时数据，如过程的参数和局部变量，以及函数的返回值。 条件码寄存器保存着最近执行的算术或逻辑指令的状态信息。它们用来实现控制或数据流中的条件变化，如用来实现 if 和 else 语句。 一组向量寄存器可以存放一个或多个整数或浮点数值。 3，数据格式 C 语言数据类型在 x86-64 中的大小。 浮点数主要有两种形式：单精度（4 字节）值，对应于 C 语言数据类型 float，双精度（8 字节）值，对应于 C 语言数据类型 double。如上图所示，大多数 GCC 生成的汇编代码指令都有一个字符的后缀，表明操作数的大小，例如，数据传送指令有四个变种: movb（传送字节）、movw（传送字）、movl（传送双字）和movq（传送四字）。 4，访问信息指令 一个 x86-64 的中央处理单元（CPU）包含一组 16 个存储 64 位值的通用目的寄存器，这些寄存器用来存储整数数据和指针。下图显示了这 16 个寄存器，它们的名字都以 %r 开头，后面还跟着一些不同的命名规则的名字。最初的 8086 中有 8 个 16 位的寄存器，即图 3-2 中的 %ax 到 %bp，每个寄存器都有特殊的用途，它们的名字就反映了这些不同的用途。拓展到 IA32 架构时，这些寄存器也拓展成 32 位寄存器，标号从 %eax 到 %ebp。拓展到 x86-64 后，用来的 8 个寄存器拓展成 64 位，标号从 **%rax 到 %rbp，除此之外，还增加了 8 个寄存器，标号从 %r8 到 %r15**。 4.1，操作数指示符 大多数指令有一个或多个操作数（operand），指示出执行一个操作中要使用的源数据值，以及防止结果的目的位置。不同操作数被分为三种类型。 立即数（immediate），用来表示常数值。 寄存器（register） 内存引用: 它会根据计算出来的地址访问某个内存位置。 如图 3-3 所示，有多重不同的寻址模式，允许不同形式的内存引用。 4.2，数据传送指令 最频繁使用的指令是将数据从一个位置复制到另一个位置的指令。图 3-4 列出的最简单形式的数据传送指令-MOV 类: 把数据从源位置复制到目的位置，不做任何变化。 简单的数据传送指令示例代码如下。（记住，第一个是源操作数，第二个是目的操作数） 4.3，数据传送示例 4.4，压入和弹出数据 栈和队列都是一种\"操作受限\"的线性表（逻辑结构），只允许在一端插入和删除数据；栈的特性是先进后出，队列是先进先出。栈在处理函数调用过程中很重要，通过 push 指令把数据压入栈中，通过 pop 指令删除数据。 栈可以可以通过数组实现，总是从数组的一端插入和删除元素，这一端称为栈顶，栈顶元素的地址是所有栈中元素地址中最低的，栈指针 **%rsp** 保存着站定元素的地址。入栈和出栈汇编指令描述如下，栈操作指令都只有一个操作数-压入的数据源和弹出的栈顶数据。 将一个四字值压入栈中，分为两步，首先先将栈指针减 8，然后将值写到新的栈顶地址，因此 pushq 指令等价于下面两条指令: subq $8,%rsp Decrement stack pointer moq %rbp,(%rsp) 5，算术和逻辑操作指令 图 3-10 列出了 x86-64 的一些整数和逻辑操作。和访问信息指令一样，算术和逻辑操作指令类也有各种带不同大小操作数的变种。例如，指令类 ADD 由四条加法指令组成: addb、addw、addl 和 addq，分别是字节加法、字加法、双字加法和四字加法。算术和逻辑操作指令分为四组：加载有效地址、一元操作、二元操作和移位。二元操作数有两个操作数，而一元操作数有一个操作数。 5.1，加载有效地址 加载有效地址（load effective address）指令 leaq 实际上是 movq 指令的变形。leaq 指令可以简洁地描述普通的算术操作，如果寄存器 %rdx 的值为 x，那么指令 leaq 7(%rdx, %rdx, 4),%rax 将设置寄存器 %rax 的值为 5x+7。目的操作数必须是一个寄存器。 5.2，一元和二元操作 对于二元操作指令，第一个操作数可以是立即数、寄存器或内存位置，第二个操作可以是寄存器或内存位置。 5.3，移位操作 移位操作指令，先给出移位量，第二项给出要移位的数，移位量可以是立即数，或者在单字节寄存器 %cl 中，移位量是由 %c1 寄存器的低 m 位决定的。例如当寄存器 %c1 的十六进制位 0xFF 时，指令 salb 会移 7 位，salw 会移 15 位，sall 会移 31 位，salq 会移 63 位。 5.4，总结 6，控制指令 前面的两类指令都是直线代码行为，也就是指令一条接着一条顺序地执行。C 语言中的某些结构，比如条件语句、循环语句和分支语句，要求有条件的执行，根据数据测试的结果来决定操作执行的顺序。 6.1，条件码 除了整数寄存器，CPU 还维护着一组单个位的条件码（condition code）寄存器，它们描述了最近的算术或逻辑操作的属性，可以通过检测条件码寄存器来执行条件分支指令。最常用的条件码有： CF： 进位标志。最近的操作使最高位产生了进位，可用来检查无符号操作的溢出。 ZF: 零标志。最近的操作得出的结果为 0。 SF: 符号标志位。最近的操作得到的结果为负数。 OF: 溢出标志。最近的操作导致一个补码已出-正溢出或负溢出。 除了图 3-10 的整数算术操作指令会设置条件码，还有两类指令 CMP 和 TEST ，但它们只设置条件码而不改变任何其他寄存器。如下图 3-13 所示，CMP 指令根据两个操作数之差来设置条件码。 6.2，访问条件码 条件码通常不会直接读取，常使用方法有三种： 可以根据条件码的某种组合，将一个字节设置为 0 或者1。 可以有条件跳转到程序的某个其他的部分。 可以有条件地传送数据。 6.3，跳转指令 跳转（jump）指令会导致执行切换到程序中一个全新的位置，示例代码如下。 图 3-15 列举了不同的跳转指令，jmp 指令是无条件跳转，可以是直接跳转，即跳转目标是作为指令的一部分编码的；也可以是间接跳转，即跳转目标是从寄存器或内存位置中读出的。汇编语言中，直接跳转是会给出一个标号作为跳转目标的，例如上面示例代码中的标号“.L1\"；间接跳转的写法是 \"\"后面跟一个操作数指示符 jmp *%rax，用寄存器 %rax 中的值作为跳转目标。 6.4，跳转指令的编码 略 6.5，用条件控制来实现条件分支 6.6，用条件传送来实现条件分支 实现条件操作的传统方法是通过使用控制的条件转移。当条件满足时，程序沿着一条路径执行，而当条件不满足时，就走另一条路径。这种机制虽然简单通用，但是在现代处理器上，它可能会非常低效。 为了理解为什么教育条件数据传送的代码会比基于条件控制转移的代码性能要好，我们必须了解一些关于现代处理器如何运行的知识。处理器是通过流水线（pipelining）来获得高性能，在流水线中，一条指令的处理要经过一系列的阶段，每个阶段执行所需操作的一小部分（例如，从内存取指令、确定指令类型、从内存读数据、执行算术运算、向内存写数据，已经更新程序计数器）。这种方法通过重叠连续指令的步骤获得高性能，例如在取一条指令的同事，执行它前面一条指令的算术运算。要做到这一点，要求事先确定要执行的指令序列，这样才能保持流水线中充满了待执行的指令。 同条件跳转不同，使用条件传送指令，处理器无需预测测试的结果就可以执行条件传送。处理器只是读源值（可能从内存中），检查条件码，然后要么更新目的寄存器，要么保持不变。条件传送指令类如图 3-18 所示。 6.7，循环 C 语言提供了多种循环结构，如 do-while、while 和 for。汇编中没有直接的循环指令存在，但可以用条件测试和跳转指令组合起来实现循环的效果。GCC 和其他汇编器产生的循环代码主要基于两种基本的循环模式：do-while 循环和 while 循环。这里以给出一个函数实现 $n!$，其 C 代码、goto 代码和汇编代码如下。 long fact_do(long n) n in %rdi fact_do: mov1 $1, %eax Set result = 1 .L2: imulq %rdi, %rax Compute result *= n subq $1, %rdi Decrement n cmpq $1, %rdi Copare n:1 jg .L2 if >, goto loop rep; ret Return 7，过程 过程是软件中一种很重要的抽象，不同编程语言中，过程的形式多样：函数（function）、方法（method）、子例程（subroutine）、处理函数（handler）等等，但是它们都有一些共同的特性。 编程语言过程调用机制的一个关键特性在于使用了栈数据结构提供的先进后出的内存管理原则。 8，数组的分配和访问 数据结构-数组的定义和使用参考这篇文章常见数据结构-数组。 8.1，基本原则 x86-64 的内存引用指令可以用来简化数组访问。例如，假设 E 是一个 int 型的数组，而我们想计算 $E[i]$，在此，E 的地址存放在寄存器 %rdx 中，而 i 存放在寄存器 %rcx 中，指令 movl (%rdx, %rcx, 4), %eax 会执行地址计算 $x_{E} + 4i$,读取这个内存位置的值，并将结果存放到寄存器 %eax 中。 8.2，指针运算 单操作数操作符 & 和 * 可以产生指针和间接引用指针。 8.3，嵌套到数组（多维数组） 要访问多维数组元素，编译器会以数组起始为基地址，（可能需要经过伸缩的）偏移量为索引，产生计算期望的元素的偏移量，然后使用某种 MOV 指令。对于一个声明如下的二维数组: T D[R][C]; // T 是数据类型 它的数组元素 D[i][j] 的内存地址为 $\\&D[i][j] = x_{D} + L(C\\cdot i + j)$。 这里，$L$是数据类型 $T$以字节为单位的大小。以 $5\\times 3$的整形数组 A 为例，假设数组起始地址、行列索引 $x_A$、$i$和$j$分别在寄存器 %rdi、%rsi 和 %rdx 中，然后可以用下面的代码将数组元素 A[i][j] 复制到寄存器 %eax 中： A in %rdi, i in %rsi, and j in %rdx leaq (%rsi, %rsi, 2), %rax Compute 3i leaq (rdi, %rax, 4), %rax Compute x_A + 12i movq (rax, %rdx, 4), %eax Read from M[x_A + 12i + 4j] 上面这段代码计算元素的地址为 $x_A + 12i + 4j = x_A + 4(3i + j)$，使用了 x86-64 地址运算的伸缩和加法特性。 9，异数的数据结构 略 10，在机器级程序中将控制与数据结合起来 略 11，浮点代码 略 参考资料 《深入理解操作系统第三版-第3章》 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/Linux系统/Linux基础-学会使用命令帮助.html":{"url":"1-computer_basics/Linux系统/Linux基础-学会使用命令帮助.html","title":"Linux 基础-学会使用命令帮助","keywords":"","body":"Linux 基础-学会使用命令帮助 概述 Linux 命令及其参数繁多，大多数人都是无法记住全部功能和具体参数意思的。在 linux 终端，面对命令不知道怎么用，或不记得命令的拼写及参数时，我们需要求助于系统的帮助文档； linux 系统内置的帮助文档很详细，通常能解决我们的问题，我们需要掌握如何正确的去使用它们。 需要知道某个命令的简要说明，可以使用 whatis；而更详细的介绍，则可用 info 命令； 在只记得部分命令关键字的场合，我们可通过 man -k 来搜索； 查看命令在哪个位置，我们需要使用 which； 而对于命令的具体参数及使用方法，我们需要用到强大的 man ； 使用 whatis 使用方法如下： $ whatis ls # 查看 ls 命令的简要说明 ls (1) - list directory contents $ info ls # 查看 ls 命令的详细说明，会进入一个窗口内，按 q 退出 File: coreutils.info, Node: ls invocation, Next: dir invocation, Up: Directory listing 10.1 'ls': List directory contents The 'ls' program lists information about files (of any type, including directories). Options and file arguments can be intermixed arbitrarily, as usual. ... 省略 使用 man 查看命令 cp 的说明文档。 $ man cp # 查看 cp 命令的说明文档，主要是命令的使用方法及具体参数意思 CP(1) User Commands CP(1) NAME cp - copy files and directories ... 省略 在 man 的帮助手册中，将帮助文档分为了 9 个类别，对于有的关键字可能存在多个类别中， 我们就需要指定特定的类别来查看；（一般我们查询的 bash 命令，归类在1类中）；如我们常用的 printf 命令在分类 1 和分类 3 中都有(CentOS 系统例外)；分类 1 中的页面是命令操作及可执行文件的帮助；而3是常用函数库说明；如果我们想看的是 C 语言中 printf 的用法，可以指定查看分类 3 的帮助： $man 3 printf man 页面所属的分类标识(常用的是分类 1 和分类 3 ) (1)、用户可以操作的命令或者是可执行文件 (2)、系统核心可调用的函数与工具等 (3)、一些常用的函数与数据库 (4)、设备文件的说明 (5)、设置文件或者某些文件的格式 (6)、游戏 (7)、惯例与协议等。例如Linux标准文件系统、网络协议、ASCⅡ，码等说明内容 (8)、系统管理员可用的管理条令 (9)、与内核有关的文件 查看命令程序路径 which 查看程序的 binary 文件所在路径，可用 which 命令。 $ which ls # 查看 ping 程序(命令)的 binary 文件所在路径 /bin/ls $ cd /bin;ls 查看程序的搜索路径： $ whereis ls ls: /bin/ls /usr/share/man/man1/ls.1.gz 当系统中安装了同一软件的多个版本时，不确定使用的是哪个版本时，这个命令就能派上用场。 总结 本文总共讲解了 whatis info man which whereis 五个帮助命令的使用，Linux 命令的熟练使用需要我们在项目中多加实践、思考和总结。 参考资料 《Linux基础》 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/Linux系统/Linux基础-新手必备命令.html":{"url":"1-computer_basics/Linux系统/Linux基础-新手必备命令.html","title":"Linux 基础-新手必备命令","keywords":"","body":"Linux 基础-新手必备命令 概述 常见执行 Linux 命令的格式是这样的： 命令名称 [命令参数] [命令对象] 注意，命令名称、命令参数、命令对象之间请用空格键分隔。 命令对象一般是指要处理的文件、目录、用户等资源，而命令参数可以用长格式（完整的选项名称），也可以用短格式（单个字母的缩写），两者分别用 -- 与 - 作为前缀。 系统工作 echo：用于在 shell 编程中打印 shell 变量的值，或者直接输出指定的字符串。 date：显示或设置系统时间与日期。 reboot：重新启动正在运行的 Linux 操作系统。 poweroff：关闭计算机操作系统并且切断系统电源。 wget：用来从指定的 URL下载文件。wget 非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性，如果是由于网络的原因下载失败，wget 会不断的尝试，直到整个文件下载完毕。 ps：将某个时间点的进程运作情况撷取下来，可以搭配 kill 指令随时中断、删除不必要的程序。ps 命令可以查看进程运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等情况。使用 ps -l 则仅列出与你的操作环境 ( bash) 有关的进程而已；使用 ps aux 观察系统所有进程。 top：动态观察进程的变化。 pstree：pstree -A 列出目前系统上面所有的进程树的相关性。 pidof：查找指定名称的进程的进程号 id 号。 kill：删除执行中的程序或工作，后面必须要加上 PID (或者是 job number)，用法：killall -signal 指令名称/PID。kill 可将指定的信息送至程序，预设的信息为 SIGTERM(15),可将指定程序终止，若仍无法终止该程序，可使用 SIGKILL(9) 信息尝试强制删除程序。程序或工作的编号可利用 ps 指令或 job 指令查看。 系统状态检测 ifconfig：于配置和显示 Linux 内核中网络接口的网络参数。 uname：打印当前系统相关信息（内核版本号、硬件架构、主机名称和操作系统类型等），-a 或 --all：显示全部的信息。 uptime：打印系统总共运行了多长时间和系统的平均负载。uptime 命令可以显示的信息显示依次为：现在时间、系统已经运行了多长时间、目前有多少登陆用户、系统在过去的1分钟、5分钟和15分钟内的平均负载。 free：显示当前系统未使用的和已使用的内存数目，还可以显示被内核使用的内存缓冲区，-m：以MB为单位显示内存使用情况。 who：显示目前登录系统的用户信息。执行 who 命令可得知目前有那些用户登入系统，单独执行 who命令会列出登入帐号，使用的终端机，登入时间以及从何处登入或正在使用哪个 X 显示器。 last：显示用户最近登录信息。单独执行 last 命令，它会读取 /var/log/wtmp 的文件，并把该给文件的内容记录的登入系统的用户名单全部显示出来。 history：显示指定数目的指令命令，读取历史命令文件中的目录到历史命令缓冲区和将历史命令缓冲区中的目录写入命令文件。 sosreport 命令：收集并打包诊断和支持数据 文件与目录管理 pwd 命令：以绝对路径的方式显示用户当前工作目录。 cd 命令：切换工作目录至 dirname。 其中 dirName 表示法可为绝对路径或相对路径。~ 也表示为 home directory 的意思，.则是表示目前所在的目录，.. 则表示目前目录位置的上一层目录。 cp, rm, mv：复制、删除与移动文件或目录 。 ls：显示文件的文件/目录的名字与相关属性。-l 参数：长数据串行出，包含文件的属性与权限等等数据 (常用)。 touch：有两个功能：一是用于把已存在文件的时间标签更新为系统当前的时间（默认方式），它们的数据将原封不动地保留下来；二是用来创建新的空文件。 file：用来探测给定文件的类型。file 命令对文件的检查分为文件系统、魔法幻数检查和语言检查 3 个过程 文件内容查阅与编辑 文件内容查阅命令如下： cat：由第一行开始显示文件内容 tac：从最后一行开始显示，可以看出 tac 是 cat 的倒着写！ nl：显示的时候，顺道输出行号！ more：一页一页的显示文件内容 less：与 more 类似，但是比 more 更好的是，他可以往前翻页！ head：只看头几行 tail：只看尾巴几行 od：以二进制的方式读取文件内容！ 文件内容查阅命令总结： 直接查阅一个文件的内容可以使用 cat/tac/nl 这几个命令； 需要翻页检视文件内容使用 more/less 命令； 取出文件前面几行 (head) 或取出后面几行 (tail)文字的功能使用 head 和 tail 命令，注意 head 与 tail 都是以『行』为单位来进行数据撷取的； 文本内容编辑命令如下： tr：可以用来删除一段讯息当中的文字，或者是进行文字讯息的替换。 wc：可以帮我们计算输出的讯息的整体数据。 stat：用于显示文件的状态信息。stat 命令的输出信息比 ls 命令的输出信息要更详细 cut：可以将一段讯息的某一段给他『切』出来，处理的讯息是以『行』为单位。 diff：在最简单的情况下，比较给定的两个文件的不同。如果使用 “-” 代替“文件”参数，则要比较的内容将来自标准输入。diff 命令是以逐行的方式，比较文本文件的异同处。如果该命令指定进行目录的比较，则将会比较该目录中具有相同文件名的文件，而不会对其子目录文件进行任何比较操作。 打包压缩与搜索 tar：利用 tar 命令可以把一大堆的文件和目录全部打包成一个文件，这对于备份文件或将几个文件组合成为一个文件以便于网络传输是非常有用的。注意打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。为什么要区分这两个概念呢？这源于 Linux 中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar 命令），然后再用压缩程序进行压缩（gzip bzip2 命令）。 grep：（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）一种强大的文本搜索工具，能够使用正则表达式搜索文本，并把匹配的行打印出来。grep 它是分析一行信息， 若当中有我们所需要的信息，就将该行拿出来。用法：grep [-acinv] [--color=auto] '搜寻字符串' filename。 which：查找命令的完整文件名。用法：which [-a] command，a : 将所有由 PATH 目录中可以找到的指令均列出，而不止第一个被找到的指令名称。find 命令是根据『PATH』这个环境变量所规范的路径，去搜寻命令的完整文件名。 find：用来在指定目录下查找文件。任何位于参数之前的字符串都将被视为欲查找的目录名。如果使用该命令时，不设置任何参数，则 find 命令将在当前目录下查找子目录与文件。并且将查找到的子目录和文件全部进行显示。用法举例：在 /home 目录及其子目录下查找以 .txt 结尾的文件名 find /home -name \"*.txt\"。 whereis/locate：whereis 只找系统中某些特定目录底下的文件而已， locate则是利用数据库来搜寻文件名，两者速度更快， 但没有实际搜寻硬盘内的文件系统状态。 常见命令图解 这个思维导图记录了常见命令，有利于索引，来源Linux基础命令（01）【Linux基础命令、ip查看、目录结构、网络映射配置】 参考资料 新手linux命令必须掌握命令 鸟哥的Linux私房菜 基础篇 第四版 Linux基础命令（01）【Linux基础命令、ip查看、目录结构、网络映射配置】 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/Linux系统/Linux基础-文件权限与属性.html":{"url":"1-computer_basics/Linux系统/Linux基础-文件权限与属性.html","title":"Linux 基础-文件权限与属性","keywords":"","body":" 一，文件类型 1.1，概述 1.2，正规文件(regular file) 1.3，目录(directory) 1.4，链接文件(link) 1.5，设备与装置文件(device) 1.6，资料接口文件(sockets)： 1.7，数据输送文件(FIFO, pipe)： 1.8，文件拓展名 二，文件属性与权限 2.1，Linux 文件属性 2.2，Linux 文件权限 2.3，如何改变文件属性和权限 2.4，文件与目录的权限意义 三，Linux 文件属性与权限总结 四，参考资料 Linux 系统由 Linux 内核、shell、文件系统和第三方应用软件组成。Linux 文件权限与属性是学习 Linux 系统的一个重要关卡，必须理解这个部分内容的概念。 一，文件类型 1.1，概述 一个基本概念：任何装置在 Linux 下都是文件，数据沟通的接口也有专属的文件在负责，Linux 的文件种类繁多，常用的是一般文件(-)与目录文件(d)。 注意：Linux 文件类型和文件的文件名所代表的意义是两个不同的概念，在 linux 中文件类型与文件扩展名没有关系。它不像 Windows 那样是依靠文件后缀名来区分文件类型的，在 linux 中文件名只是为了方便操作而的取得名字。Linux 文件类型常见的有：普通文件、目录、字符设备文件、块设备文件、符号链接文件等。 查看文件类型方法，使用 ls -al 命令列出的信息中第一栏十个字符中，第一个字符为文件的类型。 1.2，正规文件(regular file) 就是一般我们在进行存取的类型的文件，在由 ls -al 所显示出来的属性方面，第一个字符为 -，例如 -rwxrwxrwx。另外，依照文件的内容，又大略可以分为： 纯文本档(ASCII)：Linux 系统中最为常见的一种文件类型，称为纯文本是因为文件内容为人类可以直接读取到的数据，例如数字、字母等。 二进制文件(binary)：Linux 系统仅认识且可以执行的二进制文件 binary file，Linux 系统中可执行的文件就是这种类型，例如 cat 就是一个 binary file。 数据格式文件(data)： 有些程序在运作的过程当中会读取某些特定格式的文件，那些特定格式的文件可以被称为数据文件 (data file)。举例来说，我们的 Linux 在使用者登入时，都会将登录的数据记录在 /var/log/wtmp 文件内，该文件是一个 data file，它能够通过 last 这个指令读出来，但是使用 cat 命令读取时会读出乱码，因为他是属于一种特殊格式的文件。 1.3，目录(directory) 第一个属性为 d，例如 drwx------。 1.4，链接文件(link) 类似 Windows 系统下的快捷键，第一个属性为 l，例如 lrwxrwxrwx。 1.5，设备与装置文件(device) 与系统周边设备及存储相关的一些文件，通常集中在 /dev 目录下，一般分为两种： 区块(block)设备类型：就是一些储存数据， 以提供系统随机存取的接口设备，比如硬盘设备，第一个属性为 b。 字符(character)设备文件：一些串行端口的接口设备，例如键盘、鼠标、摄像头等。这些设备的特性是**一次性读取\"，不能够截断输出，第一个属性为 c。 1.6，资料接口文件(sockets)： 被用于网络上的数据连接了。我们可以启动一个程序来监听客户端的要求， 而客户端就可以透过这个 socket 来进行数据的沟通了。第一个属性为 s，最常在 /run 或 /tmp 这些目录中可以看到这种文件类型。 1.7，数据输送文件(FIFO, pipe)： FIFO 也是一种特殊的文件类型，他主要的目的在解决多个程序同时存取一个文件所造成的错误问题。FIFO 是 first-in-first-out 的缩写。第一个属性为 p。 1.8，文件拓展名 注意 Linux 系统的文件是没有所谓的拓展名的。 一个 Linux 文件能不能被执行，与他的 ls -al 展示的文件信息的第一栏的十个属性有关， 与文件名根本一点关系也没有，这与 Windows 不同，在Linux 系统下，只要用户的权限有 x，文件就可以被执行。拥有了 x 权限，表示文件可以被执行，但是只有具有可执行的程序代码文件才能被执行，文本等文件即使有权限也是不能执行成功的。 二，文件属性与权限 2.1，Linux 文件属性 ls -al 命令：列出所有的文件详细的权限与属性 (包含隐藏文件-文件名第一个字符为『 . 』的文件)。 ls -al 展示的文件属性信息如下： 第一列代表这个文件的类型与权限(permission)；第一列的第一个字符代表这个文件是『目录、 文件或链接文件等等文件类型』： 当为 d 则是目录，例如上图文件名为『.config』的那一行； 当为 - 则是文件，例如上图文件名为『initial-setup-ks.cfg』那一行； 若是 l 则表示为链接文件(link file)； 若是 b 则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是 c 则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置)。 第二列表示有多少文件名连结到此节点(i-node)； 第三列表示这个文件(或目录)的『拥有者账号』； 第四列表示这个文件的所属群组； 第五列为这个文件的容量大小，默认单位为 bytes； 第六列为这个文件的建档日期或者是最近的修改日期； 第七列为这个文件的文件名。 ls -al 命令展示的文件属性的七个字段的意义很重要，必须理解和熟记，这是掌握 Linux 文件权限与目录管理的基础知识。 2.2，Linux 文件权限 Linux 文件的基本权限就有九个，分别是 owner/group/others 三种身份各有自己的 read/write/execute 权限。在 Linux 中，对于文件的权限（rwx），分为三部分，第一部分是该文件的拥有者所拥有的权限，第二部分是该文件所在用户组的用户所拥有的权限，最后一部分是其他用户所拥有的权限。 每个文件/文件夹的属性都用 10 个字符表示，第一个字符如果是 d：表示文件夹，如果是 -：表示文件。用（rwx）表示文件权限，其中r: 可读（4），w: 可写（2），x: 可执行（1）。举例，drwxr-xrw- 表示文件夹拥有者拥有可读可写可执行的权限，用户所在用户组和其他用户无任何权限，命令的详细解释如下。 从第二到第四位 (rwx) 是文件所有者的权限：可读、可写、可执行。 从第五到第七位 (r-x) 文件夹用户拥有者所在组的权限：可读、可执行。 从第八位到第十位 (rw-) 其他人对这个文件夹操作的权限.：可读、可写。 Linux 系统查看文件/目录权限示例，如下： root@5b84c8f05303:/data/project# ls -al total 16 drwx------ 4 1018 1002 4096 Jul 20 02:59 . drwx------ 8 1018 1002 4096 Jul 20 02:57 .. drwx------ 6 1018 1002 4096 Jul 20 02:57 DeepPruner -rw-r--r-- 1 root root 0 Jul 20 02:59 demo.py drwx------ 8 1018 1002 4096 Jul 20 02:57 nn_tools-master 2.3，如何改变文件属性和权限 Linux/Unix 是多人多工操作系统，所有的文件皆有拥有者。利用 chown 命令可以改变文件的拥有者(用户)和群组，用户可以是用户名或者用户 ID，组可以是组名或者组 ID。注意，普通用户不能将自己的文件改变成其他的拥有者，其操作权限一般为管理员(**root** 用户)；同时用户必须是已经存在系统中的账号，也就是在 /etc/passwd 这个文件中有纪录的用户名称才能改变。 三个常用于群组、拥有者、各种身份的权限之修改的命令，如下所示： chown : 改变文件的拥有者。如递归子目录修改命令： chown -R user_name folder/ chgrp : 改变文件所属群组。 chmod : 改变文件读、写、执行权限(权限分数 r:4 w:2 x:1)属性。如增加脚本可执行权限命令： chmod a+x myscript 。 添加用户和用户组的命令：adduser groupadd，其简单用法如下所示： adduser harley：新建 harley 用户 passwd harley：给 harley 用户设置密码 groupadd harley：新建 harley工作组 useradd -g harley harley：新建 harley 用户并增加到 harley 工作组 改变所属群组 chgrp/chown/chmod 命令的用法如下： $ chgrp [-R] group dirname/filename # -R : 进行递归(recursive)的持续变更，亦即连同子目录下的所有文件、目录都更新成为这个群组之意。 $ chown [-R] 账号名称 文件或目录 $ chown [-R] 账号名称:组名 文件或目录 $ chmod [-R] xyz 文件或目录 # xyz : 数字类型的权限属性分数值， 为 rwx 属性数值的相加。 chgrp 范例：将 test 的工作组从 harley 改为 root（前提是当前用户切换为 root 了，否则会提示权限不足的错误）： chmod 范例：将 .bashrc 这个文件所有的权限都设定启用。 root@17c30d837aba:~# ls -al .bashrc -rw-r--r-- 1 root root 3479 Jan 8 03:14 .bashrc root@17c30d837aba:~# chmod 777 .bashrc root@17c30d837aba:~# ls -al .bashrc -rwxrwxrwx 1 root root 3479 Jan 8 03:14 .bashrc 2.4，文件与目录的权限意义 我们可以利用 ls -al 命令查看文件属性及权限，已知了 Linux 系统内文件的三种身份(拥有者、群组与其他人)，每种身份都有三种权限(rwx)，再使用 chown, chgrp, chmod 去修改这些权限与属性。 文件是实际含有数据的地方，包括一般文本文件、数据库内容文件、二进制可执行文件(binary program)等等。因此，权限对于文件来说，他的意义是这样的： r (read)：可读取此一文件的实际内容，如读取文本文件的文字内容等； w (write)：可以编辑、新增或者是修改该文件的内容(但不含删除该文件)； x (eXecute)：该文件具有可以被系统执行的权限。Linux 底下， 文件是否能被执行，是由 x 这个权限来决定的！跟文件名是没有绝对的关系，即使是能够执行成功的 .sh 脚本文件，如果没有 x 可执行权限，文件也不能被执行。 目录主要的内容是记录文件名列表，文件名与目录有强烈的关连。对一般文件来说，rwx 主要是针对『文件的内容』来设计权限，对目录来说， rwx 则是针对『目录内的文件名列表』来设计权限。权限对文件和目录重要性的理解汇总成如下表格： 三，Linux 文件属性与权限总结 利用 ls -al 命令查看文件属性及权限，已知了 Linux 系统内文件的三种身份(文件拥有者、文件所属群组与其他用户)，每种身份都有四种权限(rwxs)。可以使用 chown, chgrp, chmod 去修改这些权限与属性。文件是实际含有数据的地方，包括一般文本文件、数据库内容文件、二进制可执行文件(binary program)等等。 四，参考资料 《鸟哥的Linux私房菜 基础篇 第四版》 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/Linux系统/Linux基础-文件及目录管理.html":{"url":"1-computer_basics/Linux系统/Linux基础-文件及目录管理.html","title":"Linux 基础-文件及目录管理","keywords":"","body":"Linux 基础-文件及目录管理 本文大部分内容参看 《Linux基础》一书，根据自己的工程经验和理解加以修改、拓展和优化形成了本篇博客，不适合 Linux 纯小白，适合有一定基础的开发者阅读。 一，概述 在 Linux 中一切皆文件。文件管理主要是涉及文件/目录的创建、删除、移动、复制和查询，有mkdir/rm/mv/cp/find 等命令。其中 find 文件查询命令较为复杂，参数丰富，功能十分强大；查看文件内容是一个比较大的话题，文本处理也有很多工具供我们使用，本文涉及到这两部分的内容只是点到为止，没有详细讲解。另外给文件创建一个别名，我们需要用到 ln，使用这个别名和使用原文件是相同的效果。 二，文件及目录常见操作 2.1，创建、删除、移动和复制 创建和删除命令的常用用法如下： 创建目录：mkdir 删除文件：rm file(删除目录 rm -r) 移动指定文件到目标目录中：mv source_file(文件) dest_directory(目录) 复制：cp(复制目录 cp -r) 这些命令的常用和复杂例子程序如下 $ find ./ | wc -l # 查看当前目录下所有文件个数(包括子目录) 14995 $ cp –r test/ newtest # 使用指令 cp 将当前目录 test/ 下的所有文件复制到新目录 newtest 下 $ mv test.txt demo.txt # 将文件 test.txt 改名为 demo.txt 2.2，目录切换 切换到上一个工作目录： cd - 切换到 home 目录： cd or cd ~ 显示当前路径: pwd 更改当前工作路径为 path: $ cd path 2.3，列出目录内容 显示当前目录下的文件及文件属性：ls 按时间排序，以列表的方式显示目录项：ls -lrt ls 命令部分参数解释如下： -a：显示所有文件及目录 (. 开头的隐藏文件也会列出) -l：除文件名称外，亦将文件型态、权限、拥有者、文件大小等资讯详细列出 -r：将文件以相反次序显示(原定依英文字母次序) -t： 将文件依建立时间之先后次序列出 常用例子如下： $ pwd / $ ls -al # 列出根目录下所有的文件及文件类型、大小等资讯 total 104 drwxr-xr-x 1 root root 4096 Dec 24 01:24 . drwxr-xr-x 1 root root 4096 Dec 24 01:24 .. drwxrwxrwx 11 1019 1002 4096 Jan 13 09:34 data drwxr-xr-x 15 root root 4600 Dec 24 01:24 dev drwxr-xr-x 1 root root 4096 Jan 8 03:15 etc drwxr-xr-x 1 root root 4096 Jan 11 05:49 home drwxr-xr-x 1 root root 4096 Dec 23 01:15 lib drwxr-xr-x 2 root root 4096 Dec 23 01:15 lib32 ... 省略 2.4，查找目录或者文件 find/locate 1，查找文件或目录 $ find ./ -name \"cali_bin*\" | xargs file # 查找当前目录下文件名含有 cali_bin 字符串的文件 ./classifynet_calib_set/cali_bin.txt: ASCII text ./calib_set/cali_bin.txt: ASCII text ./cali_bin.txt: ASCII text 2，查找目标文件夹中是否含有 obj 文件: $ find ./ -name '*.o' find 是实时查找，如果需要更快的查询，可试试 locate；locate 会为文件系统建立索引数据库，如果有文件更新，需要定期执行更新命令来更新索引库。 $ locate string # 寻找包含有 string 的路径 2.5，查看及搜索文件内容 1，查看文件内容命令：cat vi head tail more。 $ cat -n # 显示时同时显示行号 $ ls -al | more # 按页显示列表内容 $ head -1 filename # 显示文件内容第一行 $ diff file1 file1 # 比较两个文件间的差别 2，使用 egrep 查询文件内容: $ egrep \"ls\" log.txt # 查找 log.txt 文件中包含 ls 字符串的行内容 -rw-r--r-- 1 root root 2009 Jan 13 06:56 ls.txt 三，总结 利用 ls -al 命令查看文件属性及权限，已知了 Linux 系统内文件的三种身份(文件拥有者、文件所属群组与其他用户)，每种身份都有四种权限(rwxs)。可以使用 chown, chgrp, chmod 去修改这些权限与属性。文件是实际含有数据的地方，包括一般文本文件、数据库内容文件、二进制可执行文件(binary program)等等。 文件管理，目录的创建、删除、查询、管理: mkdir rm mv cp 文件的查询和检索命令： find locate 查看文件内容命令：cat vi tail more 管道和重定向命令： ; | && > 四，参考资料 《Linux基础》 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/Linux系统/Linux基础-文本处理命令.html":{"url":"1-computer_basics/Linux系统/Linux基础-文本处理命令.html","title":"Linux 基础-文本处理命令","keywords":"","body":" 概述 find 文件查找 grep 文本搜索 参考资料 概述 Linux 下使用 Shell 处理文本时最常用的工具有： find、grep、xargs、sort、uniq、tr、cut、paste、wc、sed、awk。 find 文件查找 man 文档给出的 find 命令的一般形式为： find [-H] [-L] [-P] [-D debugopts] [-Olevel] [starting-point...] [expression] 这对于大部分人来说都太复杂了，[-H] [-L] [-P] [-D debugopts] [-Olevel] 这几个选项并不常用，find 命令的常用形式可以简化为： $ find [PATH] [option] [action] 1，根据文件或者正则表达式进行匹配 $ find . # 查找当前目录及子目录下所有文件及文件夹 $ find /data -name \"*.txt\" # 在 /data 目录及子目录下查找以 .txt 结尾的文件名 $ find . \\( -name \"*.txt\" -o -name \"*.pdf\" \\) # 当前目录及子目录下查找所有以 .txt 和 .pdf 结尾的文件 $ find . -maxdepth 1 -type d # 查找当前目录下所有的子目录 $ find . -maxdepth 1 -regex \".*\\.txt$\" # 基于正则表达式匹配当前目录下的所有以 .txt 结尾的文件 ./multi_classifynet_infer_ret.txt ./cali_left_img.txt ... 省略 2，根据文件类型进行搜索 find . -type 类型参数，f 普通文件，l 符号连接，d 目录，c 字符设备，b 块设备，s 套接字，p Fifo $ find . -maxdepth 1 -type d # 查找当前目录下的所有子目录 3，基于目录深度搜索 $ find . maxdepth 3 -type f # 目录向下最大深度限制 3 4，根据文件时间戳进行搜索 find . -type -f 时间戳参数。与时间有关的选项：共有 -atime, -ctime 与 -mtime，以 -mtime 说明 -mtime n ： n 为数字，意义为在 n 天之前的『一天之内』被更改过内容的文件； -mtime +n ：列出在 n 天之前(不含 n 天本身)被更改过内容的文件名； -mtime -n ：列出在 n 天之内(含 n 天本身)被更改过内容的文件名。 -newer file ： file 为一个存在的文件，列出比 file 还要新的文件名 $ find /etc -newer /etc/passwd # 寻找 /etc 底下的文件，如果文件日期比 /etc/passwd 新就列出 5，与文件权限及名称有关的参数： -name filename：搜寻文件名为 filename 的文件。 -size [+-]SIZE：搜寻比 SIZE 还要大(+)或小(-)的文件。 这个 SIZE 的规格有：c: 代表 byte， k: 代表 1024 bytes。所以，要找比 50KB还要大的文件，就是 -size +50k。 -type TYPE：搜寻文件的类型为 TYPE 的， 类型主要有：一般正规文件 (f), 装置文件 (b, c), 目录 (d), 连结档 (l), socket (s), 及 FIFO (p) 等属性。 -perm mode：搜寻文件权限『刚好等于』 mode 的文件， 这个 mode 为类似 chmod 的属性值， 举例来说， -rwxr-xr-x 的属性为 755。 -perm -mode：搜寻文件权限『必须要全部囊括 mode 的权限』的文件， 举例来说，我们要搜寻 -rwxr--r--，亦即 744 的文件，使用 -perm -744，但是当一个文件的权限为 -rwxr-xr-x ，亦即 755 时，也会被列出来，因为 -rwxr-xr-x 的属性已经包括了-rwxr--r-- 的属性了。 -perm /mode：搜寻文件权限『包含任一 mode 的权限』的文件， 举例来说，我们搜寻 -rwxr-xr-x ，亦即 -perm /755 时，但一个文件属性为 -rw-------也会被列出来，因为他有 -rw.... 的属性存在。范例： ```shell root@17c30d837aba:/data# find . -maxdepth 1 -perm 777 # 查找当前目录下文件权限刚好等于777 的文件 . ./honggaozhang ./demo.sh grep 文本搜索 grep 支持使用正则表达式搜索文本，并把匹配的行打印出来。grep 命令常见用法，在文件中搜索一个单词，命令会返回一个包含 “match_pattern” 的文本行：grep match_pattern file_name grep \"match_pattern\" file_name 常用参数 -o：只输出匹配的文本行，-v 只输出没有匹配的文本行 -c：统计文件中包含文本的次数： `grep -c “text” filename -n：打印匹配的行号 -i：搜索时忽略大小写 -l：只打印文件名 $ grep \"class\" . -R -n # 在多级目录中对文本递归搜索(程序员搜代码的最爱) $ grep -e \"class\" -e \"vitural\" file # 匹配多个模式 参考资料 【日常小记】linux中强大且常用命令：find、grep 鸟哥的Linux私房菜 基础篇 第四版 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/Linux系统/Linux基础-查看cpu、内存和环境等信息.html":{"url":"1-computer_basics/Linux系统/Linux基础-查看cpu、内存和环境等信息.html","title":"Linux 基础-查看cpu、内存和环境等信息","keywords":"","body":"Linux 基础-查看 cpu、内存和环境等信息 在使用 Linux 系统的过程中，我们经常需要查看系统、资源、网络、进程、用户等方面的信息，查看这些信息的常用命令值得了解和熟悉。 1，系统信息查看常用命令如下： lsb_release -a # 查看操作系统版本(裁剪版不一定支持) cat /etc/os-release # 查看操作系统版本 (适用于所有的linux，包括Redhat、SuSE、Debian等发行版，但是在debian下要安装lsb) cat /proc/cpuinfo # 查看CPU信息 hostname # 查看计算机名 lsusb -tv # 列出所有USB设备 env # 查看环境变量 2，资源信息查看常用命令如下： free -m # 查看内存使用量和交换区使用量（单位MB） df -h # 查看各分区使用情况 df -hT # 查看硬盘使用情况 du -sh # 查看指定目录的大小 uptime # 查看系统运行时间、用户数、负载 3，网络信息查看常用命令如下 ifconfig # 查看所有网络接口的属性 route -n # 查看路由表 4，进程信息查看常用命令如下 ps -ef # 查看所有进程 top # 实时显示进程状态 5，用户信息查看常用命令如下 w # 查看活动用户 id # 查看指定用户信息 last # 查看用户登录日志 cut -d: -f1 /etc/passwd # 查看系统所有用户 cut -d: -f1 /etc/group # 查看系统所有组 crontab -l # 查看当前用户的计划任务 更多命令及理解，参考此链接。 6，查看操作系统、框架、库以及工具版本命令汇总： cat /etc/os-release # 适合所有linux系统，查看操作系统版本，显示信息比较全 cat /etc/issue # 该命令适用于所有Linux系统，显示的版本信息较为简略，只有系统名称和对应版本号。 uname -a # 查看linux 内核 cat /proc/version # 查看linux 内核 nvcc -V # 查看 cuda 版本 cat /usr/local/cuda/version.txt # 没有安装 nvcc 条件用 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 # 查看cudnn版本 find / -name NvInferVersion.h && cat /usr/local/cuda-11.0/targets/x86_64-linux/include/NvInferVersion.h | grep NV_TENSORRT # 查看cudnn版本通用 gcc -v # 查看 gcc 版本 cmake -version # 查看 cmake 版本 pkg-config --modversion opencv # 查看 opencv 版本 ffmpeg -version # 查看 ffmpeg 版本 参考资料 怎么查看Linux服务器硬件信息，这些命令告诉你 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/Linux系统/Linux基础-查看进程命令ps和top.html":{"url":"1-computer_basics/Linux系统/Linux基础-查看进程命令ps和top.html","title":"Linux 基础-查看进程命令ps和top","keywords":"","body":" 1，使用 ps 命令找出 CPU 占用高的进程 2，通过 top 命令定位占用 cpu 高的进程 3，htop 系统监控与进程管理软件 4，参考资料 1，使用 ps 命令找出 CPU 占用高的进程 ps 是 进程状态 (process status) 的缩写，它能显示系统中活跃的/运行中的进程的信息。它提供了当前进程及其详细信息，诸如用户名、用户 ID、CPU 使用率、内存使用、进程启动日期时间、命令名等等的快照。只打印命令名字而不是命令的绝对路径，以运行下面的格式 ps 命令： ~$ ps -eo pid,ppid,%mem,%cpu,comm --sort=-%cpu | head 运行结果如下： 上面命令语句的各部分参数解释： ps：命令名字 -e：选择所有进程 -o：自定义输出格式 –sort=-%cpu：基于 CPU 使用率对输出结果排序 head：显示结果的前 10 行 PID：进程的 ID PPID：父进程的 ID %MEM：进程使用的 RAM 比例 %CPU：进程占用的 CPU 比例 Command：进程名字 2，通过 top 命令定位占用 cpu 高的进程 查看 cpu 占用最高进程（查看前3位）：top，然后按下 M（大写 M）。 查看内存占用最高进程：top，然后按下 P（大写 P ）。 可视化查看所有用户所有进程使用情况：ps axf。 在所有监控 Linux 系统性能的工具中，Linux 的 top 命令是最好的也是最知名的一个（htop 是其升级版）。top 命令提供了 Linux 系统运行中的进程的动态实时视图。它能显示系统的概览信息和 Linux 内核当前管理的进程列表。它显示了大量的系统信息，如 CPU 使用、内存使用、交换内存、运行的进程数、目前系统开机时间、系统负载、缓冲区大小、缓存大小、进程 PID 等等。默认情况下，top 命令的输出结果按 CPU 占用进行排序，每 5 秒中更新一次结果。 ps -ef # 查看所有进程 top # 实时显示进程状态 Linux 系统下执行 top 命令得到以下结果（第一列为进程的 PID，第二列为进程所属用户）： 上图各个参数的意义： PID：进程的ID USER：进程所有者 PR：进程的优先级别，越小越优先被执行 NInice：值 VIRT：进程占用的虚拟内存 RES：进程占用的物理内存 SHR：进程使用的共享内存 S：进程的状态。S表示休眠，R表示正在运行，Z表示僵死状态，N表示该进程优先值为负数 %CPU：进程占用CPU的使用率 %MEM：进程使用的物理内存和总内存的百分比 TIME+：该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值。 COMMAND：进程启动命令名称 通过上图可以看出排在一行的进程 PID 2438占用 cpu 过高，定位到了进程 id。如果只想观察 进程PID 2438的 CPU和内存以及负载情况，可以使用以下命令： top -p 2438 结果如下： 还可以通过 top 命令定位问题进程中每个线程占用 cpu 情况，如查看进程 PID 2438 的每一个线程占用 cpu 的情况，使用如下命令： top -p 2438 -H 结果如下（单线程，所以只显示一行）： 3，htop 系统监控与进程管理软件 与 top 只提供最消耗资源的进程列表不同，htop 提供所有进程的列表，并且使用彩色标识出处理器、swap 和内存状态。可以通过 htop 查看单个进程的线程，然后按 来进入 htop 的设置菜单。选择“设置”栏下面的“显示选项”，然后开启“树状视图”和“显示自定义线程名”选项。按 退出设置。 4，参考资料 线上linux系统故障排查之一：CPU使用率过高 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"1-computer_basics/Linux系统/Linux基础-查看和设置环境变量.html":{"url":"1-computer_basics/Linux系统/Linux基础-查看和设置环境变量.html","title":"Linux 基础-查看和设置环境变量","keywords":"","body":" 一，查看环境变量 二，环境变量类型 三，设置环境变量 四，参考资料 一，查看环境变量 在 Linux中，环境变量是一个很重要的概念。环境变量可以由系统、用户、Shell 以及其他程序来设定，其是保存在变量 PATH 中。环境变量是一个可以被赋值的字符串，赋值范围包括数字、文本、文件名、设备以及其他类型的数据。 值得一提的是，Linux 系统中环境变量的名称一般都是大写的，这是一种约定俗成的规范。 1，使用 echo 命令查看单个环境变量，例如：echo $PATH；使用 env 查看当前系统定义的所有环境变量；使用 set 查看所有本地定义的环境变量。查看 PATH 环境的实例如下： 使用 unset 删除指定的环境变量，set 也可以设置某个环境变量的值。清除环境变量的值用 unset 命令。如果未指定值，则该变量值将被设为 NULL。示例如下： $ export TEST=\"Test...\" # 增加一个环境变量 TEST $ env|grep TEST # 此命令有输入，证明环境变量 TEST 已经存在了 TEST=Test... unset TEST # 删除环境变量 TEST $ env|grep TEST # 此命令没有输出，证明环境变量 TEST 已经删除 2，C 程序调用环境变量函数 getenv()： 返回一个环境变量。 setenv()：设置一个环境变量。 unsetenv()： 清除一个环境变量。 二，环境变量类型 1，按照变量的生存周期划分，Linux 变量可分为两类： 永久的：需要修改配置文件，变量永久生效。 临时的：使用 export 命令声明即可，变量在关闭 shell 时失效。 2，按作用的范围分，在 Linux 中的变量，可以分为环境变量和本地变量： 环境变量：相当于全局变量，存在于所有的 Shell 中，具有继承性； 本地变量：相当于局部变量只存在当前 Shell 中，本地变量包含环境变量，非环境变量不具有继承性。 环境变量名称都是大写，常用的环境变量意义如下所示。 PATH：决定了 shell 将到哪些目录中寻找命令或程序 HOME：当前用户主目录 HISTSIZE：历史记录数 LOGNAME：当前用户的登录名 HOSTNAME：指主机的名称 SHELL：当前用户 Shell 类型 LANGUGE：语言相关的环境变量，多语言可以修改此环境变量 MAIL：当前用户的邮件存放目录 PS1：基本提示符，对于 root 用户是 #，对于普通用户是 $ 三，设置环境变量 设置环境有多种用途，比如配置交叉编译工具链的时候一般需要指定编译工具的路径，此时就需要设置环境变量。 在 Linux 中设置环境变量有三种方法： 1，所有用户永久添加环境变量: vi /etc/profile，在 /etc/profile 文件中添加变量。 vi /etc/profile # 通过这种方式，在关闭 xshell后，添加的环境变量不生效 文件末尾添加：export PATH=\"/usr/local/cuda/lib64:$PATH\" source /etc/profile # 激活后，环境变量才可永久生效 2，当前用户永久添加环境变量: vi ~/.bash_profile，在用户目录下的 ~/.bash_profile 文件中添加变量。 vim ~/.bashrc # 编辑 .bashrc 文件，在关闭 xshell 后，添加的环境变量仍然生效 文件末尾添加: export PATH=\"/usr/local/cuda/lib64:$PATH\" source ~/.bashrc 3，临时添加环境变量 PATH: 可通过 export 命令，如运行命令 export PATH=/usr/local/cuda/lib64:$PATH，将 /usr/local/cuda/lib64 目录临时添加到环境变量中。查看是否已经设置好，可用命令 export 查看。 前面两种方法可以通过 echo $PATH 命令查看终端打印结果是否有添加的路径，来确认已经设置好环境变量。 四，参考资料 Linux环境变量总结 在Linux里设置环境变量的方法（export PATH） Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"2-programming_language/":{"url":"2-programming_language/","title":"2. 编程开发","keywords":"","body":"前言 本目录内容旨在分享cv算法工程师经常需要使用到的 c/c++、python 和 shell 编程语言的知识总结和学习笔记。 cpp C++ Primer 学习笔记 C++ 基础-资源管理:堆栈与RAII C++ 日期和时间编程总结 C++ 编程基础 python Python3 编程面试题 shell Shell 语法基础 参考资料 《C++ Primer 第五版》 https://zh.cppreference.com/w/%E9%A6%96%E9%A1%B5 《Python3 教程-廖雪峰》 《菜鸟教程-shell》 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"2-programming_language/python3/Python3编程面试题.html":{"url":"2-programming_language/python3/Python3编程面试题.html","title":"Python3编程面试题","keywords":"","body":" Python global 语句的作用 lambda 匿名函数好处 Python 错误处理 Python 内置错误类型 简述 any() 和 all() 方法 Python 中什么元素为假？ 提高 Python 运行效率的方法 Python 单例模式 为什么 Python 不提供函数重载 实例方法/静态方法/类方法 __new__和 __init __方法的区别 Python 的函数参数传递 Python 实现对函参做类型检查 为什么说 Python 是动态语言 Python 装饰器理解 map 与 reduce 函数用法解释 Python 深拷贝、浅拷贝区别 Python 继承多态理解 Python 面向对象的原则 参考资料 Python global 语句的作用 在编写程序的时候，如果想要改变(重新赋值)函数外部的变量，并且这个变量会作用于许多函数中，就需要告诉 Python 程序这个变量的作用域是全局变量，global 语句可以实现定义全局变量的作用。 lambda 匿名函数好处 精简代码，lambda省去了定义函数，map 省去了写 for 循环过程: str_1 = [\"中国\", \"美国\", \"法国\", \"\", \"\", \"英国\"] res = list(map(lambda x: \"填充值\" if x==\"\" else x, str_1)) print(res) # ['中国', '美国', '法国', '填充值', '填充值', '英国'] Python 错误处理 和其他高级语言一样，Python 也内置了一套try...except...finally... 的错误处理机制。 当我们认为某些代码可能会出错时，就可以用 try 来运行这段代码，如果执行出错，则后续代码不会继续执行，而是直接跳转至跳转至错误处理代码，即 except 语句块，执行完 except 后，如果有 finally 语句块，则执行。至此，执行完毕。跳转至错误处理代码， Python 内置错误类型 IOError：输入输出异常 AttributeError：试图访问一个对象没有的属性 ImportError：无法引入模块或包，基本是路径问题 IndentationError：语法错误，代码没有正确的对齐 IndexError：下标索引超出序列边界 KeyError: 试图访问你字典里不存在的键 SyntaxError: Python 代码逻辑语法出错，不能执行 NameError: 使用一个还未赋予对象的变量 简述 any() 和 all() 方法 any(): 只要迭代器中有一个元素为真就为真; all(): 迭代器中所有的判断项返回都是真，结果才为真. Python 中什么元素为假？ 答案：（0，空字符串，空列表、空字典、空元组、None, False） 提高 Python 运行效率的方法 使用生成器，因为可以节约大量内存; 循环代码优化，避免过多重复代码的执行; 核心模块用 Cython PyPy 等，提高效率; 多进程、多线程、协程; 多个 if elif 条件判断，可以把最有可能先发生的条件放到前面写，这样可以减少程序判断的次数，提高效率。 Python 单例模式 为什么 Python 不提供函数重载 参考知乎为什么 Python 不支持函数重载？其他函数大部分都支持的？ 我们知道 函数重载 主要是为了解决两个问题。 可变参数类型。 可变参数个数。 另外，一个函数重载基本的设计原则是，仅仅当两个函数除了参数类型和参数个数不同以外，其功能是完全相同的，此时才使用函数重载，如果两个函数的功能其实不同，那么不应当使用重载，而应当使用一个名字不同的函数。 对于情况 1 ，函数功能相同，但是参数类型不同，Python 如何处理？答案是根本不需要处理，因为 Python 可以接受任何类型的参数，如果函数的功能相同，那么不同的参数类型在 Python 中很可能是相同的代码，没有必要做成两个不同函数。 对于情况 2 ，函数功能相同，但参数个数不同，Python 如何处理？大家知道，答案就是缺省参数(默认参数)。对那些缺少的参数设定为缺省参数(默认参数)即可解决问题。因为你假设函数功能相同，那么那些缺少的参数终归是需要用的。所以，鉴于情况 1 跟 情况 2 都有了解决方案，Python 自然就不需要函数重载了。 实例方法/静态方法/类方法 Python 类语法中有三种方法，实例方法，静态方法，类方法，它们的区别如下： 实例方法只能被实例对象调用，静态方法(由 @staticmethod 装饰器来声明)、类方法(由 @classmethod 装饰器来声明)，可以被类或类的实例对象调用; 实例方法，第一个参数必须要默认传实例对象，一般习惯用self。静态方法，参数没有要求。类方法，第一个参数必须要默认传类，一般习惯用 cls . 实例代码如下： class Foo(object): \"\"\"类三种方法语法形式 \"\"\" def instance_method(self): print(\"是类{}的实例方法，只能被实例对象调用\".format(Foo)) @staticmethod def static_method(): print(\"是静态方法\") @classmethod def class_method(cls): print(\"是类方法\") foo = Foo() foo.instance_method() foo.static_method() foo.class_method() print('##############') Foo.static_method() Foo.class_method() 程序执行后输出如下： 是类 的实例方法，只能被实例对象调用 是静态方法 是类方法 # 是静态方法 是类方法 __new__和 __init __方法的区别 __init__ 方法并不是真正意义上的构造函数, __new__ 方法才是(类的构造函数是类的一种特殊的成员函数，它会在每次创建类的新对象时执行); __new__ 方法用于创建对象并返回对象，当返回对象时会自动调用 __init__ 方法进行初始化, __new__ 方法比 __init__ 方法更早执行; __new__ 方法是静态方法，而 __init__ 是实例方法。 Python 的函数参数传递 参考这两个链接，stackoverflow的最高赞那个讲得很详细 How do I pass a variable by reference? Python 面试题 个人总结（有点不好）： 将可变对象：列表list、字典dict、NumPy数组ndarray和用户定义的类型（类），作为参数传递给函数，函数内部将其改变后，函数外部这个变量也会改变（对变量进行重新赋值除外 rebind the reference in the method） 将不可变对象：字符串string、元组tuple、数值numbers，作为参数传递给函数，函数内部将其改变后，函数外部这个变量不会改变 Python 实现对函参做类型检查 Python 自带的函数一般都会有对函数参数类型做检查，自定义的函数参数类型检查可以用函数 isinstance() 实现，例如： def my_abs(x): \"\"\" 自定义的绝对值函数 :param x: int or float :return: positive number, int or float \"\"\" if not isinstance(x, (int, float)): raise TypeError('bad operand type') if x > 0: return x else: return -x 添加了参数检查后，如果传入错误的参数类型，函数就可以抛出一个 TypeError 错误。 为什么说 Python 是动态语言 在 Python 中，等号 = 是赋值语句，可以把任意数据类型赋值给变量，同样一个变量可以反复赋值，而且可以是不同类型的变量，例如： a = 100 # a是int型变量 print(a) a = 'ABC' # a 是str型变量 print(a) Pyhon 这种变量本身类型不固定，可以反复赋值不同类型的变量称为动态语言，与之对应的是静态语言。静态语言在定义变量时必须指定变量类型，如果赋值的时候类型不匹配，就会报错，Java/C++ 都是静态语言（int a; a = 100） Python 装饰器理解 装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。 map 与 reduce 函数用法解释 1、map() 函数接收两个参数，一个是函数，一个是 Iterable，map 将传入的函数依次作用到序列的每个元素，并将结果作为新的 Iterator 返回，简单示例代码如下： # 示例１ def square(x): return x ** 2 r = map(square, [1, 2, 3, 4, 5, 6, 7]) squareed_list = list(r) print(squareed_list) # [1, 4, 9, 16, 25, 36, 49] # 使用lambda匿名函数简化为一行代码 list(map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9])) # 示例２ list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9]))　＃　['1', '2', '3', '4', '5', '6', '7', '8', '9'] 注意map函数返回的是一个Iterator（惰性序列），要通过list函数转化为常用列表结构。map()作为高阶函数，事实上它是把运算规则抽象了。 2、reduce() 函数也接受两个参数，一个是函数（两个参数），一个是序列，与 map 不同的是reduce 把结果继续和序列的下一个元素做累积计算,效果如下： reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) 示例代码如下： from functools import reduce CHAR_TO_INT = { '0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9 } def str2int(str): ints = map(lambda x:CHAR_TO_INT[x], str) # str对象是Iterable对象 return reduce(lambda x,y:10*x + y, ints) print(str2int('0')) print(str2int('12300')) print(str2int('0012345')) # 0012345 Python 深拷贝、浅拷贝区别 Python 中的大多数对象，比如列表 list、字典 dict、集合 set、numpy 数组，和用户定义的类型（类），都是可变的。意味着这些对象或包含的值可以被修改。但也有些对象是不可变的，例如数值型 int、字符串型 str 和元组 tuple。 1、复制不可变数据类型： 复制不可变数据类型，不管 copy 还是 deepcopy, 都是同一个地址。当浅复制的值是不可变对象（数值，字符串，元组）时和=“赋值”的情况一样，对象的 id 值与浅复制原来的值相同。 2、复制可变数据类型： 直接赋值：其实就是对象的引用（别名）。 浅拷贝(copy)：拷贝父对象，不会拷贝对象内部的子对象（拷贝可以理解为创建内存）。产生浅拷贝的操作有以下几种： 使用切片 [:] 操作 使用工厂函数（如 list/dir/set ）, 工厂函数看上去像函数，实质上是类，调用时实际上是生成了该类型的一个实例，就像工厂生产货物一样. 使用copy 模块中的 copy() 函数，b = a.copy(), a 和 b 是一个独立的对象，但他们的子对象还是指向统一对象（是引用）。 深拷贝(deepcopy)： copy 模块的 deepcopy() 方法，完全拷贝了父对象及其子对象，两者是完全独立的。深拷贝，包含对象里面的子对象的拷贝，所以原始对象的改变不会造成深拷贝里任何子元素的改变。 注意：浅拷贝和深拷贝的不同仅仅是对组合对象来说，所谓的组合对象（容器）就是包含了其它对象的对象，如列表，类实例。而对于数字、字符串以及其它“原子”类型（没有子对象），没有拷贝一说，产生的都是原对象的引用。更清晰易懂的理解，可以参考这篇文章。 看一个示例程序，就能明白浅拷贝与深拷贝的区别了： #!/usr/bin/Python3 # -*-coding:utf-8 -*- import copy a = [1, 2, 3, ['a', 'b', 'c']] b = a # 赋值，传对象的引用 c = copy.copy(a) # 浅拷贝 d = copy.deepcopy(a) # 深拷贝 a.append(4) a[3].append('d') print(id(a), id(b), id(c), id(d)) # a 与 b 的内存地址相同 print('a = ', a) print('b = ', b) print('c = ', c) print('d = ', d) # [1, 2, 3, ['a', 'b', 'c']] 程序输出如下： 2061915781832 2061915781832 2061932431304 2061932811400 a = [1, 2, 3, ['a', 'b', 'c', 'd'], 4] b = [1, 2, 3, ['a', 'b', 'c', 'd'], 4] c = [1, 2, 3, ['a', 'b', 'c', 'd']] d = [1, 2, 3, ['a', 'b', 'c']] Python 继承多态理解 多态是指对不同类型的变量进行相同的操作，它会根据对象（或类）类型的不同而表现出不同的行为。 继承可以拿到父类的所有数据和方法，子类可以重写父类的方法，也可以新增自己特有的方法。 先有继承，后有多态，不同类的对象对同一消息会作出不同的相应。 Python 面向对象的原则 Python 工匠：写好面向对象代码的原则（上） Python 工匠：写好面向对象代码的原则（中） Python 工匠：写好面向对象代码的原则（下） 参考资料 参考这里 110道Python面试题（真题） 关于Python的面试题 继承和多态 Python 直接赋值、浅拷贝和深度拷贝解析 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"2-programming_language/shell/Shell语法基础.html":{"url":"2-programming_language/shell/Shell语法基础.html","title":"Shell语法基础","keywords":"","body":" Shell 变量 使用变量 只读变量 删除变量 变量类型 Shell 字符串 单引号与双引号字符串 获取字符串长度 提取子字符串 拼接字符串 Shell 数组 定义数组 读取数组 获取数组的长度 Shell 传递参数 Shell 基本运算符 算术运算符 关系运算符 布尔运算符 逻辑运算符 字符串运算符 Shell 信息输出命令 Shell echo 命令 Shell printf 命令 %d %s %c %f 格式替代符详解: printf 的转义序列 Shell test 命令 数值测试 test 检查文件属性 Shell 流程控制 if else if else-if else for 循环 while 语句 Shell 函数 局部变量与全局变量 递归函数 常用命令 Shell 正则表达式 参考资料 Shell 变量 在 Shell 脚本中，定义变量直接赋值即可，使用变量时需要在变量名前加美元符号$，注意定义变量时变量名和等号之间不能有空格。变量名的命名必须遵循以下规则： 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用 bash 里的关键字（可用 help 命令查看保留关键字）。 使用变量 使用一个定义过的变量，只要在变量名前面加美元符号即可（推荐给所有变量加上花括号，这是一个好的编程习惯），如： #!/bin/bash my_name=\"hongghao.zhang\" echo $my_name echo ${my_name} hongghao.zhang hongghao.zhang 只读变量 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 删除变量 使用 unset 变量可以删除变量，语法：unset variable_name。 变量类型 运行 shell 时，会同时存在三种变量： 1) 局部变量： 局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。 2) 环境变量： 所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候shell脚本也可以定义环境变量。 3) shell变量： shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了shell的正常运行。 Shell 字符串 字符串是 shell 编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号，也可以不用引号。单双引号的区别跟 PHP 类似。 单引号与双引号字符串 单引号字符串限制： 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的； 单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用。 双引号字符串优点： 双引号里可以有变量； 双引号里可以出现转义字符，Shell 脚本程序字符型建议都用双引号。 获取字符串长度 string=\"honggao.zhang\" echo ${#string} # 输出13 提取子字符串 下面实例从字符串第8个字符开始截取5个字符： string=\"honggao.zhang\" echo ${string:7:5} # 输出zhang 拼接字符串 实际脚本中，拼接字符串可能有以下场景：灵活应用即可。 your_name=\"qinjx\" greeting=\"hello, \"$your_name\" !\" greeting_1=\"hello, ${your_name} !\" echo $greeting $greeting_1 Shell 数组 bash 支持一维数组，不支持多维数组，并且没有限定数组的大小。类似 C 语言，数组的元素下标也是从 0 开始。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于 0。 定义数组 在 Shell 中，用括号来表示数组，数组元素用\"空格\"符号分割开。定义数组的一般形式为： 数组名=(值1 值2 ... 值n) 读取数组 读取数组元素值的一般格式是： ${数组名[下表标]} 使用 @ 符号可以获取数组中的所有元素，例如： echo ${array_name[@]} 获取数组的长度 获取数组长度的方法与获取字符串长度的方法相同，例如： # 取得数组元素的个数 length=${#array_name[@]} # 或者 length=${#array_name[*]} # 取得数组单个元素的长度 lengthn=${#array_name[n]} 更多内容参考shell脚本——字符串 数组。 Shell 传递参数 命令行执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：$n。n 代表一个数字，1 为脚本的第一个参数，2 为脚本的第二个参数，以此类推。特殊字符表示的参数如下： 参数处理 说明 $# 传递到脚本的参数个数 $$ 脚本运行的当前进程ID号 $! 后台运行的最后一个进程的ID号 $* 以一个单字符形式显示所有向脚本传递的参数，\"$1 $2 ... $n\"的形式输出所有参数 $@ 与 * 相同，但是使用时加引号，并在引号中返回每个参数。如 \"$@\" 用「\"」括起来的情况、以 \"$1\" \"$2\" … \"$n\" 的形式输出所有参数。 $- 显示 Shell 使用的当前选项，与set命令功能相同。 $? 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。 示例代码如下： #!/bin/bash # author:harley echo \"=== $* 演示 ===\" for i in \"$*\"; do echo $i done echo \"====$@ 演示===\" for i in \"$@\";do echo $i done 执行脚本，bash demo1.sh harley zhang hong，输出结果如下： === $* 演示 === harley zhang hong ====$@ 演示=== harley zhang hong Shell 基本运算符 Shell 支持多种运算符，如下： 算法运算符 关系运算符 布尔运算符 字符串运算符 文件测试运算符 字符串判断相等用=，数值判断相等用==。原生 bash 不支持简单的数学运算，但是可以通过其他命令来实现，例如 awk 和 expr，expr 最常用。expr是一款表达式计算工具，使用它能完成表达式的求值操作。用法如下： #!/bin/bash val=`expr 3 + 6` echo \"两数之和为：\" ${val} # 两数之和为：9 注意： 表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。 条件表达式都要放在方括号之间，并且要有空格，例如: [$a==$b] 是错误的，必须写成 [ $a == $b ]。 完整的算数表达式要被 ` ` 包含，注意这个字符不是常用的单引号，在 Esc 键下边。 bash 不支持浮点运算，如果需要进行浮点运算，需要借助 bc,awk 处理。 算术运算符 运算符 说明 举例 + 加法 expr $a + $b 结果为 30 - 减法 expr $a - $b 结果为 -10 * 乘法 expr $a \\* $b 结果为 200 / 除法 expr $b / $a 结果为 2 % 取余 expr $b % $a 结果为 0 = 赋值 a=$b 将把变量 b 的值赋给 a == 相等 用于比较两个数字，相同则返回 true。 [ $a == $b ] 返回 false != 不相等 用于比较两个数字，不相同则返回 true。 [ $a != $b ] 返回 true 算数运算符实例脚本如下： #!/bin/bash a=10 b=20 val=`expr $a + $b` echo \"a + b : $val\" val=`expr $a - $b` echo \"a - b : $val\" val=`expr $a \\* $b` echo \"a * b : $val\" 脚本运行结果如下： a + b : 30 a - b : -10 a * b : 200 关系运算符 关系运算符只支持数字，不支持字符串，除非字符串的值是数字。Shell 的关系运算符和 C/C++/Python 不一样，它们的大于用 > 表示即可，但是 Shell 得用关键字表示，下表列出了常用得关系运算符，假定变量 a 为 10，变量 b 为 20： 参数 说明 举例 -eq 等于则为真 [ $a -eq $b]返回false -ne 不等于则为真 [ $a -ne $b]返回true -gt 大于则为真 [ $a -gt $b]返回false -ge 大于等于则为真 [ $a -ge $b]返回false -lt 小于则为真 [ $a -lt $b]返回true -le 小于等于则为真 [ $a -le $b]返回true 这些关系运算符初学时不必全部记住，编写脚本用到时再来查询也可。 布尔运算符 运算符 说明 举例 ! 非运算符，表达式为 true 则返回 false，否则返回 true [ ! false ] 返回 true -o 或运算，有一个表达式为 true 则返回 true [ $a -lt 20 -o $b -gt 100 ] 返回 true -a 与运算，两个表达式都为 true 才返回 true [ $a -lt 20 -a $b -gt 100 ] 返回 false 实例代码如下： $ a=120;if [ $a != 120 ];then echo \"a != 120\";else echo \"a == 120\";fi # ! 运算符的用法 a == 120 逻辑运算符 运算符 说明 举例 && 逻辑的 AND [[ $a -lt 100 && $b -gt 100 ]] 返回 false ` ` 逻辑的 OR `[[ $a -lt 100 $b -gt 100 ]]` 返回 true 字符串运算符 下表列出了常用的字符串运算符，假定变量 a 为 \"abc\"，变量 b 为 \"efg\"： 运算符 说明 举例 = 检测两个字符串是否相等，相等返回 true [ $a = $b ] 返回 false != 检测两个字符串是否相等，不相等返回 true [ $a != $b ] 返回 true -z 检测字符串长度是否为0，为0返回 true [ -z $a ] 返回 false -n 检测字符串长度是否为0，不为0返回 true [ -n \"$a\" ] 返回 true $ 检测字符串是否为空，不为空返回true [ $a ]返回true 字符串运算符使用示例代码如下： #!/bin/bash a=\"abc\" b=\"efg\" if [ $a = $b ] then echo \"$a = $b : a 等于 b\" else echo \"$a = $b: a 不等于 b\" fi if [ $a != $b ] then echo \"$a != $b : a 不等于 b\" else echo \"$a != $b: a 等于 b\" fi if [ -z $a ] then echo \"-z $a : 字符串长度为 0\" else echo \"-z $a : 字符串长度不为 0\" fi if [ -n \"$a\" ] then echo \"-n $a : 字符串长度不为 0\" else echo \"-n $a : 字符串长度为 0\" fi if [ $a ] then echo \"$a : 字符串不为空\" else echo \"$a : 字符串为空\" fi 执行脚本，输出结果如下： abc = efg: a 不等于 b abc != efg : a 不等于 b -z abc : 字符串长度不为 0 -n abc : 字符串长度不为 0 abc : 字符串不为空 Shell 信息输出命令 Shell echo 命令 echo 命令用于字符串的输出，echo打印字符串默认换行。 Shell printf 命令 printf 命令和 echo 命令类似，都是用于信息的输出。 printf 命令模仿 C 程序库（library）里的 printf() 程序。 printf 由 POSIX 标准所定义，因此使用 printf 的脚本比使用 echo 移植性好。 printf 使用引用文本或空格分隔的参数，外面可以在 printf 中使用格式化字符串，还可以制定字符串的宽度、左右对齐方式等。默认 printf 不会像 echo 自动添加换行符，我们可以手动添加 \\n。 printf 命令语法如下： printf format-string [arguments...] 参数说明： format-string: 为格式控制字符串 arguments:为参数列表 示例程序如下： #!/bin/bash printf \"%-10s %-8s %-4s %12s\\n\" 姓名 性别 体重kg 学号 printf \"%-10s %-8s %-4.2f %12d\\n\" 郭靖 男 66.1234 2017210675 printf \"%-10s %-8s %-4.2f %12d\\n\" 杨过 男 48.6543 2017210688 printf \"%-10s %-8s %-4.2f %12d\\n\" 郭芙 女 47.9876 2017210889 执行脚本，程序输出如下： 姓名 性别 体重kg 学号 郭靖 男 66.12 2017210675 杨过 男 48.65 2017210688 郭芙 女 47.99 2017210889 格式控制字符串解释： %s %c %d %f都是格式替代符 %-10s 指一个宽度为10个字符（-表示左对齐，没有则表示右对齐），任何字符都会被显示在10个字符宽的字符内，如果不足则自动以空格填充，超过也会将内容全部显示出来。 %-4.2f 指格式化为小数，其中.2指保留2位小数。 %d %s %c %f 格式替代符详解: d: Decimal 十进制整数 -- 对应位置参数必须是十进制整数，否则报错！ s: String 字符串 -- 对应位置参数必须是字符串或者字符型，否则报错！ c: Char 字符 -- 对应位置参数必须是字符串或者字符型，否则报错！ f: Float 浮点 -- 对应位置参数必须是数字型，否则报错! printf 的转义序列 序列 说明 \\a 警告字符，通常为ASCII的BEL字符 \\f 换页 \\n 换行 \\t 水平制表符 \\r 回车 Shell test 命令 Shell 中的 test 命令用于检查某个条件是否成立，可以进行数值、字符和文件三个方面的测试。 数值测试 这是关系运算符，只支持数字，不支持字符串，除非字符串的值是数字。 参数 说明 -eq 等于则为真 -ne 不等于则为真 -gt 大于则为真 -ge 大于等于则为真 -lt 小于则为真 -le 小于等于则为真 符号含义： eq （equal的缩写），表示等于为真 ne (not equal的缩写），表示不等于为真 gt (greater than的缩写），表示大于为真 ge （greater&equal的缩写），表示大于等于为真 lt （lower than的缩写），表示小于为真 le （lower&equal的缩写），表示小于等于为真 实例代码如下： #!/bin/bash # 关系运算符判断 num1=100 num2=333 if test $num1 -eq $num2 then echo \"两个数相等\" else echo \"两个数不相等\" fi # 算术运算符判断 str1=\"honggao\" str2=\"hong.hao\" echo \"传递的参数为: $*\" if [ $1 = $2i ] then echo \"两个输入字符串相等\" else echo \"输入的两个字符串不相等\" fi 执行脚本(sh comm_test.sh eere wdwe2)，输出如下： 两个数不相等 传递的参数为: eere wdwe2 输入的两个字符串 不相等 test 检查文件属性 检查文件属性也是 test 的常见用法，比如检查一个文件类型是不是普通文件，可以使用 -f 选项，检查路径是否是目录可以用 -d 选项： touch test.sh filename=\"test.sh\" # 检查文件 if test -f \"$filename\";then echo \"It's a regular file.\" fi # 检查目录 dirname=\"test_directory\" mkdir $dirname if test -d \"$dirname\";then echo \"It's a directory.\" fi 运行脚本，输出如下： test 命令是shell编程中非常重要的命令，一定要掌握！下面是其他一些常用的文件检查运算符： -b file : 文件存在并且是块设备文件。 -c file : 文件存在并且是字符设备文件。 -d file : 文件存在并且是一个目录。 -e file : 文件存在。 -f file : 文件存在并且是一般文件。 -g file : 文件存在并且设置了 setgid 位。 -h file : 文件存在并且是一个链接文件。 -p file : 文件存在并且是一个命名管道(FIFO)。 -r file : 文件存在并且是可读的。 -s file : 文件存在并且有内容。 -u file : 文件存在并且设置了 setuid。 -w file : 文件存在并且是可写的。 -x file : 文件存在并且是可执行的。 -S file : 文件存在并且是一个 socket。 Shell 流程控制 Shell 的流程控制不可为空。 if else if else语法格式： if condition then command1 command2 command3 else command fi if else-if else if else-if else 语法格式如下： if condition1 then command1 elif condition2 then command2 else commandN fi 根据 width、height 计算 BMI 指数脚本实例代码如下： echo \"pleae input your weight and height\" # 无法支持输入小数 pf=`expr $2 \\* $2` bmi=`expr $1 / $pf` echo \"your bmi is: $bmi\" a=18 b=25 c=28 d=32 if [ $bmi -le $a ] then echo \"体重过轻\" elif [ $bmi -le $b ] then echo \"体重正常\" elif [ $bmi -le $c ] then echo \"体重过重\" elif [ $bmi -le $d ] then echo \"体重肥胖\" elif [ $bmi -gt $d ] then echo \"严重肥胖\" fi 执行脚本(sh if_else.sh 64 2)，程序输出如下： pleae input your weight and height your bmi is: 16 体重过轻 for 循环 for 循环格式为： for var in item1 item2 ... itemN do command1 command2 ... commandN done while 语句 while 循环用于不断执行一系列命令，也可用于从输入文件中读取数据；命令通常为测试条件，其格式为： while condition do command done Shell 函数 shell 函数中的定义格式如下： [ function ] funname [()] { action; [return int;] } 参数说明： 可以带 function fun() 定义，也可以直接 fun() 定义,不带任何参数。 执行函数直接使用 funname 即可。 局部变量与全局变量 # !/bin/bash a=\"this is a\" # 定义全局变量 b=\"this is b\" function funname() { local_c=\"this is c\" # 定义局部变量 echo $a, $b echo $local_c return 0 # shell 函数返回值是整形，并且在 0-257 之间 } echo $d # 打印不会生效，因为 d 是局部变量 funname # 执行函数 funname 执行上诉程序 bash fun_test.sh，输出如下： this is a, this is b this is c 递归函数 bash 也是支持递归函数的（能够调用自身的函数），示例程序如下： #!/bin/bash function name() { echo $1 name hello sleep 1 } name 运行此脚本后不断打印出 hello，按 ctrl+c 结束。 常用命令 ps、grep、awk、sed 三剑客 Shell 正则表达式 参考博客Shell 正则表达式。 参考资料 菜鸟教程-shell教程 Linux 命令行与 Shell 脚本教程 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"2-programming_language/cpp/C++日期和时间编程总结.html":{"url":"2-programming_language/cpp/C++日期和时间编程总结.html","title":"C++日期和时间编程总结","keywords":"","body":"目录 [toc] C++11 的日期和时间编程内容在 C++ Primer(第五版)这本书并没有介绍，目前网上的文章又大多质量堪忧或者不成系统，故写下这篇文章用作自己的技术沉淀和技术分享，大部分内容来自网上资料，文末也给出了参考链接。 日期和时间库是每个编程语言都会提供的内部库，其可以用打印模块耗时，从而方便做性能分析，也可以用作打印运行时间点。本文的内容着重于 C++11-C++17的内容，C++20的日期和时钟库虽然使用更方便也更强大，但是考虑到版本兼容和程序移植问题，故不做深入探讨。 一，概述 C++ 中可以使用的日期时间 API 分为两类： C-style 日期时间库，位于 头文件中。这是原先 头文件的 C++ 版本。 chrono 库：C++ 11 中新增API，增加了时间点，时长和时钟等相关接口（使用较为复杂）。 在 C++11 之前，C++ 编程只能使用 C-style 日期时间库，其精度只有秒级别，这对于有高精度要求的程序来说，是不够的。但这个问题在C++11 中得到了解决，C++11 中不仅扩展了对于精度的要求，也为不同系统的时间要求提供了支持。另一方面，对于只能使用 C-style 日期时间库的程序来说，C++17 中也增加了 timespec 将精度提升到了纳秒级别。 二，C-style 日期和时间库 #include 该头文件包含了获取和操作日期和时间的函数和相关数据类型定义。 2.1，数据类型 名称 说明 time_t 能够表示时间的基本算术类型的别名，能够表示函数 time 返回的时间，单位为秒级别。 clock_t 能够表示时钟滴答计数的基本算术类型的别名（可用作进程运行时间） size_t sizeof 运算符返回的无符号整数类型。 struct tm 包含日历日期和时间的结构体类型 timespec* 以秒和纳秒表示的时间 2.2，函数 C-style 日期时间库中包含的时间操作函数如下： 函数 说明 std::clock_t clock() 返回自程序启动时起的处理器时钟时间 double difftime(std::time_t time_end, std::time_t time_beg) 计算开始和结束之间的秒数差 std::time_t time (time_t* timer) 返回自纪元起计的系统当前时间, 函数可以为空指针 std::time_t mktime (struct tm * timeptr) 将 tm 格式的时间转换成 time_t 表示的时间 时间转换函数如下： 函数 说明 char* asctime(const struct tm* timeptr) 将 tm 结构体对象转换为字符串的文本 char* ctime(const time_t* timer) 将 time_t 对象转换为 C 字符串，用于表示日历时间 struct tm* gmtime(const time_t* time) 将 time_t 转换成 UTC 表示的时间 struct tm* localtime(const time_t* timer) 将 time_t 转换成本地时间 localtime 函数使用参数 timer 指向的值来填充 tm 结构体，其中的值表示对应的时间，以本地时区表示。 strftime 和 wcsftime 函数一般不常用，故不做介绍。tm 结构体的一般定义如下： /* Used by other time functions. */ struct tm { int tm_sec; /* Seconds. [0-60] (1 leap second) */ int tm_min; /* Minutes. [0-59] */ int tm_hour; /* Hours. [0-23] */ int tm_mday; /* Day. [1-31] */ int tm_mon; /* Month. [0-11] */ int tm_year; /* Year - 1900. */ int tm_wday; /* Day of week. [0-6] */ int tm_yday; /* Days in year.[0-365] */ int tm_isdst; /* DST. [-1/0/1]*/ }; 2.3，数据类型与函数关系梳理 时间和日期相关的函数及数据类型比较多，单纯看表格和代码不是很好记忆，第一个参考链接的作者给出了如下所示的思维导图，方便记忆与理解上面所有函数及数据类型之间各自的联系。 在这幅图中，以数据类型为中心，带方向的实线箭头表示该函数能返回相应类型的结果。 clock 函数是相对独立的一个函数，它返回进程运行的时间，具体描述见下文。 time_t 描述了纪元时间，通过 time 函数可以获得它，但它只能精确到秒级别。 timespec 类型在 time_t 的基础上，增加了纳秒的精度，通过 timespec_get 获取。这是 C++17 上新增的特性。 tm 是日历类型，因为它其中包含了年月日等信息。通过 gmtime，localtime 和 mktime 函数可以将 time_t 和 tm 类型互相转换。 考虑到时区的差异，因此存在 gmtime 和 localtime 两个函数。 无论是 time_t 还是 tm 结构，都可以将其以字符串格式输出。ctime 和 asctime 输出的格式是固定的。如果需要自定义格式，需要使用 strftime 或者 wcsftime 函数。 2.4，时间类型 2.4.1，UTC 时间 协调世界时（Coordinated Universial Time，简称 UTC）是最主要的时间标准，其以原子时秒长为基础，在时刻上尽量接近于格林威治标准时间。 协调世界时是世界上调节时钟和时间的主要时间标准，它与0度经线的平太阳时相差不超过 1 秒。因此UTC时间+8即可获得北京标准时间（UTC+8）。 2.4.2，本地时间 本地时间与当地的时区相关，例如中国当地时间采用了北京标准时间（UTC+8）。 2.4.3，纪元时间 纪元时间（Epoch time）又叫做 Unix 时间或者 POSIX 时间。它表示自1970 年 1 月 1 日 00:00 UTC 以来所经过的秒数（不考虑闰秒）。它在操作系统和文件格式中被广泛使用。** 头文件中通过 time_t 以秒级别表示纪元时间**。 纪元时间这个想法很简单：以一个时间为起点加上一个偏移量便可以表达任何一个其他的时间。 为什么选这个时间作为起点，可以点击这里：Why is 1/1/1970 the “epoch time”?。 通过 time 函数获取当前时刻的纪元时间示例代码如下： time_t epoch_time = time(nullptr); cout time 函数接受一个指针，指向要存储时间的对象，通常可以传递一个空指针，然后通过返回值来接受结果。虽然标准中没有给出定义，但time_t 通常使用整形值来实现。 2.5，输出时间和日期 使用 ctime 函数，可以将时间以固定格式的字符串的形式打印出来，格式为：Www Mmm dd hh:mm:ss yyyy\\n。代码示例如下： // 以字符串形式输出当前时间和日期 time_t now = time(nullptr); cout 2.6，综合示例代码 asctime() 和 difftime() 函数等sample 代码如下（复制可直接运行）： /* asctime example */ #include /* printf */ #include /* time_t, struct tm, time, localtime, asctime */ #include #include using namespace std; // 冒泡排序: 将数据从小到大排序 void bubbleSort(vector &arr){ size_t number = arr.size(); if (number arr[j+1]){ temp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = temp; } } } } // difftime() 函数: 计算时间差，单位为 s void difftime_test() { vector input_array; for (int i = 90000; i > 0; i--) { input_array.emplace_back(i); } time_t time1 = time(nullptr); bubbleSort(input_array); time_t time2 = time(nullptr); double time_diff = difftime(time2, time1); cout g++ time_demo.cpp -std=c++11 编译后，运行程序 ./a.out 后，输出结果： 三，chrono 库 “chrono” 是英文 chronology 的缩写，其含义是“年表；年代学”。 chrono 既是头文件名字也是子命名空间的名字，chrono 头文件下的所有 elements 都是在 std::chrono 命名空间下定义的。 std::chrono 是 C++11 引入的日期时间处理库，chrono 库里包括三种主要类型：Clocks，Time points 和 Durations 。 3.1，时钟 C++11 chrono 库中包含了三种的时钟类： 名称 说明 chrono::system_clock 系统时钟（可以调整） chrono::steady_clock 单调递增时钟（不能调整） chrono::high_resolution_clock 拥有可用的最短嘀嗒周期的时钟 system_clock 是当前所在系统的时钟。因为系统时钟随时都可能被调整，所以如果想要计算两个时间点的时间差，是不推荐使用系统时钟的。 steady_clock 会保证时间的单调递增性，只会向前移动不会减少，所以最适合用来度量时间间隔。 high_resolution_clock 表示实现提供的拥有最小计次周期的时钟。它可以是 system_clock 或 steady_clock 的别名，也可能是第三个独立时钟。在不同的标准库中，high_resolution_clock 的实现不一致，所以官方不建议使用这个时钟。 这三个时钟类有一些共同的成员函数和数据类型，如下所示： 名称 说明 now() 静态成员函数，返回当前时间，类型为 clock::time_point time_point 成员类型，当前时钟的时间点类型，用于表示一个具体时间，详情见下文“时间点” duration 成员类型，时钟的时长类型，用于表示时间间隔(一段时间)，详情见下文“时长” rep 成员类型，时钟的 tick 类型，等同于 clock::duration::rep period 成员类型，时钟的单位，等同于 clock::duration::period is_steady 静态成员类型：是否是稳定时钟，对于 steady_clock 来说该值一定是 true 每一个时钟类都有一个 now() 静态函数来获取当前时间，返回的类型由 time_point 描述。std::chrono::time_point 是模板类，模版类实例如：std::chrono::time_pointclock>，这样写比较长，庆幸的是在 C++11 中可以通过 auto 关键字来自动推导变量类型。 std::chrono::time_point now1 = std::chrono::steady_clock::now(); auto now2 = std::chrono::steady_clock::now(); 3.2，与C-style转换 system_clock 与另外两个 clock 不一样的地方在于，它还提供了两个静态函数用来将 time_point 与 std::time_t 来回转换。 名称 说明 to_time_t 将系统时钟时间点转换为 time_t from_time_t 将 time_t 转换到系统时钟时间点 第一篇参考链接的文章给出了下面这幅图来描述 c 风格和 c++11 的几种时间类型的转换： 3.3，时长 ratio 为了支持更高精度的系统时钟，C++11 新增了一个新的头文件 和类型，用于自定义时间单位。std::ratio 是一个模板类，提供了编译期的比例计算功能，为 std::chrono::duration 提供基础服务。其声明如下： template class ratio; 第一个模板参数 Num (numerator) 表示分子，第二个参数 Denom (denominator) 表示分母。typedef ratio milli; 表示一千分之一，因为约定了基本计算单位是秒，所以 milli 表示一千分之一秒。所以通过 ratio 可以表示毫秒、微秒、纳秒等。 typedef ratio nano; // 纳秒单位 typedef ratio micro; // 微秒单位 typedef ratio milli; // 毫秒单位 typedef ratio s // 秒单位 ratio 能表达的数值不仅仅是以 10 为基底的，同时也可以表达任意的分数秒，例如：5/7秒，89/23409 秒等等对于一个具体的 ratio 来说，可以通过 den 获取分母的值，num 获取分子的值。不仅仅如此，头文件还包含了：ratio_add，ratio_subtract，ratio_multiply，ratio_divide 来完成分数的加减乘除四则运算。例如，想要计算 5/7+59/1023，可以用以下代码表示： ratio_add, ratio> result; double value = ((double) result.num) / result.den; cout 在C++中，如果分子和分母都是整形，则整形除法结果依然是整形，即小数点右边部分会被抛弃，因此想要获取 double 类型的结果，需要先将其转换成 double。 3.3.1，时长运算 时长对象之间可以进行相加或相减运算。chrono 提供了以下几个常用时长运算的函数： 函数 说明 duration_cast 进行时长的转换 floor（C++17） 以向下取整的方式，将一个时长转换为另一个时长 ceil（C++17） 以向上取整的方式，将一个时长转换为另一个时长 round（C++17） 转换时长到另一个时长，就近取整，偶数优先 abs（C++17） 获取时长的绝对值 3.4，时间间隔 duration 类模板 std::chrono::duration 表示时间间隔，其声明如下： template > class duration; 类成员类型描述： member type definition notes rep The first template parameter (Rep) Representation type used as the type for the internal count object. period The second template parameter (Period) The ratio type that represents a period in seconds. duration 由 Rep 类型的计次数和Period 类型的计次周期组成，其中计次周期是一个编译期有理数常量，表示从一个计次到下一个的秒数。存储于 duration 的数据仅有 Rep 类型的计次数。若 Rep 是浮点数，则 duration 能表示小数的计次数。 Period 被包含为时长类型的一部分，且只在不同时长间转换时使用。 Rep 表示一种数值类型，用来表示 Period 的数量，比如 int float double (count of ticks)。 Period 是 std::ratio 类型，用来表示【用秒表示的时间单位】比如 second milisecond (a tick period)。 成员函数 count() 返回 Rep 类型的 Period 数量。 常用的 duration 已经定义好了，在 std::chrono 头文件中，常用时长单位的代码如下： /// nanoseconds typedef duration nanoseconds; /// microseconds typedef duration microseconds; /// milliseconds typedef duration milliseconds; /// seconds typedef duration seconds; /// minutes typedef duration> minutes; /// hours typedef duration> hours; 类型 定义 std::chrono::nanoseconds duration std::chrono::microseconds duration std::chrono::milliseconds duration std::chrono::seconds duration std::chrono::minutes duration std::chrono::hours duration duration 类的 count() 成员函数返回时间间隔的具体数值。 3.4.1，时间间隔转换函数 duration_cast 因为有各种 duration 表示不同的时长单位，所以 chrono 库提供了 duration_cast 函数来换 duration 类型，其声明如下： template constexpr ToDuration duration_cast(const duration& d); 其定义比较复杂，但是我们日常使用可以直接使用 auto 推导函数返回对象类型，示例代码如下： #include #include #include #include void f() { std::this_thread::sleep_for(std::chrono::seconds(1)); } int main() { auto t1 = std::chrono::high_resolution_clock::now(); f(); auto t2 = std::chrono::high_resolution_clock::now(); // 整数时长：要求 duration_cast auto int_ms = std::chrono::duration_cast(t2 - t1); // 小数时长：不要求 duration_cast std::chrono::duration fp_ms = t2 - t1; std::cout 3.5，时间点 time_point std::chrono::time_point 表示时间中的一个点（一个具体时间），如上个世纪80年代、你的生日、今天下午、火车出发时间等，只要它能用计算机时钟表示。其包含了时钟和时长两个信息。它被实现成如同存储一个 Duration 类型的自 Clock 的纪元起始开始的时间间隔的值。其声明如下： template class time_point; 时钟的 now() 函数返回的值就是一个时间点。time_point 中的 time_since_epoch() 返回从其时钟起点开始的时长。可以通过两个时间点相减计算一个时间间隔，下面是代码示例: #include /* printf */ #include #include #include using namespace std; void time_point_test() { auto start = chrono::steady_clock::now(); double sum = 0; for(int i = 0; i (time_diff); cout 3.5.1，时间点运算 时间点有加法和减法操作，计算结果和常识一致：时间点 + 时长 = 时间点；时间点 - 时间点 = 时长。 参考资料 C++ 日期和时间编程 C++日期和时间工具 C++ 头文件内容官方英文版资料 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"2-programming_language/cpp/C++编程基础.html":{"url":"2-programming_language/cpp/C++编程基础.html","title":"C++编程基础","keywords":"","body":" STL 库描述 内存中堆和栈的区别 堆栈溢出一般是由什么原因导致的？ sizeof 的作用 C++ 面向对象特点 多态的理解 虚函数的理解 动态绑定的理解 C++构造函数初始化时什么时候只能用初始化列表？ C++ 构造函数和析构函数的初始化顺序 全局变量和局部变量在内存中是否有区别？如果有，是什么区别？ C++ 中的 new delete 和 C 语言中的 malloc free 有什么区别 new、delete、malloc、free 区别 static 关键字作用 类的 static 变量在什么时候初始化？函数的 static 变量在什么时候初始化？ C++ 变量作用域 C++ 指针和引用的区别 C++ 中析构函数的作用 C++ 静态函数和虚函数的区别 ++i 和 i++ 区别 const 关键字作用 C++如何传递数组给函数 STL 库描述 STL 库包括：容器、算法以及融合两者的迭代器。 容器分为顺序容器和关联容器。顺序容器比如 vector 是一个动态分配存储空间的容器。区别于 C++ 中的 array，array 分配的空间是静态的，分配之后不能被改变，而 vector 会自动重分配（扩展）空间。 内存中堆和栈的区别 栈内存：由编译器自动分配释放，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈，都是先进后出。栈使用的是一级缓存，他们通常都是被调用时处于存储空间中，调用完毕立即释放**。 堆内存：一般由程序员分配释放，若程序员不释放，程序结束时可能由 OS 回收。堆是存放在二级缓存中，生命周期由虚拟机的垃圾回收算法来决定（并不是一旦成为孤儿对象就能被回收）。所以调用这些对象的速度要相对来得低一些。 堆栈溢出一般是由什么原因导致的？ 没有垃圾回收资源。 sizeof 的作用 sizeof 运算符返回一条表达式或一个类型名字所占的字节数，其满足右结合规律。 C++ 面向对象特点 数据抽象、继承和动态绑定。 多态的理解 同一函数作用于不同的对象，会对应不同的实现，从而产生不同的执行结果。在运行时，可以通过指向基类的指针或引用，来调用实现派生类中的方法。 C++ 的多态性具体体现在运行和编译两个方面：在程序运行时的多态性通过继承和虚函数来体现；在程序编译时多态性体现在函数和运算符的重载上； 虚函数的理解 和 Python 不同，在 C++ 中，基类将类型相关的函数与派生类不做改变直接继承的函数区分对待。对于某些函数，基类希望它的派生类各自定义适合自身的版本，此使基类就会将这些函数声明成虚函数。同时，派生类内部必须在其内部对所有重新定义的虚函数进行声明。（参考 C++ Primer p526 页） 派生类必须使用类派生列表（class derivation list）明确指出它是从哪个（哪些）基类继承而来。派生的形式是：首先一个冒号，后面紧跟以逗号分隔的基类列表，其中每个基类前面可以有访问说明符。 class Student: public People{ string names; virtual double get_gpa (vector scores); } 动态绑定的理解 在 C++ 语言中，，当我们使用基类的引用（或指针）调用一个虚函数时，将发生动态绑定，即函数运行的版本由实参决定，运行时自动选择函数的版本。 C++构造函数初始化时什么时候只能用初始化列表？ 如果类成员是 const、引用。或者属于某种未提供默认构造函数的类类型时，我们必须通过构造函数初始值列表为这些成员提供初始值。（参考 C++ Primer p259） C++ 构造函数和析构函数的初始化顺序 本回答参考C++ 构造函数初始化顺序, C++奇奇怪怪的题目之构造析构顺序 有多个基类的派生类(多继承) 的构造函数初始化按照如下顺序进行： 先执行虚拟继承的父类的构造函数; 然后从左到右执行普通继承的父类的构造函数; 接着按照定义的顺序执行数据成员的初始化; 最后调用类自身的构造函数； 析构函数就无脑的将构造函数顺序反转即可。多继承形式下的构造函数和单继承形式基本相同，只是要在派生类的构造函数中调用多个基类的构造函数。 实例代码如下： #include using namespace std; class OBJ1 { public: OBJ1() { cout 程序输出结果如下： Base2 Base4 Base1 Base3 OBJ1 OBJ2 Derived ok construct ok Derived destory OBJ2 destory OBJ1 destory Base3 destory Base1 destory Base4 destory Base2 destory 全局变量和局部变量在内存中是否有区别？如果有，是什么区别？ 全局变量储存在静态数据区，局部变量在堆栈中。 C++ 中的 new delete 和 C 语言中的 malloc free 有什么区别 虽然这两者都分别是完成分配内存和释放内存的功能，但是 C++ 用 new 分配内存时会调用构造函数，用 delete 释放内存时会调用析构函数。 new、delete、malloc、free 区别 new 和 delete 是 C++ 的运算符，malloc 和 free 是 C++/C 语言的标准框函数，都可用于申请动态内存和释放内存。 new 动过调用对象的构造函数来申请动态内存；delete 通过调用对象的析构函数来释放内存。 对于非内部数据类型的对象而言，只用 maloc/free 是无法满足动态对象的要求。我们知道对象在创建的同时要自动执行构造函数，对象在消亡之前要自动执行析构函数。但由于 malloc/free 是库函数而不是运算符，不在编译器控制权限之内，不能够把执行构造函数和析构函数的任务强加于 malloc/free。因此 new/delete 其实比 malloc/free 更灵活。 static 关键字作用 声明全局静态变量：在全局变量前加上关键字 static，全局变量就定义成一个全局静态变量，作用域在声明它的文件之外是不可见的，即从定义之处开始到文件结尾。 局部静态变量：在局部变量前加上关键字 static，作用域仍然为局部作用域，即当定义它的函数或者语句块结束的时候，作用域结束。 静态函数： 在函数返回类型前加 static，静态函数只在声明他的文件中可见，不能被其他文件使用。 类的静态成员：在类中，静态成员可以实现多个对象之间的数据共享，即静态成员是类的所有对象中共享的成员，而不是某个对象成员。并且使用静态数据成员不会破坏隐藏的原则，保证了数据的安全性。 类的静态函数：把函数成员声明为静态的，就可以把函数与类的任何特定对象独立开来。静态成员函数即使在类对象不存在的情况下也能被调用，静态函数只要使用类名加范围解析运算符 :: 就可以访问(::()) 类的 static 变量在什么时候初始化？函数的 static 变量在什么时候初始化？ 类的静态成员变量在类实例化之前就已经存在了，并且分配了内存。函数的static 变量在执行此函数时进行初始化。 C++ 变量作用域 作用域即是程序的一个区域，在程序中变量的作用域一般有三个地方： 在函数或者一个代码块内部声明的变量，称为局部变量； 在函数参数中定义的变量，称为形参； 在所有函数外部声明的变量，比如在程序文件开头定义的变量，称为全局变量。 C++ 指针和引用的区别 指针有自己的内存空间，而引用只是一个别名，类似于Python浅拷贝和深拷贝的区别 不存在空引用, 引用必须链接到一块合法的内存地址； 一旦引用被初始化为一个对象，就不能指向另一个对象。指针可以在任何时候指向任何一个对象； 引用必须在创建时被初始化。指针可以在任何时间初始化。 指针可以有多级，但是引用只能是一级（int **p；合法 而 int &&a 是不合法的）。 C++ 中析构函数的作用 析构函数与构造函数对应，类的析构函数是类的一种特殊的成员函数，它会在每次删除所创建的对象时执行。析构函数的名称与类的名称是完全相同的，只是在前面加了个波浪号（~）作为前缀，它不会返回任何值，也不能带有任何参数。析构函数有助于在跳出程序（比如关闭文件、释放内存等）前释放资源。 C++ 静态函数和虚函数的区别 静态函数在编译的时候就已经确定运行时机，虚函数在运行的时候动态绑定。虚函数因为用了虚函数表机制，调用的时候会增加一次内存开销。 ++i 和 i++ 区别 ++i 先自增1，再返回，i++，先返回 i，再自增1. const 关键字作用 const类型的对象在程序执行期间不能被修改改变。 C++如何传递数组给函数 首先要知道的是，C++ 传数组给一个函数，该数组类型会自动转换为指针，因此实际传递的是地址。 一维数组作为形参有以下三种方式，多维数组作为形参类似。 形参是一个指针； 形参是已定义大小的数组； 形参是未定义大小的数组。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/":{"url":"3-data_structure-algorithm/","title":"3. 数据结构和算法","keywords":"","body":"前言 本知识点内容都是《数据结构与算法之美》 的学习笔记，与专栏的一些区别在于： 笔记都是浓缩的知识，语言更为简洁； 代码都是基于 C++ 编写，与专栏的 JAVA 代码有所区别； 每个知识点附带对应 leetcode 练习题，部分可直接运行代码； 部分知识点有个人总结和思考. 总之，这份学习笔记，是属于我个人学习专栏之后做的知识记录，比较初学者适合一开始不想学习大块头的数据结构和算法知识，想要快速语言学习相关知识，或者想要利用 C++ 实现文章代码时，可作参考作用。 目录 1，常见数据结构 数组 链表 字符串 栈 队列 跳表 散列表 散列表上-理论.md) 散列表中-实战.md) 散列表下-散列表和链表的组合.md) 哈希算法 二叉树 二叉树遍历-二叉树遍历.md) 二叉查找树-二叉查找树.md) 红黑树-基础.md) 图 图的表示 广度和深度优先搜索 数组和链表都是最基础的物理结构，栈和队列是抽象的逻辑结构；二叉树、散列表、图等结构底层都是基于数组、链表的为适应特定场景开发的。 2，算法 递归 二分查找 排序 贪心算法 分治算法 回溯算法 动态规划算法 初始动态规划 动态规划理论 动态规划实战 3，剑指offer题解 剑指offer题解-C++代码解题 4，算法图解学习笔记 算法图解学习笔记 三，参考资料 《数据结构与算法之美》 《剑指 offer》 《leetcode 题解》 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/array/数组.html":{"url":"3-data_structure-algorithm/array/数组.html","title":"数组","keywords":"","body":"一，如何实现随机访问？ 数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。数组的两个特点是： 线性表 连续的内存空间和相同类型的数据（随机访问） 计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址： a[i]_address = base_address + i * data_type_size // 如果数组从 1 开始计数 a[k]_address = base_address + (k-1)*type_size 数组从 1 开始编号，是因为根据寻址公式，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。 注意：数组支持随机访问，根据下标随机访问的时间复杂度为 $O(1)$，但找的时间复杂度并不为 $O(1)$。，毕竟即便是排好序的数组，用二分查找，时间复杂度也是 $O(logn)$。 这点很多博客没有注明。 二，低效的“插入”和“删除” 假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位。平均情况时间复杂度为 $(1+2+...n)/n=O(n)$。 三，容器能否完全替代数组？ C++、JAVA 等语言都提供了类似数组的容器类，比如 C++ STL 中的 vector。根据我的经验，一般情况下，推荐用容器开发，简单省事、代码易懂，底层和高性能开发用数组，毕竟容器的灵活性还是牺牲了一定性能的。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/linked_list/链表.html":{"url":"3-data_structure-algorithm/linked_list/链表.html","title":"链表","keywords":"","body":"一，单链表结构定义 C/C++ 数组：一组具有相同类型数据的集合。结构体：不同类型数据的集合。 // Definition for singly-linked list. struct ListNode { int val; ListNode *next; ListNode(int x) : val(x), next(NULL) {} }; 二，写链表代码的技巧 2.1，技巧一：理解指针或引用的含义 指针：指针存放某个对象地址。将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针。 2.2，警惕指针丢失和内存泄露 插入节点时，一定要注意操作是顺序； 删除链表时，也一定要记住手动释放内存空间。 2.3，利用哨兵简化实现难度 针对链表的插入、删除操作，需要对插入第一个节点和删除最后一个节点的情况进行特殊处理。head = null 表示链表中没有节点，head 表示头节点，指向链表中的第一个节点。 引入哨兵节点，哨兵节点不管链表是否空，head 指针都会已知指向哨兵节点。有哨兵节点的链表叫作带头链表，反之则是不带头链表。 利用哨兵简化编程难度的技巧再插入排序、归并排序、动态规划等代码中都有用到。 2.4，重点留意边界处理条件 写链表代码一定要检查以下情况下，代码逻辑能否正常工作。 链表为空； 链表只有一个节点； 代码处理头节点和尾节点 2.5，举例画图，辅助思考 处了举例画图辅助我们思考问题之外，剩下最关键的就是多写多练、熟能生巧。 5 个常见链表操作如下： 单链表反转 链表中环的检测 两个有序链表的合并 删除链表倒数第 n 个节点 求链表的中间节点 参考资料 链表（下）：如何轻松写出正确的链表代码？ Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/string/字符串解题模板.html":{"url":"3-data_structure-algorithm/string/字符串解题模板.html","title":"字符串","keywords":"","body":"模板代码 字符串处理这类题目可以分为两类，一类是有前置或者后置空格的，另一类是没有前置和后置空格的。 1、如果有前后置空格，那么必须判断临时字符串非空才能输出，否则会输出空串。模板如下： // 模板代码 s += \" \"; //这里在最后一个字符位置加上空格，这样最后一个字符串就不会遗漏 string temp = \"\"; //临时字符串 vector res; //存放字符串的数组 for (char ch : s) //遍历字符句子 { if (ch == ' ') //遇到空格 { if (!temp.empty()) //临时字符串非空 { res.push_back(temp); temp.clear(); //清空临时字符串 } } else temp += ch; } 2、没有前后置的空格不需要判断空串。模板如下： s += \" \"; string temp = \"\"; vector res; for (char ch : s) { if (ch == ' ') { res.push_back(temp); temp.clear(); } else temp += ch; } 参考资料 作者：eh-xing-qing Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/stack_queue_heap/栈-先进后出.html":{"url":"3-data_structure-algorithm/stack_queue_heap/栈-先进后出.html","title":"栈","keywords":"","body":"一，概述 栈和队列都是一种\"操作受限\"的线性表（逻辑结构），只允许在一端插入和删除数据；栈的特性是先进后出，队列是先进先出。在项目中当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，这时应当首选\"栈\"这种数据结构。 栈的实现有多种方式，可以用数组也可以用链表实现。基于数组实现的栈的C++代码如下： class ArrayStack{ private: int items[]; // 定义一个数组 int count; // 栈中元素个数 int n; // 栈的大小 public: // 构造函数，利用数组初始化栈，栈的大小为 n ArrayStack(int n) { this.items = new int[n]; this.n = n; this.count = 0; } // 入栈操作 bool push(int item){ if(count == n) return False; // 将插入的元素 item 放在下标为 count 的位置，并且 count 加一 items[count] = item; ++count; return True; } // 出栈操作 bool pop(){ if(count==0) return null; // 返回下标为 count-1 的数组元素，并且栈中元素减一 int temp = items[count-1]; --count; return temp; } } 二，单调栈 单调栈模板题解法：单调栈解题模板秒杀八道题 参考资料 《数据结构与算法之美》-栈 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/stack_queue_heap/队列-先进先出.html":{"url":"3-data_structure-algorithm/stack_queue_heap/队列-先进先出.html","title":"队列","keywords":"","body":"一，概述 队列这个概念非常好理解。你可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的“队列”。 二，顺序队列和链式队列 队列和栈一样，也是一种抽象的数据结构，操作上具有“先进先出”的特性，队列只允许在\"队首\"进行删除操作，而在\"队尾\"进行插入操作。基于数组实现的顺序队列的C++代码如下： // 用数组实现的队列 class ArrayQueue(){ // 数组：items，数组大小：n private: int n = 20; int head = 0; // 队头下标 int tail = 0; // 队尾下标 public: // 带参数的构造函数：申请一个大小为 capacity 的数组 ArrayQueue(int capacity){ // items = new int[capacity]; vector items(capacity); n = capacity; } // 入队 bool enqueue(int item){ if(tail == n) return False; items[tail] = item; ++tail; return True; } // 时间复杂度为O(1)的入队操作 bool enqueue2(int item){ // tail == n，表示队列末尾没有空间了 if(tail == n){ // tail == n && head == 0，表示整个队列都占满了 if(head == 0) return False; // 数据搬移 for(i=head; i 入队时间复杂度为 O(1)。分析：大部分情况下入队操作时间复杂度为 O(1)，只有在 tail 在末尾时( tail=n )才进行数据迁移，此时的入队操作时间复杂度为 O(n)，根据均摊时间复杂度得到入队时间复杂度为 O(1)。 三，循环队列 前面用数组实现队列，当 tail = n 时，会有数据搬移操作。循环队列首尾相连，用数组实现循环队列代码的关键在于判断队列满和空的条件。 非循环队列: 队满：tail = n 队空：head = tail 循环队列： 队满：(tail + 1) % n = head 队空：head = tail 基于数组实现的循环队列的C++代码如下： // 用数组实现的循环队列，关键在于创建队头和队尾下标 class CircularQueue(){ private: int n = 12; int items[]; // head表示队头下标，tail表示队尾下标 int head = 0; int tail = 0; public: CircularQueue(int capacity){ // items = new int[capacty]; vector items(capacity); n = capacity; } // 入队函数 bool enqueue(int item){ // 队列满了 if((tail+1)%n = head) return False; items[tail] = item; tail = (tail + 1) % n } // 出队函数 int dequeue(){ // // 如果head == tail 表示队列为空 if(head == tail) return null; int ret = items[head]; head = (head + 1) % n; return ret; } } 四，阻塞队列和并发队列 阻塞队列就是入队、出队操作都可以阻塞，简单来说就是队列为空时，队首取数据会被阻塞，队列为满时，队尾插入数据会被阻塞，直到队列有空闲数据才允许在队尾插入数据。使用阻塞队列结构可以轻松实现“消费者-生产者模型”。 并发队列就是队列的操作多线程安全。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。 参考资料 《数据结构与算法之美》-队列 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/skip_list/跳表.html":{"url":"3-data_structure-algorithm/skip_list/跳表.html","title":"跳表","keywords":"","body":"一，如何理解跳表 简单说跳表（Skip list）就是链表的“二分查找”。redis 的有序集合用的就是跳表算法。跳表是一种各方面性能都比较优秀的动态数据结构，可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）。 如下图所示，这种链表加多级索引的结构，就是跳表。从图中我们可以看出，原来没有索引的时候，查找 62 需要遍历 62 个结点，现在只需要遍历 11 个结点，速度是不是提高了很多？所以，当链表的长度 n 比较大时，比如 1000、10000 的时候，在构建索引之后，查找效率的提升就会非常明显。 如果包含原始链表这一层，整个跳表的高度就是 $log2n$。我们在跳表中查询某个数据的时候，如果每一层都要遍历 $m$ 个结点，那在跳表中查询一个数据的时间复杂度就是 $O(m*logn)$。这里的 $m$ 为 $3$。为什么是 3 呢？解释如下。 假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。 通过上面的分析，我们得到 m=3，所以在跳表中查询任意数据的时间复杂度就是 $O(logn)$。 二，跳表的空间复杂度 比起单纯的单链表，跳表需要存储多级索引，肯定要消耗更多的存储空间。 跳表的空间复杂度分析并不难，假设原始链表大小为 n，那第一级索引大约有 n/2 个结点，第二级索引大约有 n/4 个结点，以此类推，每上升一级就减少一半，直到剩下 2 个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。 这几级索引的结点总和就是 n/2+n/4+n/8…+8+4+2=n-2。所以，跳表的空间复杂度是 $O(n)$。也就是说，如果将包含 n 个结点的单链表构造成跳表，我们需要额外再用接近 n 个结点的存储空间。 在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。 三，高效的动态插入和删除 前问叙述了跳表的结构定义和查找数据，实际上，跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 $O(logn)$。 在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是 $O(1)$。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。 对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，我们讲过查找某个结点的时间复杂度是 $O(logn)$，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 $O(logn)$。 四，跳表索引动态更新 当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。 五，总结 跳表使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的“二分查找”。跳表是一种动态数据结构，支持快速地插入、删除、查找操作，时间复杂度都是 $O(logn)$。跳表的空间复杂度是 $O(n)$。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向用跳表。 参考资料 数据结构与算法之美-跳表 跳表(SkipList)设计与实现 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/hash_table/哈希算法.html":{"url":"3-data_structure-algorithm/hash_table/哈希算法.html","title":"哈希算法","keywords":"","body":"一，什么是哈希算法 哈希和散列其实意思是一样的，只是中文翻译的区别，英文是 Hash。 哈希算法也叫 hash 算法或散列算法。哈希算法的定义：将任意长度的二进制串映射为固定长度（一般是 128 bit）的二进制串，这个映射的规则就是哈希算法。而通过原始数据映射之后得到的二进制值串就是哈希值。 一个优秀的哈希算法一般需要满足以下几点要求（来源：《数据结构和算法之美》作者-王争的经验）： 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）； 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同； 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小； 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。 hash(key) 中的 key 可以是字符串、数字、对象等，但其底层都是二进制串。 对应的哈希算法的特点如下： 不可逆，即通过密文无法反推生成明文。 原始字符串不一样，得到的哈希结果不一样。 冲突的概率要很小。 执行效率要高，理论上来说执行时间越长的冲突概率越小。 二，哈希算法的应用场景 1，安全加密。 最常用于加密的哈希算法是 MD5（MD5 Message-Digest Algorithm，MD5 消息摘要算法）和 SHA（Secure Hash Algorithm，安全散列算法）。 对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。 对于第二点要求，实际上，不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。为什么这么说呢？依据是基于组合数学中一个非常基础的理论：鸽巢原理（也叫抽屉原理）。鸽巢原理： 槽是固定的，数据是无限的（不定的），冲突是有概率存在的。 2，唯一标识。 用在图片上的情况比较多，如果同一张图片上传多次，会通过哈希算法对图片的二进制内容进行计算，以计算结果为标识一张图片是否已经上传过。如果一张图片过大，可以采用部分二进制，比如开头 100K，中间 100K ，最后 100K 相加进行哈希算法来保证一定的执行效率。 3，文件完整性校验。 比如迅雷等 p2p 下载器，从不同主机下载文件分片最终在本地合成一个文件。种子文件里面一般存储了各个文件分片的哈希结果，下载完成后通过本地计算各个分片的哈希再跟种子里面存储的核实来确保文件分片没有被恶意更改过。 4，散列函数。 在散列表中需要的哈希算法，一般对执行效率要求高，对是否冲突要求比较低，因为散列表通常都会有冲突的解决方式，比如开放寻址法跟链表法。 5，负载均衡。 何谓一个会话粘滞（session sticky）的负载均衡算法？即在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。 通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个 IP 过来的所有请求，都路由到同一个后端服务器上。 6，数据分片。 哈希算法用于数据分片的一个例子是统计“搜索关键字”出现的次数。 假如有一个 1T 的日志文件，这里面记录了用户的搜索关键词，如果想要快速统计出每个关键词被搜索的次数，解决方案是什么？ 分析问题：1，1T 的日志文件很大，内存中基本不可能放下这么多数据；2，数据很多，1 台机器处理速度会很慢。 解题思路：参考 Map-Reduce 原理。先对数据进行切片，然后使用多台机器并行处理。 解题方案:使用 n 台机器进行数据处理，首先从日志文件中顺序依次读取每个关键字，并用哈希函数计算得到哈希值，并跟 n 取模，取模得到的值就是对应的机器编号，这样哈希值相同的关键字就被分配到同一台机器上进行处理。 最后每台机器分配统计关键字出现的次数，然后合并在一起就是最终的结果。 从上可以看出，针对这种海量数据的处理问题，我们可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制。 7，分布式存储。 负载均衡、数据分片、分布式存储，这三个应用都跟分布式系统有关，也就是说哈希算法是可以解决这些分布式系统问题。 现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。 该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。 但是这会产生一个问题，就是当数据增多的情况下，机器要扩容，增加机器数量，原来的 n 假设是 10，现在变成了 20，这样按照之前的方法所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。 所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。 这就是一致性哈希算法。假设我们有 $k$ 个机器，数据的哈希值的范围是$[0, MAX]$，我们将整个范围划分成 $m$ 个小区间（$m$ 远大于 $k$），那么每个机器就负责 $m/k$ 个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。 总结 在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。 参考资料 《数据结构与算法之美-王争》-哈希算法（下）：哈希算法在分布式系统中有哪些应用？ Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/map/图的表示.html":{"url":"3-data_structure-algorithm/map/图的表示.html","title":"图的表示","keywords":"","body":"一，图的理解 图和树一样都是非线性表数据结构，但是更复杂。树中的元素我们称为节点，图中的元素我们叫作顶点（vertex）。图中的一个顶点可以与任意其他顶点建立连接关系，这种连接关系叫作边（edge）。有方向的图叫做“有向图”。以此类推，我们把边没有方向的图就叫做“无向图”。 在有向图中，我们把度分为入度（In-degree）和出度（Out-degree）。顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。通过方向可以用来表示微博的粉丝量和关注量，入度表示有多少粉丝，出度表示关注了多少人。 每条边都有一个权重（weight）的无向图叫作带权无向图（weighted graph），通过这个权重可以用来表示QQ好友间的亲密度。 二，邻接矩阵存储方法 图最直观的一种存储方法就是，邻接矩阵（Adjacency Matrix）。 邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 A[i][j] 和 A[j][i] 标记为 1；对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 A[i][j] 标记为 1。同理，如果有一条箭头从顶点 j 指向顶点 i 的边，我们就将 A[j][i] 标记为 1。对于带权图，数组中就存储相应的权重。 邻接矩阵存储图虽然存储方式简单、也方便计算，但是在一些情况下会造成空间的浪费。 如果用邻接矩阵存储无向图会造成存储空间的浪费，因为对于无向图来说，如果 A[i][j] 等于 1，那 A[j][i] 也肯定等于 1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了。 还有，如果我们存储的是稀疏图（Sparse Matrix），也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了。 如果有 n 个顶点，所需构建的二维矩阵就是 n*n，如果每个顶点的边不是很多，就是造成矩阵的很多元素都是 0，从而导致存储空间的浪费。 三，邻接表存储方法 针对上面邻接矩阵比较浪费内存空间的问题，我们来看另外一种图的存储方法，邻接表（Adjacency List）。 邻接表的存储关系如下图所示。 图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。 邻接表的本质是用时间换空间，邻接矩阵存储起来比较浪费空间，但是使用起来比较节省时间。相反，邻接表存储起来比较节省空间，但是使用起来就比较耗时间。 就像图中的例子，如果我们要确定，是否存在一条从顶点 2 到顶点 4 的边，那我们就要遍历顶点 2 对应的那条链表，看链表中是否存在顶点 4。而且，链表的存储方式对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。 邻接表结构长得像散列表，因此也可对邻接表进行改进升级。我们可以将邻接表中的链表改成平衡二叉查找树。实际开发中，我们可以选择用红黑树。这样，我们就可以更加快速地查找两个顶点之间是否存在边了。当然，这里的二叉查找树可以换成其他动态数据结构，比如跳表、散列表等。除此之外，我们还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间否是存在边。 总结 邻接矩阵存储方法的缺点是比较浪费空间，但是优点是查询效率高，而且方便矩阵运算。邻接表存储方法中每个顶点都对应一个链表，存储与其相连接的其他顶点。尽管邻接表的存储方式比较节省存储空间，但链表不方便查找，所以查询效率没有邻接矩阵存储方式高。针对这个问题，邻接表还有改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/map/广度和深度优先搜索.html":{"url":"3-data_structure-algorithm/map/广度和深度优先搜索.html","title":"广度和深度优先搜索","keywords":"","body":"一，什么是搜索算法 算法是基于特定数据结构之上的，深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的。 树是图的一种特例（连通无环的图就是树）。 图上的搜索算法，最直接的理解就是，在图中找出从一个顶点出发，到另一个顶点的路径。具体方法有很多，两种最简单、最“暴力”的深度优先、广度优先搜索，还有 A*、IDA* 等启发式搜索算法。深度优先搜索算法和广度优先搜索算法，既可以用在无向图，也可以用在有向图上。 图（采用邻接表存储）的 C++ 代码实现如下： #include // 无向图结构的定义 class Graph{ private: int v; // 顶点个数 list adj() // 存储的邻接表 public: // 构造函数定义 Graph(int v){ adj = new List(v); for (int i = 0; i (); } } // 无向图一条边存储 2 次 void addEdge(int s, int t){ adj[s].push_back(t); adj[t].push_back(s); } } 二，广度优先搜索（BFS） 广度优先搜索（Breadth-First-Search），我们平常都简称 BFS。直观地讲，它其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。 求图中起始顶点 s 到终止顶点 t 的最短路径，可使用 bfs 算法，代码如下： #include using namespace std; void bfs(int s, int t){ if( s==t ) return; bool visited[v]; // 用来记录已经访问过的顶点, v 表示顶点个数 queue queue; // 一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。 queue.push_back(s); // 添加起始顶点 vector prev(v, -1); // prev 用来记录搜索路径, prev[w]存储的是，顶点 w 是从哪个前驱顶点遍历过来的。 while(queue.size() != 0){ int w = queue.pop(); for(int i= 0; i queue 是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。因为广度优先搜索是逐层访问的，也就是说，我们只有把第 k 层的顶点都访问完成之后，才能访问第 k+1 层的顶点。当我们访问到第 k 层的顶点的时候，我们需要把第 k 层的顶点记录下来，稍后才能通过第 k 层的顶点来找第 k+1 层的顶点。所以，我们用这个队列来实现记录的功能。 prev 用来记录搜索路径。当我们从顶点 s 开始，广度优先搜索到顶点 t 后，prev 数组中存储的就是搜索的路径。不过，这个路径是反向存储的。prev[w]存储的是，顶点 w 是从哪个前驱顶点遍历过来的。比如，我们通过顶点 2 的邻接表访问到顶点 3，那 prev[3]就等于 2。为了正向打印出路径，我们需要递归地来打印。 最坏情况下，终止顶点 t 离起始顶点 s 很远，需要遍历完整个图才能找到。这个时候，每个顶点都要进出一遍队列，每个边也都会被访问一次，所以，广度优先搜索的时间复杂度是 $O(V+E)$，其中，V 表示顶点的个数，E 表示边的个数。当然，对于一个连通图来说，也就是说一个图中的所有顶点都是连通的，E 肯定要大于等于 V-1，所以，广度优先搜索的时间复杂度也可以简写为 $O(E)$。 广度优先搜索的空间消耗主要在几个辅助变量 visited 数组、queue 队列、prev 数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是 $O(V)$。 三，深度优先搜索（DFS） 深度优先搜索（Depth-First-Search），简称 DFS。 最直观的例子就是“走迷宫”。假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。 bool found = false; // 类成员变量 void dfs(int s, int t){ found = false; bool visited[v]; // v 表示顶点个数 int prev[v]; for(int i=0; i 四，内容总结 广度优先搜索，通俗的理解就是，地毯式层层推进，从起始顶点开始，依次往外遍历。广度优先搜索需要借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。在执行效率方面，深度优先和广度优先搜索的时间复杂度都是 $O(E)$，空间复杂度是 $O(V)$。 参考资料 《数据结构与算法之美》-深度和广度优先搜索 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/two_pointers/":{"url":"3-data_structure-algorithm/two_pointers/","title":"双指针","keywords":"","body":"双指针算法 双指针从广义上来说，是指用两个变量在线性结构上遍历而解决的问题。狭义上说， 对于数组，指两个变量在数组上相向移动解决的问题； 对于链表，指两个变量在链表上同向移动解决的问题，也称为「快慢指针」问题。 双指针算法是基于暴力解法的优化。 一些思考 感觉和栈、队列和堆有关的题目，如果有必要定义一个数据结构，那么则最好是单调的，比如单调递减队列。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/recursion/递归-需要满足三个条件.html":{"url":"3-data_structure-algorithm/recursion/递归-需要满足三个条件.html","title":"递归","keywords":"","body":"一，概述 递归是一种应用非常广泛的算法（或者编程技巧）。很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等。 去的过程叫“递”，回来的过程叫“归”。基本上所有的递归问题都可以用递推公式来表示。 递归需要满足的三个条件： 一个问题的解可以分解为几个子问题的解； 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样； 存在递归终止条件。 二，常见的递归问题 斐波那契数列：递推公式：f(n) = f(n-1) + f(n-2) 跳台阶问题：递推公式：f(n) = f(n-1) + f(n-2) 斐波那契数列问题的”记忆化递归“实现代码如下： // 1，直接递归会超出时间限制，需要使用记忆化递归 int fib(int n) { if (n == 0) return 0; if (n == 1 || n == 2) return 1; if (vec[n] != -1) return vec[n]; vec[n] = (fib(n - 1) + fib(n - 2)) % mod; return vec[n]; } 二，如何编写递归代码 递归问题的层层调用分析是不符合人类直觉的，因此没必要用人脑去分解递归代码的每个步骤，正确的做法是，遇到递归问题就拆分问题并抽象成递归公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。 三，递归代码要警惕堆栈溢出 递归代码涉及到函数调用，函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。 通过在代码中限制递归调用的最大深度的方式一定程度上可以解决堆栈溢出的问题。伪代码如下： // 全局变量，表示递归的深度。 int depth = 0; int f(int n) { ++depth； if (depth > 1000) throw exception; if (n == 1) return 1; return f(n-1) + 1; } 但这种做法并不能完全解决问题，因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。所以，如果最大深度比较小，比如 10、50，就可以用这种方法，否则这种方法并不是很实用。 四，递归代码要警惕重复计算 为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的 $f(k)$。当递归调用到 $f(k)$ 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算。 这种”递归+备忘录（记忆化递归）“的方法相比简单的递归，可以减少时间复杂度，本质是用空间换时间。 五，总结 递归是一种非常高效、简洁的编码技巧。只要是满足“三个条件”的问题就可以通过递归代码来解决。 但是递归代码也比较难写、难理解。编写递归代码的关键就是不要把自己绕进去，正确姿势是写出递推公式，找出终止条件，然后再翻译成递归代码。 递归代码虽然简洁高效，但是，递归代码也有很多弊端。比如，堆栈溢出、重复计算、函数调用耗时多、空间复杂度高等，所以，在编写递归代码的时候，一定要控制好这些副作用。 参考资料 《数据结构与算法之美》-递归 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/binary_search/二分查找.html":{"url":"3-data_structure-algorithm/binary_search/二分查找.html","title":"二分查找","keywords":"","body":"一，简单的二分查找算法 二分查找（Binary Search）算法是一种针对有序且不含重复数据集合的查找算法，时间复杂度为 $O(logn)$ ，二分查找虽然性能比较优秀，但应用场景也比较有限。 因为底层依赖于数组这种结构，所以不适合数据量大的情况。再次，对于较小规模的数据查找，二分查找的优势并不明显，一般直接使用顺序遍历就可以了。二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。 如果数据使用链表存储，二分查找的时间复杂就会变得很高，变成了 $O(n)$。 简单的二分查找算法的 C++ 代码如下： // 非递归实现 int BinarySearch(int a[], int n, int value){ int low = 0, high = n-1; int mid = (low+high)/2; while( low >1) mid = (low+high)/2; if( a[mid] == value ){ return mid; } else if (value 注意：二分查找算法的核心是在于利用二分思想，每次都与区间的中间数据比对大小，缩小查找区间的范围。二分查找算法除了用上面的循环实现，实际上还可以用递归实现。简单二分查找算法的递归代码如下： int BinarySearch(int a[], int low, int high, int value){ if (low > high) return -1; int mid = low + ((high - low) >> 1); if a[mid] = value return mid; else if (a[mid] > value) return BinarySearch(a, low, mid-1, value); else return BinarySearch(a, mid + 1, high, value); } 二，二分查找算法的变形 4 种常见的二分查找变形问题： 查找第一个值等于给定值的元素； 查找最后一个值等于给定值的元素； 查找第一个值大于等于给定值的元素； 查找最后一个值小于等于给定值的元素； 其实以上四个问题都针对的是有序数据集合中存在重复的数据的情况 因为是有序数组，所以这个重复的隐含线索是连续重复数据。 2.1，变体一：查找第一个值等于给定值的元素 根据前文实现的简单二分查找代码，我们知道 a[mid] 跟要查找的 value 的大小关系有三种情况：大于、小于、等于。对于 a[mid]>value 的情况，我们需要更新 high= mid-1；对于 a[mid] 的情况，我们需要更新 low=mid+1。这两点和简单二分查找代码一样，但是当 a[mid]=value 时，二分查找算法的变形有着不同的处理形式。 二分查找问题不同的人有着不同的解决方法，第一个二分查找变形问题的简洁实现方法如下： int BinarySearch(int a[], int n, int value){ int low = 0, high = n-1; while(low > 1); if(a[mid] > value){ high = mid - 1; } else if(a[mid] 2.2，变体二：查找最后一个值等于给定值的元素 这个问题和上个问题类似，只需在上面代码基础上，稍作修改即可。 int BinarySearch(int a[], int n, int value){ int low = 0, high = n-1; while(low > 1); if(a[mid] > value){ high = mid - 1; } else if(a[mid] 2.3，变体三：查找第一个大于等于给定值的元素 实际上，实现的思路跟前面的那两种变形问题的实现思路是类似的，但是代码写起来甚至更简洁，因为考虑的情况要少一种了，a[mid] > value 和 a[mid] = value 可以放在一个 if 分支里面，分支里面的处理语句和前面代码类似。 int BinarySearch(int a[], int n, int value){ int low = 0, high = n-1; while(low > 1); if(a[mid] >= value){ if(mid == 0 || a[mid - 1] 如果 a[mid] 小于要查找的值 value，那要查找的值肯定在[mid+1, high]之间，所以，我们更新 low=mid+1。 对于 a[mid]大于等于给定值 value 的情况，我们要先看下这个 a[mid]是不是我们要找的第一个值大于等于给定值的元素。如果 a[mid]前面已经没有元素，或者前面一个元素小于要查找的值 value，那 a[mid]就是我们要找的元素。这段逻辑对应的代码是第 7 行。 如果 a[mid-1]也大于等于要查找的值 value，那说明要查找的元素在[low, mid-1]之间，所以，我们将 high 更新为 mid-1。 2.4，变体四：查找最后一个小于等于给定值的元素 有了前面的基础，这个问题的代码就可以直接写出来了，复制上文代码，修改第 7、8 行即可。 int BinarySearch(int a[], int n, int value){ int low = 0, high = n-1; while(low > 1); if(a[mid] > value){ high = mid - 1; } else { if(mid == high || a[mid + 1] > value) return mid; else low = mid + 1; } } return -1; 三，总结 二分查找的核心思想理解起来非常简单，有点类似分治思想。即每次都通过跟区间中的中间元素对比，将待查找的区间缩小为一半，直到找到要查找的元素，或者区间被缩小为 0。其代码有三个容易出错的地方：循环退出条件、mid 的取值，low 和 high 的更新。 凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或者二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。 四，二分查找算法解题模板 二分查找模板1 将区间 [l, r] 划分成 [l, mid] 和 [mid+1, r] 时，其更新操作是 r = mid 或 l = mid + 1;，计算 mid 时不需要加1. int bsearch_1(int l, int r){ while (l > 1; // (l + r)/2 向下取整 if(check(mid)) r = mid; else l = mid + 1; } return l; } 二分查找模板2 将区间 [l, r] 划分成 [l, mid-1] 和 [mid, r] 时，其更新操作是 r = mid - 1 或 l = mid;，为了防止死循环，计算 mid 时需要加1. int bsearch_1(int l, int r){ while (l > 1; // (l + r)/2 向上取整 if(check(mid)) l = mid; else r = mid - 1; } return l; } Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/sort/排序.html":{"url":"3-data_structure-algorithm/sort/排序.html","title":"排序","keywords":"","body":"一，冒泡排序（Bubble Sort） 排序算法是程序员必须了解和熟悉的一类算法，排序算法有很多种，基础的如：冒泡、插入、选择、快速、归并、计数、基数和桶排序等。 冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求，如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。 总结：如果数组有 n 个元素，最坏情况下，需要进行 n 次冒泡操作。 基础的冒泡排序算法的 C++ 代码如下： // 将数据从小到大排序 void bubbleSort(int array[], int n){ if (n a[j+1]){ temp = array[j] a[j] = a[j+1]; a[j+1] = temp; } } } } 实际上，以上的冒泡排序算法还可以优化，当某次冒泡操作已经不再进行数据交换时，说明数组已经达到有序，就不需要再继续执行后续的冒泡操作了。优化后的代码如下： // 将数据从小到大排序 void bubbleSort(int array[], int n){ if (n a[j+1]){ temp = array[j] a[j] = a[j+1]; a[j+1] = temp; flag = True; // 表示本次冒泡操作存在数据交换 } } if(!flag) break; // 没有数据交换，提交退出 } } 冒泡排序的特点： 冒泡过程只涉及相邻元素的交换，只需要常量级的临时空间，故空间复杂度为 O(1)，是原地排序算法。 当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以是稳定排序算法。 最坏情况和平均时间复杂度都为 $O(n^2)$，最好时间复杂度是 $O(n)$。 二，插入排序（Insertion Sort） 插入排序算法将数组中的数据分为两个区间：已排序区间和未排序区间。最初始的已排序区间只有一个元素，就是数组的第一个元素。 插入排序算法的核心思想就是取未排序区间的一个元素，在已排序区间中找到一个合适的位置插入，并保证已排序区间数据一直有序。 重复这个过程，直到未排序区间元素为空，则算法结束。 插入排序和冒泡排序一样，也包含两种操作，一种是元素的比较，一种是元素的移动。 当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素 a 插入。 插入排序的 C++ 代码实现如下： void InsertSort(int a[], int n){ if (n = 0){ // 元素比较 a[j+1] = a[j]; // 数据向后移动一位 j--; } a[j+1] = key; // 插入数据 } } 插入排序的特点： 插入排序并不需要额外存储空间，空间复杂度是 O(1)，所以插入排序也是一个原地排序算法。 在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。 最坏情况和平均时间复杂度都为 $O(n^2)$，最好时间复杂度是 $O(n)$。 三，选择排序（Selection Sort） 选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。 选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n2)，是原地排序算法，且是不稳定的排序算法。 选择排序的 C++ 代码实现如下： void SelectSort(int a[], int n){ for(int i=0; i 冒泡插入选择排序总结 这三种排序算法，实现代码都非常简单，对于小规模数据的排序，用起来非常高效。但是在大规模数据排序的时候，这个时间复杂度还是稍微有点高，所以更倾向于用时间复杂度为 O(nlogn) 的排序算法。 特定算法是依赖特定的数据结构的。以上三种排序算法，都是基于数组实现的。 四，归并排序（Merge Sort） 归并排序的核心思想比较简单。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。 归并排序使用的是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。 分治思想和递归思想有些类似，分治算法一般用递归实现。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。 知道了归并排序用的是分治思想，而分治思想一般用递归实现，接下来的重点就是如何用递归实现归并排序。写递归代码的技巧就是，分析问题得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码。所以，要想写出归并排序的代码，得先写出归并排序的递推公式。 递推公式： merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r)) 终止条件： p >= r 不用再继续分解，即区间数组元素为 1 归并排序的伪代码如下： merge_sort(A, n){ merge_sort_c(A, 0, n-1) } merge_sort_c(A, p, r){ // 递归终止条件 if (p>=r) then return // 取 p、r 中间的位置为 q q = (p+r)/2 // 分治递归 merge_sort_c(A[p, q], p, q) merge_sort_c(A[q+1, r], q+1, r) // 将A[p...q]和A[q+1...r]合并为A[p...r] merge(A[p...r], A[p...q], A[q+1...r]) } 4.1，归并排序性能分析 1，归并排序是一个稳定的排序算法。分析：伪代码中 merge_sort_c() 函数只是分解问题并没有涉及移动元素和比较大小，真正的元素比较和数据移动在 merge() 函数部分。在合并过程中保证值相同的元素合并前后的顺序不变，归并排序排序就是一个稳定的排序算法。 2，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 $O(nlogn)$。分析：不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式： $$T(n) = 2*T(n/2) + n; \\;n>1$$ $$ \\begin{aligned} T(n) &= 2T(n/2) + n \\ &= 2(2T(n/4) + n/2) + n = 4T(n/4) + 2n \\ &= 4(2T(n/8) + n/4) + 2n = 8T(n/8) + 3n \\ &= 8(2T(n/16) + n/8) + 3n = 16T(n/16) + 4n \\ &...... \\ &= 2^k T(n/2^k) + k * n \\ &...... \\end{aligned}$$ 一步步分解推导可得 $T(n)= 2^k T(n/2^k) + k n$ 。当 $T(n/2^k)=T(1)$ 时，也就是 $n/2^k=1$，我们得到 $k=log2n$ 。我们将 $k$ 值代入上面的公式，得到 $T(n)=Cn+nlog2n$ 。如果我们用大 O 标记法来表示的话，$T(n)$ 就等于 $O(nlogn)$。所以归并排序的时间复杂度是 O(nlogn)。 3，空间复杂度是 O(n)。分析：递归代码的空间复杂度并不能像时间复杂度那样累加。尽管算法的每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 $O(n)$。 五，快速排序（Quicksort） 快排的思想是这样的：如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。我们遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。 根据分治、递归的处理思想，我们可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。 递推公式如下： 递推公式： quick_sort(p,r) = quick_sort(p, q-1) + quick_sort(q, r) 终止条件： p >= r 归并排序和快速排序总结 归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归来实现，过程非常相似。理解归并排序的重点是理解递推公式和 merge() 合并函数。同理，理解快排的重点也是理解递推公式，还有 partition() 分区函数。 除了以上 5 种排序算法，还有 3 种时间复杂度是 $O(n)$ 的线性排序算法：桶排序、计数排序、基数排序。这八种排序算法性能总结如下图： 参考资料 排序（上）：为什么插入排序比冒泡排序更受欢迎？ 排序（下）：如何用快排思想在O(n)内查找第K大元素？ Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/greedy/贪心算法.html":{"url":"3-data_structure-algorithm/greedy/贪心算法.html","title":"贪心算法","keywords":"","body":"前言 到目前为止学了数组、链表两类物理数据结构，和栈、队列两类线性逻辑结构，以及树（二叉树、二叉查找树、红黑树）、图非线性数据结构，以及基于这些数据结构的二分查找、DFS/BFS、递归、排序等算法。 接下来会学习贪心、分治、回溯、动态规划这 4 个算法思想，这类算法思想都有原理容易理解，但是很难掌握和灵活应用的特点。 一，如何理解贪心算法 贪心算法（英语：greedy algorithm），又称贪婪算法，是一种在每一步选择中都采取在当前状态下最好或最优（即最有利）的选择，从而希望导致结果是最好或最优的算法。比如在旅行推销员问题中，如果旅行员每次都选择最近的城市，那这就是一种贪心算法。 贪心算法在有最优子结构的问题中尤为有效。最优子结构的意思是局部最优解能决定全局最优解。简单地说，问题能够分解成子问题来解决，子问题的最优解能递推到最终问题的最优解。 贪心算法与动态规划的不同在于它对每个子问题的解决方案都做出选择，不能回退。动态规划则会保存以前的运算结果，并根据以前的结果对当前进行选择，有回退功能。 贪心算法解决问题的步骤： 第一步，当我们看到这类问题的时候，首先要联想到贪心算法：针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。 第二步，我们尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。 第三步，我们举几个例子看下贪心算法产生的结果是否是最优的。 贪心算法实例 1，分糖果 有 m 个糖果和 n 个孩子。我们现在要把糖果分给这些孩子吃，但是糖果少，孩子多（m 所以糖果只能分配给一部分孩子。每个糖果的大小不等，这 m 个糖果的大小分别是 s1，s2，s3，……，sm。除此之外，每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。 假设这 n 个孩子对糖果大小的需求分别是 g1，g2，g3，……，gn。如何分配糖果，能尽可能满足最多数量的孩子？ 2，钱币找零 这个问题在我们的日常生活中更加普遍。假设我们有 1 元、2 元、5 元、10 元、20 元、50 元、100 元这些面额的纸币，它们的张数分别是 c1、c2、c5、c10、c20、c50、c100。我们现在要用这些钱来支付 K 元，最少要用多少张纸币呢？ 3，区间覆盖 假设我们有 n 个区间，区间的起始端点和结束端点分别是[l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。我们从这 n 个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢？ Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/divide_and_conquer/分治算法.html":{"url":"3-data_structure-algorithm/divide_and_conquer/分治算法.html","title":"分治算法","keywords":"","body":"一，如何理解分治算法 分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。 分治和递归的区别：分治算法是一种处理问题的思想，递归是一种编程技巧。 分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作： 分解：将原问题分解成一系列子问题； 解决：递归地求解各个子问题，若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题。 分治算法能解决的问题，一般需要满足下面这几个条件： 原问题与分解成的小问题具有相同的模式； 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别； 具有分解终止条件，也就是说，当问题足够小时，可以直接求解； 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。 二，分治算法案例 假设我们有 n 个数据，我们期望数据从小到大排列，那完全有序的数据的有序度就是 n(n-1)/2，逆序度等于 0；相反，倒序排列的数据的有序度就是 0，逆序度是 n(n-1)/2。 除了这两种极端情况外，如何编程求出一组数据的有序对个数或者逆序对个数呢？ 使用归并排序算法可以解决这个问题。归并排序中有一个非常关键的操作，就是将两个有序的小数组，合并成一个有序的数组。在每次合并操作中，我们都计算逆序对个数，把这些计算出来的逆序对个数求和，就是整个数组的逆序对个数了。C++ 代码如下： int num = 0; // 全局变量或者成员变量 void mergeSortCounting(int A[], int p, int r){ if(p>=r) return; int q = (p+r)/2; mergeSortCounting(A, p, q); mergeSortCounting(A, q+1, r); merge(A, p, q, r); } void merge(int[] A, int p, int q, int r) { // 两个游标 i 和 j，分别指向 A[p...q]和 A[q+1...r]的第一个元素。 int i = p, j = q+1, k = 0; int[] tmp = new int[r-p+1]; while (i A[j]，则A[i++]必然大于A[j] num += (q-i+1); // 统计p-q之间，比 A[j] 大的元素个数 tmp[k++] = A[j++]; } } // 将剩余的数据拷贝到临时数组 tmp while (i 其他两道比较经典的问题： 二维平面上有 n 个点，如何快速计算出两个距离最近的点对？ 有两个 $nn$ 的矩阵 A，B，如何快速求解两个矩阵的乘积 `C=AB`？ 分治算法用四个字概括就是“分而治之”，将原问题划分成 n 个规模较小而结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。分治算法的思想还是非常简单、好理解。 参考资料 分治算法：谈一谈大规模计算框架MapReduce中的分治思想 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/backtracking/回溯算法.html":{"url":"3-data_structure-algorithm/backtracking/回溯算法.html","title":"回溯算法","keywords":"","body":"一，如何理解回溯算法 深度优先搜索算法利用的就是回溯算法思想，但它除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。 除此之外，很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1 背包、图的着色、旅行商问题、全排列等等。 回溯的处理思想，有点类似枚举搜索。暴力枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。 回溯算法的模板代码总结如下： void backtracking(参数) { if (终止条件) { 存放结果; return; } for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) { 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 } } 二，回溯算法的经典应用 2.1，八皇后问题 有一个 8x8 的棋盘，希望往里放 8 个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子。这里的“对角线”指的是所有的对角线，不只是平分整个棋盘的那两条对角线。 解决思路：可以把这个问题划分成 8 个阶段，依次将 8 个棋子放到第一行、第二行、第三行……第八行，每一行都有 8 中放法（8 列）。在放置的过程中，我们不停地检查当前放法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种放法，继续尝试。这里用的是回溯思想，而回溯算法也非常适合用递归代码实现。 // N 皇后问题 leetcode 51 https://leetcode-cn.com/problems/n-queens/ class Solution { private: vector> result; void backtracking(int n, int row, vector& chessboard){ if(row == n) { result.push_back(chessboard); return; } for(int column=0; column & chessboard){ int leftup = column - 1; int rightup = column + 1; // 左上角和右上角 for(int i = row-1; i>=0; i--){ // 逐行网上考察每一行 // 判断第 i 行的 column 列是否有棋子 if(chessboard[i][column] == 'Q') { return false; } // 考察左上对角线：判断第i行leftup列是否有棋子 if(leftup >=0 ){ if(chessboard[i][leftup] == 'Q') return false; } // 考察左上对角线：判断第i行rightup列是否有棋子 if(rightup > solveNQueens(int n) { result.clear(); std::vector chessboard(n, std::string(n, '.')); backtracking(n, 0, chessboard); return result; } }; 2.2，0-1 背包问题 0-1 背包是非常经典的算法问题。0-1 背包问题有很多变体，这里介绍一种比较基础的。我们有一个背包，背包总的承载重量是 W kg。现在我们有 n 个物品，每个物品的重量不等，并且不可分割，即对于每个物品来说，都有两种选择，装进背包或者不装进背包，对于 n 个物品来说，总的装法就有 2^n 种。 我们现在期望选择几件物品，装载到背包中。在不超过背包所能装载重量 W 的前提下，如何让背包中物品的总重量最大？ 0-1 背包问题为什么不能用贪心算法求解？ 因为不可分割，所以无法判断当前情况下，哪种物品对期望值贡献更大，即不存在当前最优的选择，所以就无法使用贪心算法了。 0-1 背包问题的高效解法是动态规划算法，但也可用没那么高效的回溯方法求解。我们可以把物品依次排列，整个问题就分解为了 n 个阶段，每个阶段对应一个物品怎么选择。先对第一个物品进行处理，选择装进去或者不装进去，然后再递归地处理剩下的物品。 int maxW = 0; // cw 表示当前装进背包的物品的重量和，w 表示背包承载的重量 // items 表示物体的重量数组，n 表示总的物品个数， i 表示考察到第 i 个物品 int f(int i, int cw, vector items, int n, int w){ // 递归结束条件：cw == w 表示背包已经装满，i==n 表示考察完所有物品 if(cw == w || i == n){ if(cw > maxW) maxW = cw; return; } f(i+1, cw, items, n, w); // 不装 // 剪枝过程，当装入的物品重量大于背包的重量，就不继续执行 if(cw+items[i] 要理解 0-1 背包问题回溯解法的关键在于：对于一个物品而言，只有两种情况，不装入背包和装入背包两种情况。对应的就是 f(i+1, cw, items, n, w) 和 f(i+1, cw + items[i], items, n, w) 两个函数。 2.3，通配符匹配 假设正则表达式中只包含 “*” 和 “?” 这两种通配符，并且对这两个通配符的语义稍微做些改变，其中，“*” 匹配任意多个（大于等于 0 个）任意字符，“?” 匹配零个或者一个任意字符。基于以上背景假设，如何用回溯算法，判断一个给定的文本，是否和给定的正则表达式匹配？ 如果遇到特殊字符的时候，我们就有多种处理方式了，也就是所谓的岔路口，比如 “*” 有多种匹配方案，可以匹配任意个文本串中的字符，我们就先随意的选择一种匹配方案，然后继续考察剩下的字符。如果中途发现无法继续匹配下去了，我们就回到这个岔路口，重新选择一种匹配方案，然后再继续匹配剩下的字符。 // 暴力递归 --> 记忆化 --> DP --> 状态压缩DP； class Solution{ private: bool matched = false; void backtracking(int ti, int pj, string text, string pattern){ if (matched) return; if(pj == pattern.size()){ // 正则表达式到末尾了 if(ti == text.size()) matched = true; return; } // *匹配任意个字符 if(pattern[pj] == '*'){ for(int k=0; k 2.4，leetcode 正则表达式匹配 在 leetcode 也有变形题（leetcode10:正则表达式匹配）如下： 其他变形题：leetcode44-通配符匹配 给你一个字符串 s 和一个字符规律 p，请你来实现一个支持 '.' 和 '*' 的正则表达式匹配。 '.' 匹配任意单个字符 '*' 匹配零个或多个前面的那一个元素 所谓匹配，是要涵盖整个字符串 s 的，而不是部分字符串。 方法一：回溯（分阶段分情况讨论，暴力搜索和剪枝） 首先，考虑特俗字符只有 '.' 的情况。这种情况会很简单：我们只需要从左到右依次判断 s[i] 和 p[i] 是否匹配。 def isMatch(self,s:str, p:str) -> bool: \"\"\"字符串 s 和字符规律 p\"\"\" if not p: return not s # 边界条件 first_match = s and p[0] in {s[0],'.'} # 比较第一个字符是否匹配 return first_match and self.isMatch(s[1:], p[1:]) 最后，考虑有 ’*' 的情况，它会出现在 p[1] 的位置，匹配过程中会出现两种情况： 星号代表匹配 0 个前面的元素。如 '##' 和 a*##，这时我们直接忽略 p 的 a*，比较 ## 和 ##，也就是继续递归比较 s 和 p[i + 2:]； 星号代表匹配一个或多个前面的元素。如 aaab 和 a*b，这时我们将忽略 s 的第一个元素，比较 aab 和 a*b，也就是继续递归比较 s[i + 1:] 和 p。（这里默认要检查 s[0] 和 p[0] 是否相等）。 Python3 代码如下： class Solution: def isMatch(self, s: str, p: str) -> bool: if not p: return not s first_match = bool(s and p[0] in {s[0],'.'}) # 比较第一个字符是否匹配 if len(p) >=2 and p[1] == '*': # * 匹配前面一个字符 0 次或者多次 return self.isMatch(s, p[2:]) or first_match and self.isMatch(s[1:], p) else: return first_match and self.isMatch(s[1:], p[1:]) C++ 代码如下： // letcode10 正则表达式匹配 #include #include using namespace std; class Solution{ public: bool isMatch(string s, string p){ // 如果正则串 p 为空字符串，s 也为空，则匹配成功 if(p.empty()) return (s.empty()); // 判断 s 和 p 的首字符是否匹配，注意要先判断 s 不为空 bool match = (!s.empty()) && (s[0] == p[0] || p[0] == '.'); // 如果p的第一个元素的下一个元素是 *，则分别对两种情况进行判断 if(p.size() >= 2 && p[1] == '*'){ // * 匹配前面一个字符 0 次或者多次 return isMatch(s, p.substr(2)) || (match && isMatch(s.substr(1), p)); } else{ // 单个匹配 return match && isMatch(s.substr(1), p.substr(1)); } } }; 直接递归时间复杂度太大（指数级），可以把之前的递归过程记录下来，用空间换时间。记忆化递归的 C++ 代码如下： class Solution{ public: bool isMatch(string s, string p){ unordered_map memo; return backtracking(s, 0, p, 0, memo); } bool backtracking(string s, int i, string p, int j, unordered_map & memo){ // # 检查 s[i] 是否能被匹配，注意要先判断 s 不为空 bool match = (i = p.size()) return i >= s.size(); // p 和 s 同时遍历完 int key = i * (p.size() + 1) + j; // 哈希键 if (memo.find(key) != memo.end()) // 这个状态之前经历过，可以返回结果 return memo[key]; else if (i == s.size() && j == p.size()) // 如果s和p同时用完，匹配成功 return memo[key] = true; else if((p.size()-j) >= 2 && p[j+1] == '*'){ // * 匹配前面一个字符 0 次或者多次 if(backtracking(s, i, p, j+2, memo) || match && backtracking(s, i+1, p, j, memo)) return memo[key] = true; } else { // 单个匹配 if(match && backtracking(s, i+1, p, j+1, memo)) return memo[key] = true; } return memo[key] = false; // 没辙了，匹配失败 } }; 方法二：动态规划法 [ ] 算法思路 [ ] 代码 三，总结 回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解。回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。 尽管回溯算法的原理非常简单，但是却可以解决很多问题，比如我们开头提到的深度优先搜索、八皇后、0-1 背包问题、图的着色、旅行商问题、数独、全排列、正则表达式匹配等等。 回溯算法能解决的问题，基本用动态规划也能解决，其时间复杂度更低，空间复杂度更高，用空间换时间。 参考资料 leetcode 8皇后问题题解 回溯算法：从电影《蝴蝶效应》中学习回溯算法的核心思想 腐烂的橘子题解-回溯和动态规划 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/dp/初识动态规划.html":{"url":"3-data_structure-algorithm/dp/初识动态规划.html","title":"初始动态规划","keywords":"","body":"一，动态规划概念 动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。 它和递归一样都非常难学，主要学习难点在于求解问题的过程不太符合人类常规的思维方式。 二，0-1 背包问题 对于一组不同重量、不可分割的物品，我们需要选择一些装入背包，在满足背包最大重量限制的前提下，背包中物品总重量的最大值是多少呢？ 关于这个 0-1 背包问题，上一节学习了回溯的解决方法，也就是穷举搜索所有可能的装法（时间复杂度指数级），然后找出满足条件的最大值。有没有什么规律，可以有效降低时间复杂度呢？ 1，回溯法的求解过程： 直接看代码，规律是不好的，画个求解过程图（递归树）会好看些。假设背包的最大承载重量是 9，有 5 个不同的物品，每个物品的重量分别是 2，2，4，6，3。求解过程的递归树如下图所示。 递归树中的每个节点表示一种状态，我们用（i, cw）来表示。其中，i 表示将要决策第几个物品是否装入背包，cw 表示当前背包中物品的总重量。比如，（2，2） 表示我们将要决策第 2 个物品是否装入背包，在决策前，背包中物品的总重量是 2。这里使用回溯算法，从递归树中可以发现其中有些子问题的求解是重复的，且时间复杂度是指数级的。 使用”备忘录”（记忆化递归）的解决方式，记录已经计算好的 f(i, cw)，当再次计算到重复的 f(i, cw) 的时候，可以直接从备忘录中取出来用，就不用再递归计算了，这样就可以避免冗余计算。 int maxW = 0; int weight[6] = {2,2,4,6,3}; // 物品重量 int n = 5; // 物品个数 int w = 9; // 背包承受的最大重量 bool mem[5][10]; // 备忘录，默认值false // 记忆化递归算法实现 class SolutionBacktracking{ public: void f(int i, int cw){ // i 表示放第 i 个物品，cw 表示当前装进背包的物品的重量和 if (cw == w || i == n) { // cw==w表示装满了，i==n表示物品都考察完了 if(cw > maxW) maxW = cw; return; } if(mem[i][cw]) return; // 重复状态 mem[i][cw] = true; // 记录状态 f(i+1, cw); // 不放第 i 个物品 if(cw+weight[i] 这里依然是基于回溯算法实现的，但是采用了记忆化递归的方法，时间复杂度和空间复杂度都是 $O(n*(w+1))$，$n$ 为物品个数，$w$ 表示背包承受的最大重量。 2，动态规划求解过程如下： 把整个求解过程分为 n 个阶段，每个阶段会决策一个物品是否放到背包中。每个物品决策（放入或者不放入背包）完之后，背包中的物品的重量会有多种情况，也就是说，会达到多种不同的状态，对应到递归树中，就是有很多不同的节点。 我们把每一层重复的状态（节点）合并，只记录不同的状态，然后基于上一层的状态集合，来推导下一层的状态集合。我们可以通过合并每一层重复的状态，这样就保证每一层不同状态的个数都不会超过 w 个（w 表示背包的承载重量），也就是例子中的 9。于是，我们就成功避免了每层状态个数的指数级增长。动态规划方法的计算过程如下图： 我们用一个二维数组 states[n][w+1]，来记录每层可以达到的不同状态。0-1 背包问题的动态规划解法的 C++ 代码如下： class SolutionDP1{ public: // weight:物品重量，n:物品个数，w:背包可承载重量 int knapsack1(int weight[], int n, int w){ vector >states(n, vector(w+1, false)); // 初始化 states 第一个阶段的状态 states[0][0] = true; // 第一个物品不放进背包 if(weight[0] 0; i--){ if(states[n-1][i]) return i; } return 0; } }; 这就是一种用动态规划解决问题的思路。我们把问题分解为多个阶段，每个阶段对应一个决策。我们记录每一个阶段可达的状态集合（去掉重复的），然后通过当前阶段的状态集合，来推导下一个阶段的状态集合，动态地往前推进。这也是动态规划这个名字的由来，你可以自己体会一下 首先，可以分解为多阶段，其次，状态去重，最后当前阶段可以利用上一个阶段来获取。这是动态规划的关键。 我们知道回溯算法解决这个问题的时间复杂度是 $O(2^n)$、指数级，那动态规划解决方案的时间复杂度是多少呢？来分析一下，这个代码的时间复杂度非常好分析，耗时最多的部分就是代码中的两层 for 循环，所以时间复杂度是 $O(n*w)$。$n$ 表示物品个数，$w$ 表示背包可以承载的总重量。 虽然动态规划的时间效率较高，但是空间复杂度为 $O(n*w)$，对空间消耗比较大。我们可以考虑用一个大小为 $w+1$ 的一维数组代替二维数组，减少内存消耗。代码如下： class SolutionDP2{ public: // weight:物品重量，n:物品个数，w:背包可承载重量 int knapsack2(int weight[], int n, int w){ vector states(w+1, false); // int *states=new int [m+1]; // 动态分配,数组长度为 m states[0] = true; // 第一个物品不放进背包 if(weight[0] =0; j--) { // 第 i 个物品放进背包 if(states[j]) states[j+weight[i]] = true; } } // 在最后一层变量找到最接近 w 的重量并输出结果 for(int i=w;i>0;i--){ if(states[i]) return i; } return 0; } }; 程序分析：遍历每个物品，将该物品放入背包时，在不超过最大重量的前提下，再遍历查看之前的放入记录，将之前可能出现的重量的和当前物品的重量相加再记录下来，等所有方案都尝试过后，可能出现的背包重量也都被记录下来了，最后，从中选择一个最大值返回。 三，0-1 背包问题升级版 前面讲的背包问题，只涉及背包重量和物品重量。现在引入物品价值这一变量。对于一组不同重量、不同价值、不可分割的物品，我们选择将某些物品装入背包，在满足背包最大重量限制的前提下，背包中可装入物品的总价值最大是多少呢？ 1，这个问题依旧可以先用回溯算法来解决，代码如下： // 0-1 背包问题升级版的回溯解法 int maxV = 0; // 结果放到maxV中 int weight[] = {2，2，4，6，3}; // 物品的重量 int value[] = {3，4，8，9，6}; // 物品的价值 int n = 5; // 物品个数 int w = 9; // 背包承受的最大重量 class Solution{ public: void f(int i, int cw, int cv) { // 调用f(0, 0, 0) if (cw == w || i == n) { // cw==w表示装满了，i==n表示物品都考察完了 if(cv > maxV) maxV = cv; return; } if(cv > maxV) maxV = cv; f(i+1, cw, cv); // 不放第 i 个物品 if(cw+weight[i] 2，使用动态规划解决这个问题更高效。把整个求解过程分为 $n$ 个阶段，每个阶段会决策一个物品是否放到背包中。每个阶段决策完之后，背包中的物品的总重量以及总价值，会有多种情况，也就是会达到多种不同的状态。我们用一个二维数组 states[n][w+1]，来记录每层可以达到的不同状态。 class SolutionDP3{ int knapsack3(int weight[], int value[], int n, int w) { vector > states(n, vector(w+1, -1)); states[0][0] = 0; // 不放入第 0 个物品 if(weight[0] states[i][j+weight[i]]) states[i][j+weight[i]] = v; } } int maxV = -1; for(int j = w; j>=0; j--){ if(states[n-1][j] > maxV) maxV = states[n-1][j]; } return maxV; } } 代码的时间复杂度是 $O(n\\cdot w)$，空间复杂度也是 $O(n\\cdot w)$。 四，总结 大部分动态规划能解决的问题，都可以通过回溯算法来解决，只不过回溯算法解决起来效率比较低，时间复杂度是指数级的。动态规划算法，在执行效率方面，要高很多。尽管执行效率提高了，但是动态规划的空间复杂度也提高了，所以，很多时候，我们会说，动态规划是一种空间换时间的算法思想。 五，练习题 5.1，leetcode322 零钱兑换 给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。 你可以认为每种硬币的数量是无限的。 动态规划解法的 C++ 代码如下： class Solution { public: int coinChange(vector& coins, int amount) { int Max = amount + 1; vector dp(amount + 1, Max); dp[0] = 0; for (int i = 1; i amount ? -1 : dp[amount]; } }; 参考资料 初识动态规划：如何巧妙解决“双十一”购物时的凑单问题？ Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/dp/动态规划理论.html":{"url":"3-data_structure-algorithm/dp/动态规划理论.html","title":"动态规划理论","keywords":"","body":" 学习目标：彻底搞懂最优子结构、无后效性和重复子问题。什么样的问题可以用动态规划解决？解决动态规划问题的一般思考过程是什么样的？贪心、分治、回溯、动态规划这四种算法思想又有什么区别和联系？ 一，“一个模型三个特征”理论讲解 一个模型指的是适合用动态规划算法解决的问题的模型，这个模型也被定义为“多阶段决策最优解模型”。具体解释如下： 一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。 1. 最优子结构 最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。把最优子结构，对应到前面定义的动态规划问题模型上，就是后面阶段的状态可以通过前面阶段的状态推导出来。 2. 无后效性 无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。 3. 重复子问题 不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。 二，“一个模型三个特征”实例剖析 结合一个具体的动态规划问题更能详细理解上述理论，示例问题描述如下： 假设我们有一个 n 乘以 n 的矩阵 w[n][n]。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角。我们将棋子从左上角移动到右下角。每次只能向右或者向下移动一位。从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度。那从左上角移动到右下角的最短路径长度是多少呢？ min_dist(i, j) 可以通过 min_dist(i, j-1) 和 min_dist(i-1, j) 两个状态推导出来，所以这个问题符合“最优子结构”。 min_dist(i, j) = min(min_dist(i-1,j), min_dist(i, j-1)) 二，两种动态规划解题思路总结 知道了如何鉴别一个问题是否可以用动态规划来解决，接下来就是总结动态规划解决问题的一般思路。解决动态规划问题，一般有两种思路。分别叫作：状态转移表法和状态转移方程法。 1. 状态转移表法 一般能用动态规划解决的问题，都可以使用回溯算法的暴力搜索解决。所以，当我们拿到问题的时候，我们可以先用简单的回溯算法解决，然后定义状态，每个状态表示一个节点，然后对应画出递归树。从递归树中，我们很容易可以看出来，是否存在重复子问题，以及重复子问题是如何产生的。以此来寻找规律，看是否能用动态规划解决。 找到重复子问题之后，接下来，我们有两种处理思路，第一种是直接用回溯加“备忘录”的方法，来避免重复子问题。从执行效率上来讲，这跟动态规划的解决思路没有差别。第二种是使用动态规划的解决方法，状态转移表法。 我们先画出一个状态表。状态表一般都是二维的，所以你可以把它想象成二维数组。其中，每个状态包含三个变量，行、列、数组值。我们根据决策的先后过程，从前往后，根据递推关系，分阶段填充状态表中的每个状态。最后，我们将这个递推填表的过程，翻译成代码，就是动态规划代码了。 适合状态是二维的情况，再多维的话就不适合了，毕竟人脑不适合处理高维度的问题。 起点到终点，有很多种不同的走法，回溯算法比较适合无重复又不遗漏地穷举出所有走法，从而对比找出一个最短走法。 1，回溯解法的 C++ 代码如下： // leetcode64. 最小路径和. 回溯法-会超出时间限制 class Solution { private: int minDist = 10000; void minDistBT(vector>& grid, int i, int j, int dist, int m, int n) { if (i == 0 && j == 0) dist = grid[0][0]; if (i == m-1 && j == n-1) { if (dist >& grid) { int m = grid.size(); int n = grid[0].size(); int dist = 0; minDistBT(grid, 0, 0, dist, m, n); return minDist; } }; 有了回溯代码之后，接下来，自然要画出递归树，以此来寻找重复子问题。在递归树中，一个状态（也就是一个节点）包含三个变量 (i, j, dist)，其中 i，j 分别表示行和列，dist 表示从起点到达 (i, j) 的路径长度。从图中，可以看出，尽管 (i, j, dist) 不存在重复，但是 (i, j) 重复的有很多。对于 (i, j) 重复的节点，我们只需要选择 dist 最小的节点，继续递归求解，其他节点就可以舍弃了。 2，动态规划解法的 C++ 代码如下： // 对应 leetcode64. 最小路径和 class Solution { // 动态规划：状态转移表法 public: int minPathSum(vector>& grid) { int m = grid.size(); int n = grid[0].size(); vector > states(m, vector(n, 0)); // 第一个阶段初始化 int sum = 0; for(int i=0; i 2. 状态转移方程法 根据最优子结构，写出递归公式，也就是所谓的状态转移方程。状态转移方程，或者说递归公式是解决动态规划的关键。递归加“备忘录”的方式，将状态转移方程翻译成来 C++ 代码。 // 状态转移方程 min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j)) // 对应 leetcode64. 最小路径和 class Solution { // 状态转移方程法 private: int minDist(int i, int j, vector >& matrix, vector >& mem) { // 调用minDist(n-1, n-1); if (i == 0 && j == 0) return matrix[0][0]; if (mem[i][j] > 0) return mem[i][j]; int minUp = 10000; if (i - 1 >= 0) minUp = minDist(i - 1, j, matrix, mem); int minLeft = 10000; if (j - 1 >= 0) minLeft = minDist(i, j - 1, matrix, mem); int currMinDist = matrix[i][j] + std::min(minUp, minLeft); mem[i][j] = currMinDist; return currMinDist; } public: int minPathSum(vector>& grid) { int m = grid.size(); int n = grid[0].size(); vector > mem(m, vector(n, -1)); return minDist(m - 1, n - 1, grid, mem); } }; 三，四种算法比较分析 如果将这四种算法思想分一下类，那贪心、回溯、动态规划可以归为一类，而分治单独可以作为一类，因为它跟其他三个都不大一样。为什么这么说呢？因为前三个算法解决问题的模型，都可以抽象成多阶段决策最优解模型，而分治算法解决的问题尽管大部分也是最优解问题，但是，大部分都不能抽象成多阶段决策模型。 尽管动态规划比回溯算法高效，但是，并不是所有问题，都可以用动态规划来解决。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，动态规划和分治算法的区分非常明显。分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题。 贪心算法实际上是动态规划算法的一种特殊情况。它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性（这里我们不怎么强调重复子问题）。其中，最优子结构、无后效性跟动态规划中的无异。“贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。每一个阶段，我们都选择当前看起来最优的决策，所有阶段的决策完成之后，最终由这些局部最优解构成全局最优解。 四，内容总结 什么样的问题适合用动态规划解决？这些问题可以总结概括为“一个模型三个特征”。其中，“一个模型”指的是，问题可以抽象成分阶段决策最优解模型。“三个特征”指的是最优子结构、无后效性和重复子问题。 哪两种动态规划的解题思路？它们分别是状态转移表法和状态转移方程法。其中，状态转移表法解题思路大致可以概括为，回溯算法实现 - 定义状态 - 画递归树 - 找重复子问题 - 画状态转移表 - 根据递推关系填表 - 将填表过程翻译成代码。状态转移方程法的大致思路可以概括为，找最优子结构 - 写状态转移方程 - 将状态转移方程翻译成代码。 练习题 假设我们有几种不同币值的硬币 v1，v2，……，vn（单位是元）。如果我们要支付 w 元，求最少需要多少个硬币。比如，我们有 3 种不同的硬币，1 元、3 元、5 元，我们要支付 9 元，最少需要 3 个硬币（3 个 3 元的硬币）。 参考资料 动态规划理论：一篇文章带你彻底搞懂最优子结构、无后效性和重复子问题 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/dp/动态规划实战.html":{"url":"3-data_structure-algorithm/dp/动态规划实战.html","title":"动态规划实战","keywords":"","body":" 动态规划实战：如何实现搜索引擎中的拼写纠错功能？ 一，如何量化两个字符串的相似度？ 前面学习 Trie 树，我们知道利用 Trie 树，可以实现搜索引擎的关键词提示功能，这样可以节省用户输入搜索关键词的时间。实际上，搜索引擎在用户体验方面的优化还有很多，比如你可能经常会用的拼写纠错功能。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/剑指offer题解c++版.html":{"url":"3-data_structure-algorithm/剑指offer题解c++版.html","title":"剑指offer题解c++版","keywords":"","body":" 前言 一，常见数据结构 1，数组 3-找出数组中重复的数字 4-二维数组中的查找 5-替换空格 29-顺时针打印矩阵 leetcode 989-数组形式的整数加法 leetcode26-删除有序数组中的重复项 leetcode35-搜索插入位置 2，链表 6-从尾到头打印单链表 18.1-删除链表的节点 18.2 删除链表中重复的节点 22-链表中倒数第k个节点 23-链表中环的入口结点 24-反转一个单链表 52-两个链表的第一个公共节点 leetcode 61-旋转链表 leetcode 24-两两交换链表中的节点 leetcode876-链表的中间节点 3，栈队列堆 9-用两个栈实现队列 30-包含 min 函数的栈 31-栈的压入、弹出序列 40-最小的 K 个数 41.1-数据流中的中位数 41.2-字符流中第一个不重复的字符 59-滑动窗口的最大值 59.2-队列的最大值 leetcode 768-最多能完成排序的块 II leetcode 215. 数组中的第K个最大元素 4，字符串 leetcode 58-最后一个单词的长度 leetcode 557-反转字符串中的单词 III leetcode 1805-字符串中不同整数的数目 leetcode 1816-截断句子 leetcode 394-字符串解码 leetcode 821-字符的最短距离 5，哈希表 50-第一个只出现一次的字符位置 leetcode 146-LRU 缓存机制 leetcode 30-串联所有单词的子串 leetcode 6，二叉树 07-重建二叉树 leetcode 104-二叉树的最大深度 55.2-平衡二叉树 leetcode 109-有序链表转换二叉搜索树 7，图 二，算法 1，递归 10-1. 斐波那契数列 2，二分查找 3，排序 4，贪心 63-股票的最大利润 5，分治 6，回溯 7，动态规划 10.2-青蛙跳台阶问题 42-连续子数组的最大和 47-礼物的最大价值 48-最长不含重复字符的子字符串 66-构建乘积数组 前言 本文是作者对《剑指 offer》书籍试题的补充，并给出了较为详细算法题解和 C++ 代码实践。离线 pdf 版本最近更新时间为 2021-10-9，最新版本请参考Github 在线文档。 一，常见数据结构 1，数组 3-找出数组中重复的数字 剑指 Offer 03. 数组中重复的数字 在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。 解题方法： 直接排序，然后遍历，思路很简单但是执行起来比较麻烦 哈希表，就是找另一个数组，把 nums 的元素一个一个放进去，放进去之前判断里面有没有，如果里面已经有了那就遇到重复元素，结束。 原地置换。思路是重头扫描数组，遇到下标为 i 的数字如果不是 i 的话（假设为m), 那么我们就拿与下标 m 的数字交换。在交换过程中，如果有重复的数字发生，那么终止返回 ture。 示例 1： 输入： [2, 3, 1, 0, 2, 5, 3] 输出：2 或 3 限制： $2 C++代码： class Solution { private: void swap(int &a, int &b) { int temp = a; a = b; b = temp; } public: /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * @param numbers int整型vector * @return int整型 */ // 哈希表法 int duplicate(vector& numbers) { // write code here multiset set1; for(auto i: numbers){ set1.insert(i); if (set1.count(i) > 1) return i; } return -1; } // 原地置换法 int findRepeatNumber(vector& numbers) { int n = numbers.size(); for(int i=0; i 4-二维数组中的查找 剑指offer 04. 二维数组中的查找 在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个高效的函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 示例:现有矩阵 matrix 如下： [ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30] ] 给定 target = 5，返回 true。 给定 target = 20，返回 false。 c++ 代码如下： class Solution { public: bool findNumberIn2DArray(vector>& matrix, int target) { if(matrix.size() == 0) return false; int rows = matrix.size(); int cols = (*matrix.begin()).size(); int r = 0, c = cols -1; // 从右上角开始 while(r>0){ if(target == matrix[r][c]) return true; else if(target > matrix[r][c]) r++; else c--; } return false; } }; 5-替换空格 剑指 offer 05. 替换空格 请实现一个函数，把字符串 s 中的每个空格替换成\"%20\"。 示例 1： 输入：s = \"We are happy.\" 输出：\"We%20are%20happy.\" 限制：0 解题方法： 题解：双指针法： p2 指针指向扩容之后的 string 最后一位，p1 指向原指针最后一位，遍历指针，如果 p1 遇到空格，就将 p2 向前移动三次并赋值为'%20'，没有，则将 p1 字符赋值给 p2 字符。 C++代码： class Solution { public: string replaceSpace(string s) { int count = 0, len = s.size(); for(char& c:s){ if(c == ' ') count++; } s.resize(len + 2*count); cout 29-顺时针打印矩阵 剑指 Offer 29. 顺时针打印矩阵 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字。 示例 1： 输入：matrix = [[1,2,3],[4,5,6],[7,8,9]] 输出：[1,2,3,6,9,8,7,4,5] 示例 2： 输入：matrix = [[1,2,3,4],[5,6,7,8],[9,10,11,12]] 输出：[1,2,3,4,8,12,11,10,9,5,6,7] 限制： 0 0 解题方法： 从左到右，从上到下，检查一次是否遍历完，从右到左，从下到上 C++代码： class Solution { public: vector printMatrix(vector > matrix) { // left to right, top to bottom if(matrix.empty()) return {}; vector res; int l = 0, r = matrix[0].size()-1, t = 0, b = matrix.size()-1; int nums = (r+1) * (b+1); while(res.size() != nums){ for(int i=l; i=l; m--) // 从右往左遍历：行索引不变，列索引减少 res.push_back(matrix[b][m]); b--; for(int n=b; n>=t; n--) // 从下往上遍历：列索引不变，行索引减少 res.push_back(matrix[n][l]); l++; } return res; } }; leetcode 989-数组形式的整数加法 leetcode 989-数组形式的整数加法 对于非负整数 X 而言，X 的数组形式是每位数字按从左到右的顺序形成的数组。例如，如果 X = 1231，那么其数组形式为 [1,2,3,1]。 给定非负整数 X 的数组形式 A，返回整数 X+K 的数组形式。 示例 1： 输入：A = [1,2,0,0], K = 34 输出：[1,2,3,4] 解释：1200 + 34 = 1234 示例 2： 输入：A = [2,7,4], K = 181 输出：[4,5,5] 解释：274 + 181 = 455 限制： 1 0 0 如果 A.length > 1，那么 A[0] != 0 解题方法： 两数相加形式的题目，可用以下加法公式模板。 当前位 = (A 的当前位 + B 的当前位 + 进位carry) % 10 while ( A 没完 || B 没完) A 的当前位 B 的当前位 和 = A 的当前位 + B 的当前位 + 进位carry 当前位 = 和 % 10; 进位 = 和 / 10; 判断是否还有进位 复杂度分析： 时间复杂度: $O(n)$ 空间复杂度: $O(n)$ C++代码： class Solution { public: // 逐位相加法，使用加法模板 vector addToArrayForm(vector& num, int k) { int sum = 0; int carry = 0; int n = num.size()-1; vector res; while(n >=0 || k != 0){ int remainder = k % 10; // k 的当前位 if(n>=0) sum = num[n] + remainder + carry; else sum = remainder + carry; carry = sum / 10; // 进位计算 sum %= 10; // 当前位计算 res.push_back(sum); k /= 10; n -= 1; } if(carry != 0) res.push_back(carry); // 判断是否还有进位 reverse(res.begin(), res.end()); // 反转数组 return res; } }; leetcode26-删除有序数组中的重复项 leetcode26-删除有序数组中的重复项 给你一个有序数组 nums ，请你 原地 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。 不要使用额外的数组空间，你必须在 原地 修改输入数组 并在使用 O(1) 额外空间的条件下完成。 解题思路： 双指针： 一个读指针、一个写指针遍历数组； 遇到重复的元素，读指针继续前进，写指针不做操作； 遇到不同的元素，写指针前进一步，并写入那个元素。 class Solution { public: // 双指针法 int removeDuplicates(vector& nums) { if (nums.empty()) return 0; int r=0, w = 0; // int n = nums.size(); // 数组长度 while(r & nums) { if (nums.empty()) return 0; int duplicatedNum = nums[0]; int j=0; for(int i=1;i leetcode35-搜索插入位置 leetcode35-搜索插入位置 给定一个排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。 请必须使用时间复杂度为 O(log n) 的算法。 解题方法： 二分查找法（非递归实现），查找结束如果没有相等值则返回 left，该值为插入位置 class Solution { public: // 二分法+非递归实现 int searchInsert(vector& nums, int target) { int low = 0, high = nums.size()-1; while( low >1) // int mid = (low+high)/2; if( nums[mid] == target ) return mid; else if (target 2，链表 6-从尾到头打印单链表 剑指 offer 6: 从尾到头打印单链表 输入一个链表的头节点，从尾到头反过来返回每个节点的值（用数组返回）。 示例 1： 输入：head = [1,3,2] 输出：[2,3,1] 限制：0 解题方法： 使用栈的思想(Python 用 list 模拟栈, pop 弹出栈头元素)，栈具有后进先出的特点，在遍历链表时将值按顺序放入栈中，最后出栈的顺序即为逆序。注意和 C 语言不同，C++ 的结构体可以有构造函数！ C++代码： class Solution{ public: vector reversePrint(ListNode* head) { stack values; // 创建一个不包含任何元素的 stack 适配器，并采用默认的 deque 基础容器： vector result; while (head != nullptr){ values.push(head->val); head = head->next; } while(!values.empty()){ result.push_back(values.top()); values.pop(); } return result; } }; 18.1-删除链表的节点 剑指 Offer 18.1 删除链表的节点 给定单向链表的头指针和一个要删除的节点的值，定义一个函数删除该节点。返回删除后的链表的头节点。（注意：此题对比原题有改动） 示例 1: 输入: head = [4,5,1,9], val = 5 输出: [4,1,9] 解释: 给定你链表中值为 5 的第二个节点，那么在调用了你的函数之后，该链表应变为 4 -> 1 -> 9. 示例 2: 输入: head = [4,5,1,9], val = 1 输出: [4,5,9] 解释: 给定你链表中值为 1 的第三个节点，那么在调用了你的函数之后，该链表应变为 4 -> 5 -> 9. 说明： 题目保证链表中节点的值互不相同 若使用 C 或 C++ 语言，你不需要 free 或 delete 被删除的节点 解题思路： 定位节点： 遍历链表，直到 head.val == val 时跳出，即可定位目标节点。 修改引用： 设节点 cur 的前驱节点为 pre ，后继节点为 cur.next ；则执行 pre.next = cur.next ，即可实现删除 cur 节点。 C++代码： class Solution { public: ListNode* deleteNode(ListNode* head, int val) { if(head->val == val) return head -> next; ListNode* pre = head; ListNode* cur = head->next; while(cur != nullptr && cur->val != val) { pre = cur; cur = cur->next; } pre->next = cur->next; return head; } }; 18.2 删除链表中重复的节点 剑指 Offer 18.2 删除链表中重复的节点 题目描述：在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1->2->3->3->4->4->5 处理后为 1->2->5 解题方法： 迭代解法 C++代码： class Solution { public: ListNode* deleteDuplication(ListNode* head) { ListNode *vhead = new ListNode(-1); vhead->next = head; ListNode* pre = vhead,*cur = head; while(cur){ if(cur->next && cur->val==cur->next->val){ cur = cur->next; while(cur->next && cur->val == cur->next->val){ cur = cur->next; } cur = cur -> next; pre->next = cur; } else{ pre = cur; cur = cur->next; } } return vhead->next; } }; 22-链表中倒数第k个节点 剑指 Offer 22. 链表中倒数第k个节点 输入一个链表，输出该链表中倒数第k个节点。为了符合大多数人的习惯，本题从1开始计数，即链表的尾节点是倒数第1个节点。 例如，一个链表有 6 个节点，从头节点开始，它们的值依次是 1、2、3、4、5、6。这个链表的倒数第 3 个节点是值为 4 的节点。 示例： 给定一个链表: 1->2->3->4->5, 和 k = 2. 返回链表 4->5. 解题方法： 双指针法。不用统计链表长度。前指针 former 先向前走 k 步。 C++代码： // Definition for singly-linked list. struct ListNode { int val; ListNode *next; ListNode(int x) : val(x), next(NULL) {} }; class Solution { public: ListNode* getKthFromEnd(ListNode* head, int k) { ListNode* former = head; ListNode* latter = head; for(int i=0;inext; } while(former != NULL){ former = former->next; latter = latter->next; } return latter; } }; 23-链表中环的入口结点 链表中环的入口结点 给一个长度为 n 的链表，若其中包含环，请找出该链表的环的入口结点，否则，返回 null。 输入描述：输入分为2段，第一段是入环前的链表部分，第二段是链表环的部分，后台将这2个会组装成一个有环或者无环单链表 返回值描述：返回链表的环的入口结点即可。而我们后台程序会打印这个节点 示例1： 输入：{1,2},{3,4,5} 返回值：3 说明：返回环形链表入口节点，我们后台会打印该环形链表入口节点，即3 解题思路： 采用双指针解法，一快一慢指针。快指针每次跑两个element，慢指针每次跑一个。如果存在一个圈，总有一天，快指针是能追上慢指针的。 C++代码： class Solution { public: ListNode* EntryNodeOfLoop(ListNode* pHead) { ListNode* fast = pHead; ListNode* slow = pHead; while( fast && fast->next) { // 找到 fast 指针和 slow 指针相遇位置 fast = fast->next->next; slow = slow->next; if(fast == slow ) break; } if (!fast || !fast->next) return nullptr; fast = pHead; // fast 指针指向头节点，slow 指针原地不变 while(fast != slow ) { // 两个指针重新相遇于环的入口点 fast = fast->next; slow = slow->next; } return fast; } }; 24-反转一个单链表 剑指 offer 24: 反转一个单链表 给你单链表的头节点 head ，请你反转链表，并返回反转后的链表。 示例1： 输入：head = [1,2,3,4,5] 输出：[5,4,3,2,1] 解题思路： 双指针迭代法。 C++代码： class Solution { public: // 双指针迭代法 ListNode* reverseList(ListNode* head) { // 判断链表为空或长度为1的情况 if(head == nullptr || head->next == nullptr){ return head; } ListNode* pre = nullptr; // 当前节点的前一个节点 ListNode* next = nullptr; // 当前节点的下一个节点 while( head != nullptr){ next = head->next; // 记录当前节点的下一个节点位置； head->next = pre; // 让当前节点指向前一个节点位置，完成反转 pre = head; // pre 往右走 head = next;// 当前节点往右继续走 } return pre; } }; 52-两个链表的第一个公共节点 剑指 offer 52: 两个链表的第一个公共节点 输入两个链表，找出它们的第一个公共节点。 解题方法： 1，双指针法：设节点指针 A 指向头节点 headA, 节点指针 B 指向头节点 headB。 A 先遍历完链表 headA，然后遍历 headB; B 先遍历完链表 headB，然后遍历 headA; 只要有公共节点，总路程数，或者说 A 经过的节点数和 B 经过的节点数是一样的， 如果没有公共节点，只有当 A 和 B都变成了 nullptr的时候，两者最终走的路程才是一样的。 然后只需比较 A和 B是否相等，相等的那个位置即为公共节点，因为此使，两者走的步数开始相等了。 2，栈特性解法。两个链表从公共结点开始后面都是一样的，顺着链表从后向前查找，很容易就能查找到链表的公共结点（第一个不相同的结点的下一个结点即所求）；而从后向前的特性自然联想到栈。 3，哈希表法。 复杂度分析： 时间复杂度：O(n) 空间复杂度：O(1) C++代码： class Solution { public: // 双指针法 ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) { ListNode* A = headA; ListNode* B = headB; while(A != B){ if(A != nullptr) A = A->next; else A = headB; if (B != nullptr) B = B->next; else B = headA; } return A; } // 哈希表法，哈希表中存储链表节点指针 ListNode *getIntersectionNode2(ListNode *headA, ListNode *headB) { unordered_set visited; ListNode* temp = headA; while(temp != nullptr){ visited.insert(temp); temp = temp -> next; } temp = headB; while(temp != nullptr){ // count 方法判断哈希表中是否存在 temp 关键字 if(visited.count(temp)) return temp; else temp = temp -> next; } return nullptr; } // vector 法，vector 中元素为链表节点指针 ListNode *getIntersectionNode3(ListNode *headA, ListNode *headB) { vector visited; ListNode* temp = headA; while(temp != nullptr){ visited.push_back(temp); temp = temp -> next; } temp = headB; while(temp != nullptr){ // find 函数查找 vector 中是否存在 temp 元素 if(visited.end() != find(visited.begin(), visited.end(), temp)) return temp; else temp = temp -> next; } return nullptr; } // 栈特性解法 ListNode *getIntersectionNode4(ListNode *headA, ListNode *headB) { ListNode *l1 = headA; ListNode *l2 = headB; stack st1, st2; while(headA != nullptr){ st1.push(headA); headA = headA->next; } while(headB != nullptr){ st2.push(headB); headB = headB->next; } ListNode* ans = nullptr; while(!st1.empty()&&!st2.empty()&&st1.top()==st2.top()){ ans = st1.top(); st1.pop(); st2.pop(); } return ans; } }; leetcode 61-旋转链表 leetcode 61-旋转链表 给你一个链表的头节点 head ，旋转链表，将链表每个节点向右移动 k 个位置。 示例1 输入：head = [1,2,3,4,5], k = 2 输出：[4,5,1,2,3] 解题思路： 将原来的链表首尾相连变成环，然后找倒数第 k 个点作为新的表头，即原来的表头向右移动 (n-1)-(k%n) 次后断开。 C++代码： class Solution { public: ListNode* rotateRight(ListNode* head, int k) { if (k == 0 || head == nullptr || head->next == nullptr) { return head; } ListNode* cur = head; int n = 1; while(cur -> next != nullptr){ cur = cur -> next; n += 1; } cur -> next = head; // 将链表首尾相连变成环 cur = head; int move = (n-1)-(k % n); while(move--){ cur = cur -> next; } ListNode* ret = cur -> next; cur -> next = nullptr; // cur 向右移动 move 次后，断掉连接 return ret; } }; leetcode 24-两两交换链表中的节点 leetcode 24. 两两交换链表中的节点 给定一个链表，两两交换其中相邻的节点，并返回交换后的链表。你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。 解题思路： 1，迭代法：关键是高清如何交换两个相邻节点，然后迭代交换即可。 复杂度分析： 时间复杂度：O(n) 空间复杂度：O(1) C++代码： class Solution { public: ListNode* swapPairs(ListNode* head) { if(head == nullptr) return nullptr; else if(head->next == nullptr) return head; ListNode* temp = new ListNode(-1); temp ->next = head; ListNode* pre = temp; while(pre->next != nullptr && pre->next->next != nullptr) { ListNode* cur = pre->next; ListNode* next = pre->next->next; pre->next = cur->next; cur->next = next->next; next->next = cur; pre = cur; } return temp->next; } }; leetcode876-链表的中间节点 leetcode 876. 链表的中间结点 给定一个头结点为 head 的非空单链表，返回链表的中间结点。如果有两个中间结点，则返回第二个中间结点。 解题方法: 数组法。 快慢指针法：用两个指针 slow 与 fast 一起遍历链表。slow 一次走一步，fast 一次走两步。那么当 fast 到达链表的末尾时，slow 必然位于中间。值得注意的是，快指针可以前进的前提是当前快指针和当前快指针的下一个节点非空。 class Solution { public: ListNode* middleNode(ListNode* head) { ListNode* slow = head; ListNode* fast = head; while(fast && fast->next){ slow = slow->next; fast = fast->next->next; } return slow; } }; 3，栈队列堆 9-用两个栈实现队列 剑指 offer 面试题9 用两个栈实现队列, 用两个栈来实现一个队列，完成队列的 Push 和 Pop 操作。 解题思路： in 栈用来处理入栈（push）操作，out 栈用来处理出栈（pop）操作。一个元素进入 in 栈之后，出栈的顺序被反转。 当元素要出栈时，需要先进入 out 栈，此时元素出栈顺序再一次被反转，因此出栈顺序就和最开始入栈顺序是相同的，先进入的元素先退出，这就是队列的顺序。 C++代码： class Solution { public: void push(int node) { stack1.push(node); } int pop() { int res; if(stack2.empty()){ while(!stack1.empty()){ int temp = stack1.top(); stack1.pop(); stack2.push(temp); } } res = stack2.top(); stack2.pop(); return res; } private: stack stack1; stack stack2; }; 30-包含 min 函数的栈 30-包含 min 函数的栈 定义栈的数据结构，请在该类型中实现一个能够得到栈的最小元素的 min 函数在该栈中，调用 min、push 及 pop 的时间复杂度都是 O(1)。 解题思路： 数据栈 A ： 栈 A 用于存储所有元素，保证入栈 push() 函数、出栈 pop() 函数、获取栈顶 top() 函数的正常逻辑。 辅助栈 B ： 栈 B 中存储栈 A 中所有 非严格降序 的元素，则栈 A 中的最小元素始终对应栈 B 的栈顶元素，即 min() 函数只需返回栈 B 的栈顶元素即可。 C++代码： class MinStack { // 利用辅助栈 private: stack stack1; stack stack2; public: /** initialize your data structure here. */ MinStack() { } void push(int x) { stack1.push(x); if (stack2.empty()) { stack2.push(x); } else { if (x 31-栈的压入、弹出序列 31-栈的压入、弹出序列 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如，序列 {1,2,3,4,5} 是某栈的压栈序列，序列 {4,5,3,2,1} 是该压栈序列对应的一个弹出序列，但 {4,3,5,1,2} 就不可能是该压栈序列的弹出序列。 示例 1： 输入：pushed = [1,2,3,4,5], popped = [4,5,3,2,1] 输出：true 解释：我们可以按以下顺序执行： push(1), push(2), push(3), push(4), pop() -> 4, push(5), pop() -> 5, pop() -> 3, pop() -> 2, pop() -> 1 示例 2： 输入：pushed = [1,2,3,4,5], popped = [4,3,5,1,2] 输出：false 解释：1 不能在 2 之前弹出。 提示： 0 0 pushed 是 popped 的排列。 解题思路： C++代码实现： // 剑指offer31: 栈的压入、弹出序列 class Solution { // 辅助栈解法，时间超过 77.62%，空间超过 74.84% public: bool validateStackSequences(vector& pushed, vector& popped) { int k = 0; stack st; for(int i=0; i 40-最小的 K 个数 40-最小的 K 个数 输入整数数组 arr ，找出其中最小的 k 个数。例如，输入 4、5、1、6、2、7、3、8 这 8 个数字，则最小的 4 个数字是 1、2、3、4。 示例 1： 输入：arr = [3,2,1], k = 2 输出：[1,2] 或者 [2,1] 示例 2： 输入：arr = [0,1,2,1], k = 1 输出：[0] 限制： 0 0 解题方法： 数组原地排序法：对原数组从小到大排序后取出前 k 个数即可。时间复杂度：O(nlog n)，空间复杂度：O(log n)。 使用最大堆结构：优先队列(最大堆，优先输出最大数)。时间复杂度：O(nlongk), 插入容量为k的大根堆时间复杂度为O(longk), 一共遍历n个元素；空间复杂度：O(k)。 快速排序算法：TODO. C++代码： // priority_queue // 默认定义最大堆 // priority_queue, greater >p; // 定义最小堆 class Solution { public: // // stl 自带的 sort() 排序算法 vector getLeastNumbers1(vector& arr, int k) { vector ret(k, 0); sort(arr.begin(), arr.end()); for(int i=0;i getLeastNumbers2(vector& arr, int k) { vector vec(k,0); if(k==0) return vec; priority_queue heap; // 大顶堆，堆顶为最大值 for(int i=0;i arr[i]){ // 使用大顶堆来维护最小堆 heap.pop(); heap.push(arr[i]); } } } for(int i=0;i getLeastNumbers(vector& arr, int k) { vector vec(k,0); if(k==0) return vec; priority_queue, greater > heap; // 小顶堆，堆顶为最小值 priority_queue heap2; // 大顶堆，堆顶为最大值 for(int i=0;i 41.1-数据流中的中位数 41.1-数据流中的中位数 如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。例如，[2,3,4] 的中位数是 3；[2,3] 的中位数是 (2 + 3) / 2 = 2.5 设计一个支持以下两种操作的数据结构： void addNum(int num) - 从数据流中添加一个整数到数据结构中。 double findMedian() - 返回目前所有元素的中位数。 解题方法： 数据流左半边的数用大顶堆，右半边的数用小顶堆，中位数由两个堆的堆顶元素求得。 C++代码： class MedianFinder { // 大根堆+小根堆 解法，时间超过 99..38%，空间超过 18.07% private: // 从左到右，数据依次从大到小 priority_queue right; // 大顶堆，堆顶为最大值 priority_queue, greater > left; // 小顶堆，堆顶为最小值 public: /** initialize your data structure here. */ MedianFinder() { } void addNum(int num) { // 插入数据要始终保持两个堆处于平衡状态，即较大数在左边，较小数在右边 // 两个堆元素个数不超过 1 if(left.size() == right.size()){ right.push(num); left.push(right.top()); // 保证左边堆插入的元素始终是右边堆的最大值 right.pop(); // 删除堆顶元素 } else{ left.push(num); right.push(left.top()); left.pop(); } } double findMedian() { if(left.size() == right.size()) return (left.top() + right.top())*0.5; else return left.top()*1.0; } }; 41.2-字符流中第一个不重复的字符 41.2-字符流中第一个不重复的字符 请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符\"go\"时，第一个只出现一次的字符是\"g\"。当从该字符流中读出前六个字符“google\"时，第一个只出现一次的字符是\"l\"。后台会用以下方式调用 Insert 和 FirstAppearingOnce 函数 string caseout = \"\"; 1.读入测试用例字符串 casein 2.如果对应语言有 Init()函数的话，执行 Init() 函数 3.循环遍历字符串里的每一个字符ch { Insert(ch); caseout += FirstAppearingOnce() } 输出 caseout，进行比较 返回值描述： 如果当前字符流没有存在出现一次的字符，返回 # 字符。 解题思路（参考牛客网题解）： 对于“重复问题”，惯性思维应该想到哈希或者set。对于“字符串问题”，大多会用到哈希。因此一结合，应该可以想到，判断一个字符是否重复，可以选择用哈希，在 c++ 中，可以选择用 unordered_map。 对于字符流，源源不断的往池子中添加字符，然后还要返回第一个满足什么条件的字符，显然设计到了“顺序”，也就是先来的先服务，这种先进先出的数据结构不就是队列嘛。因此，这里可以用队列 queue。 算法过程如下： 初始化一个哈希表 unordered_map mp 和队列 queue q。 对于 void Insert(char ch) 字符插入函数的实现，当且仅当 ch 是第一次出现，则将 ch 添加到队列中；同时，不管 ch 是不是第一次出现，都需要在 mp 中更新一下字符的出现次数，方便后续判断字符是否是第一次出现。 对于 char FirstAppearingOnce() 函数，通过哈希表 mp 判断队列 q 的头部元素的出现次数，如果是 1 则返回对应字符 ch；如果不是 1，则队列 pop() 弹出头部元素继续判断下一个字符。 class Solution { public: //Insert one char from stringstream queue q; unordered_map mp; void Insert(char ch) { // 如果是第一次出现，则添加到队列中 if (mp.find(ch) == mp.end()) { q.push(ch); } // 不管是不是第一次出现，都进行计数 ++mp[ch]; } // return the first appearence once char in current stringstream char FirstAppearingOnce() { while (!q.empty()) { char ch = q.front(); // 拿出头部，如果是第一次出现，则返回 if (mp[ch] == 1) { return ch; } // 不是第一次出现，则弹出，然后继续判断下一个头部 else { q.pop(); } } return '#'; } }; 59-滑动窗口的最大值 剑指offer 59-滑动窗口的最大值 给定一个数组 nums 和滑动窗口的大小 k，请找出所有滑动窗口里的最大值。 示例: 输入: nums = [1,3,-1,-3,5,3,6,7], 和 k = 3 输出: [3,3,5,5,6,7] 解释: 滑动窗口的位置 最大值 --------------- ----- [1 3 -1] -3 5 3 6 7 3 1 [3 -1 -3] 5 3 6 7 3 1 3 [-1 -3 5] 3 6 7 5 1 3 -1 [-3 5 3] 6 7 5 1 3 -1 -3 [5 3 6] 7 6 1 3 -1 -3 5 [3 6 7] 7 解题方法： 对于每个滑动窗口，可以使用 O(k) 的时间遍历其中的每一个元素，找出其中的最大值。对于长度为 n 的数组 nums 而言，窗口的数量为 n-k+1，算法的时间复杂度为 $O((n-k+1)\\ast k)=O(n\\ast k)$。 维护单调递减的双端队列！如果发现队尾元素小于要加入的元素，则将队尾元素出队，直到队尾元素大于新元素时，再让新元素入队，从而维护一个单调递减的队列。 C++代码： class Solution { public: // 简单方法：遍历滑动窗口找最大值，合理选择区间,时间超出限制 vector maxSlidingWindow2(vector& nums, int k) { vector ret; if (nums.size() == 0 && k == 0) return ret; for (int i = 0; i maxNum) maxNum = nums[j]; } ret.push_back(maxNum); } return ret; } // 维护一个单调队列，队头是最大值 vector maxSlidingWindow(vector& nums, int k) { vector ret; deque window; // 创建双端队列,单调递减的队列 // 先将第一个窗口的值按照规则入队 for (int i = 0; i 59.2-队列的最大值 剑指offer 59.2-队列的最大值 请定义一个队列并实现函数 max_value 得到队列里的最大值，要求函数 max_value、push_back 和 pop_front 的均摊时间复杂度都是 $O(1)$。 若队列为空，pop_front 和 max_value 需要返回 -1。 示例 1： 输入: [\"MaxQueue\",\"push_back\",\"push_back\",\"max_value\",\"pop_front\",\"max_value\"] [[],[1],[2],[],[],[]] 输出: [null,null,null,2,1,2] 解题思路： 定义一个单调递减的辅助队列（双端队列） C++代码实现： class MaxQueue { private: queue que1; deque que2; // 辅助队列，头部位置存放最大值值 public: MaxQueue() { } int max_value() { if(que1.empty()) return -1; return que2.front(); } void push_back(int value) { // 维护单调递减队列 while(!que2.empty() && que2.back() max_value(); * obj->push_back(value); * int param_3 = obj->pop_front(); */ leetcode 768-最多能完成排序的块 II leetcode 768-最多能完成排序的块 II 这个问题和“最多能完成排序的块”相似，但给定数组中的元素可以重复，输入数组最大长度为 2000，其中的元素最大为 10**8。 arr 是一个可能包含重复元素的整数数组，我们将这个数组分割成几个“块”，并将这些块分别进行排序。之后再连接起来，使得连接的结果和按升序排序后的原数组相同。我们最多能将数组分成多少块？ 示例 1: 输入: arr = [5,4,3,2,1] 输出: 1 解释: 将数组分成2块或者更多块，都无法得到所需的结果。 例如，分成 [5, 4], [3, 2, 1] 的结果是 [4, 5, 1, 2, 3]，这不是有序的数组。 解题思路： 1，辅助栈法：栈中存放每个块内元素的最大值，栈的 size() 即为最多分块数。 题中隐含结论： 下一个分块中的所有数字都会大于等于上一个分块中的所有数字，即后面块中的最小值也大于前面块中最大值。 只有分的块内部可以排序，块与块之间的相对位置是不能变的。 直观上就是找到从左到右开始不减少（增加或者不变）的地方并分块。 要后面有较小值，那么前面大于它的都应该在一个块里面。 复杂度分析： 时间复杂度: O(n) 空间复杂度: O(1) C++代码： class Solution { public: int maxChunksToSorted(vector& arr) { stack ret; // 创建单调栈 // 单调栈中只保留每个分块的最大值 for (int i = 0; i leetcode 215. 数组中的第K个最大元素 leetcode 215. 数组中的第K个最大元素 给定整数数组 nums 和整数 $k$，请返回数组中第 $k$ 个最大的元素。 请注意，你需要找的是数组排序后的第 $k$ 个最大的元素，而不是第 $k$ 个不同的元素。 解题思路：小顶堆维护大顶堆的方法 维护一个有 k 个元素的最小堆： 如果当前堆不满，直接添加； 堆满的时候，如果新读到的数大于堆顶元素，则将堆顶元素弹出，同时将新读到的数放入最小堆中（堆会自己调整内部结构）。 复杂度分析： 时间复杂度：$O(nlogk)$ 空间复杂度：$O(k)$ class Solution { public: // 小顶堆维护大顶堆的方法，时间复杂度 O(n logk) int findKthLargest(vector& nums, int k) { vector vec(k,0); // priority_queue heap; // 大顶堆，堆顶为最大值 priority_queue, greater > heap; // 小顶堆，堆顶为最小值 for(int i=0; i 4，字符串 leetcode 58-最后一个单词的长度 leetcode 58-最后一个单词的长度 给你一个字符串 s，由若干单词组成，单词前后用一些空格字符隔开。返回字符串中最后一个单词的长度。单词：是指仅由字母组成、不包含任何空格字符的最大子字符串。 C++代码： class Solution { public: int lengthOfLastWord(string s) { s += ' '; vector res; // 存放字符串的数组 string temp; // 临时字符串 for(char ch:s){ if(ch == ' '){ if(!temp.empty()){ res.push_back(temp); temp.clear(); } } else{ temp += ch; } } string last_word = res.back(); // 数组最后一个元素 return last_word.size(); } }; leetcode 557-反转字符串中的单词 III leetcode 557. 反转字符串中的单词 III 给定一个字符串，你需要反转字符串中每个单词的字符顺序，同时仍保留空格和单词的初始顺序。 示例： 输入：\"Let's take LeetCode contest\" 输出：\"s'teL ekat edoCteeL tsetnoc\" 提示：在字符串中，每个单词由单个空格分隔，并且字符串中不会有任何额外的空格。 C++代码： class Solution { public: string reverseWords(string s) { s += ' '; vector res; // 存放字符串的数组 string temp; // 临时字符串 for(char ch:s){ if(ch == ' '){ if(!temp.empty()){ res.push_back(temp); temp.clear(); } } else{ temp += ch; } } s.clear(); for(string &str: res){ reverse(str.begin(), str.end()); s += str + ' '; } s.pop_back(); return s; } }; leetcode 1805-字符串中不同整数的数目 leetcode 1805-字符串中不同整数的数目 给你一个字符串 word ，该字符串由数字和小写英文字母组成。 请你用空格替换每个不是数字的字符。例如，\"a123bc34d8ef34\" 将会变成 \" 123 34 8 34\" 。注意，剩下的这些整数为（相邻彼此至少有一个空格隔开）：\"123\"、\"34\"、\"8\" 和 \"34\" 。 返回对 word 完成替换后形成的 不同 整数的数目。只有当两个整数的 不含前导零 的十进制表示不同， 才认为这两个整数也不同。 示例 1： 输入：word = \"a123bc34d8ef34\" 输出：3 解释：不同的整数有 \"123\"、\"34\" 和 \"8\" 。注意，\"34\" 只计数一次。 C++代码： class Solution { public: int numDifferentIntegers(string word) { set s; word += 'a'; string temp; // 临时字符串 for(char ch:word){ // 如果遇到字母且临时字符串非空，就把它加入集合并重置临时字符串 if(isalpha(ch)){ if(!temp.empty()){ s.insert(temp); temp.clear(); } } else{ if(temp == \"0\") temp.clear(); // \"001\" 和 \"1\" 是等值的 temp += ch; } } return s.size(); } }; leetcode 1816-截断句子 leetcode 1816-截断句子 句子 是一个单词列表，列表中的单词之间用单个空格隔开，且不存在前导或尾随空格。每个单词仅由大小写英文字母组成（不含标点符号）。 例如，\"Hello World\"、\"HELLO\" 和 \"hello world hello world\" 都是句子。给你一个句子 s​​​​​​ 和一个整数 k​​​​​​ ，请你将 s​​ 截断 ​，​​​使截断后的句子仅含 前 k​​​​​​ 个单词。返回 截断 s​​​​​​ 后得到的句子。 示例 1： 输入：s = \"Hello how are you Contestant\", k = 4 输出：\"Hello how are you\" 解释： s 中的单词为 [\"Hello\", \"how\" \"are\", \"you\", \"Contestant\"] 前 4 个单词为 [\"Hello\", \"how\", \"are\", \"you\"] 因此，应当返回 \"Hello how are you\" C++代码： class Solution { public: string truncateSentence(string s, int k) { s += ' '; vector res; // 存放字符串的数组 string temp; // 临时字符串 for(char ch:s){ if(ch == ' '){ res.push_back(temp); temp.clear(); } else{ temp += ch; } } s.clear(); for(int i=0; i leetcode 394-字符串解码 leetcode 394. 字符串解码 给定一个经过编码的字符串，返回它解码后的字符串。编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。 你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4] 的输入。 示例 1： 输入：s = \"3[a]2[bc]\" 输出：\"aaabcbc\" 解题思路： 本题难点在于括号内嵌套括号，需要从内向外生成与拼接字符串，这与栈的先入后出特性对应。 复杂度分析： 时间复杂度 O(N)： s； 空间复杂度 O(N)：辅助栈在极端情况下需要线性空间，例如 2[2[2[a]]]。 C++代码： class Solution { public: string decodeString(string s) { stack> s1; string res = \"\"; int num = 0; for (int i = 0; i = '0' && s[i] leetcode 821-字符的最短距离 leetcode 821-字符的最短距离 给你一个字符串 s 和一个字符 c ，且 c 是 s 中出现过的字符。 返回一个整数数组 answer ，其中 answer.length == s.length 且 answer[i] 是 s 中从下标 i 到离它 最近 的字符 c 的 距离 。 两个下标 $i$ 和 $j$ 之间的 距离 为 abs(i - j) ，其中 abs 是绝对值函数。 示例 1： 输入：s = \"loveleetcode\", c = \"e\" 输出：[3,2,1,0,1,0,0,1,2,2,1,0] 解释：字符 'e' 出现在下标 3、5、6 和 11 处（下标从 0 开始计数）。 距下标 0 最近的 'e' 出现在下标 3 ，所以距离为 abs(0 - 3) = 3 。 距下标 1 最近的 'e' 出现在下标 3 ，所以距离为 abs(1 - 3) = 2 。 对于下标 4 ，出现在下标 3 和下标 5 处的 'e' 都离它最近，但距离是一样的 abs(4 - 3) == abs(4 - 5) = 1 。 距下标 8 最近的 'e' 出现在下标 6 ，所以距离为 abs(8 - 6) = 2 解题方法： 1，两次遍历 从左向右遍历，记录上一个字符 C 出现的位置 prev，那么答案就是 i - prev。 从右想做遍历，记录上一个字符 C 出现的位置 prev，那么答案就是 prev - i。 2，哈希表法 获取 s 中所有目标字符 c 的位置，并提前存储在数组 c_indexs 中。 遍历字符串 s 中的每个字符，如果和 c 不相等，就到 c_indexs 中找距离当前位置最近的下标。 复杂度分析： 时间复杂度: O(n) 空间复杂度: O(1) C++代码： class Solution { public: // 1，两次遍历法 vector shortestToChar(string s, char c) { vector ret; int prev = -10000; int distance = 0; for (int i = 0; i = 0; i--) { if (s[i] == c) prev = i; distance = prev - i; ret[i] = min(ret[i], distance); } return ret; } // 解法2：空间换时间，时间复杂度 O(n*k) vector shortestToChar2(string s, char c) { int n = s.size(); vector c_indexs; // Initialize a vector of size n with default value 0. vector ret(n, 0); for (int i = 0; i 5，哈希表 50-第一个只出现一次的字符位置 剑指offer 题50. 第一个只出现一次的字符位置 在字符串 s 中找出第一个只出现一次的字符。如果没有，返回一个单空格。 s 只包含小写字母。 示例 1: 输入：s = \"abaccdeff\" 输出：'b' 示例 2: 输入：s = \"\" 输出：' ' 限制：0 解题方法： 哈希表法。map：基于红黑树，元素有序存储; unordered_map：基于散列表，元素无序存储 C++代码： class Solution { public: char firstUniqChar(string s) { unordered_map dic; for(char c:s){ dic[c] = dic.find(c) == dic.end(); } for(char c:s){ if(dic[c] == true) return c; } return ' '; } char FirstNotRepeatingChar(string s) { unordered_map dic; for(char c:s){ dic[c] = dic.find(c) == dic.end(); } for(int i=0; i leetcode 146-LRU 缓存机制 leetcode 146-LRU 缓存机制 解题方法： LRU 缓存机制可以通过哈希表辅以双向链表实现，我们用一个哈希表和一个双向链表维护所有在缓存中的键值对。 双向链表按照被使用的顺序存储了这些键值对，靠近头部的键值对是最近使用的，而靠近尾部的键值对是最久未使用的。 哈希表即为普通的哈希映射（HashMap），通过缓存数据的键映射到其在双向链表中的位置（双向链表的节点地址）。 C++代码： //定义双链表 struct Node{ int key, val; Node* left ,*right; Node(int _key, int _value): key(_key),val(_value),left(NULL),right(NULL){} }*head,*tail; // 双链表的最左和最右节点，不存贮值。 class LRUCache { private: unordered_map cache; int n; public: LRUCache(int capacity) { n = capacity; // head、tail 双链表的头尾节点 head = new Node(-1, -1), tail = new Node(-1, -1); head -> right = tail; tail -> left = head; } int get(int key) { if(!cache.count(key)) return -1; Node* p = cache[key]; // 通过哈希表定位 key 对应的键值 p removeNode(p); addToHead(p); return p->val; } void put(int key, int value) { if(cache.count(key)){ Node* p = cache[key]; p -> val = value; // 定位双向链表的节点 p，并更新 val removeNode(p); addToHead(p); } else{ if(cache.size() == n){ auto p = tail->left; removeNode(p); cache.erase(p->key); // 更新哈希表 delete p; } auto p = new Node(key, value); addToHead(p); cache[key] = p; } } void removeNode(Node* p){ // 移除指定节点 p p->left->right = p->right; p->right->left = p->left; } void addToHead(Node* p){ // 插入到头节点 L 之后 p->right = head->right; p->left = head; head->right->left = p; head->right = p; } }; /** * Your LRUCache object will be instantiated and called as such: * LRUCache* obj = new LRUCache(capacity); * int param_1 = obj->get(key); * obj->put(key,value); */ leetcode 30-串联所有单词的子串 leetcode 30. 串联所有单词的子串 给定一个字符串 s 和一些 长度相同 的单词 words。找出 s 中恰好可以由 words 中所有单词串联形成的子串的起始位置。 注意子串要与 words 中的单词完全匹配，中间不能有其他字符 ，但不需要考虑 words 中单词串联的顺序。 示例 1： 输入：s = \"barfoothefoobarman\", words = [\"foo\",\"bar\"] 输出：[0,9] 解释： 从索引 0 和 9 开始的子串分别是 \"barfoo\" 和 \"foobar\" 。 输出的顺序不重要, [9,0] 也是有效答案。 解题思路：滑动窗口 + 哈希表。滑动窗口的大小为 $k*len$。 从 words 出发，考虑 words 所有单词排列生成的字符串 X，通过字符串匹配查看 X 在 s 中的出现位置，但是 X 的可能情况有 $k!$ 种，$k$ 为 words 中单词的个数，明显超时！ 从 s 串出发，遍历 s 串中所有长度为 (words[0].length * words.length) 的子串 Y，并判断 Y 是否可以由 words 数组构造生成。 代码步骤：首先构建 words 单词出现次数的哈表表，然后滑动窗口移动的时候，每次获取 len 长度的子串，并判断这个子串是否在 words 中，并构建子串出现次数的哈希表，同时要求子串出现的次数不能大于原来 words 中单词出现的次数。 时间复杂度：$O((n-k len) k)$，$n$ 是字符串 s 的长度，$k$ 是 words 中单词的个数，len 是每个单词的长度。 这道 hard 题目居然被我做出来了！代码第二次修改参考了西法的剪枝代码，之前自己用嵌套 if 判断实在太傻了。 class Solution { public: vector findSubstring(string s, vector& words) { unordered_map freq; // 计算字符串数组中每个单词出现的频率 for(auto s1: words){ freq[s1]++; } vector ret; int len = words[0].size(); for(int i=0; i freq2; while(num freq[target]) break; // 剪枝 pos += len; num++; } if(num-1 == words.size()) ret.push_back(i); } return ret; } }; leetcode 解题方法： 根据同余定理，只要求前缀和 $p[i]$ 和 $p[j]$ 模数 $k$ 同余出现的次数。 前缀和：使用公式 $pre[i]=pre[i−1]+nums[i]$ 得到每一位前缀和的值，从而通过前缀和进行相应的计算和解题。 同余定理：给定一个正整数 $m$，如果两个整数 $a$ 和 $b$ 满足 $a-b$ 能够被 $m$ 整除，即 $(a-b)/m$ 得到一个整数，那么就称整数 $a$ 与 $b$ 对模 $m$ 同余，记作 a≡b(mod m)。对模 $m$ 同余是整数的一个等价关系。 class Solution { public: int subarraysDivByK(vector& nums, int k) { // 哈希表初始化，record[0] = 1 unordered_map record = {{0, 1}}; int sum = 0, ans = 0; for (int elem: nums) { sum += elem; // 注意 C++ 取模的特殊性，当被除数为负数时取模结果为负数，需要纠正 int modulus = (sum % k + k) % k; // 边遍历边计算答案 if (record.count(modulus)) { ans += record[modulus]; } ++record[modulus]; } return ans; } }; 6，二叉树 07-重建二叉树 剑指 Offer 07-重建二叉树 输入某二叉树的前序遍历和中序遍历的结果，请构建该二叉树并返回其根节点。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。 示例1： Input: preorder = [3,9,20,15,7], inorder = [9,3,15,20,7] Output: [3,9,20,null,null,15,7] 解题方法： 1，递归法 中序遍历的结果可以获取左右子树的元素个数； 前序遍历结果可以获取树的根节点 node 的值。 C++代码： class Solution { public: TreeNode* buildTree(vector& preorder, vector& inorder) { // 哈希表 dic 存储中序遍历的值与索引的映射 for(int i=0; i index; TreeNode* recur(vector& preorder, int root, int left, int right){ if (left > right) return nullptr; int i = index[preorder[left]]; // 获取中序遍历中根节点值的索引 TreeNode* node = new TreeNode(preorder[left]); node->left = recur(preorder, root+1, left, i-1); node->right = recur(preorder, root+i-left+1, i+1, right); return node; } }; leetcode 104-二叉树的最大深度 104-二叉树的最大深度 给定一个二叉树，找出其最大深度。二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。 说明: 叶子节点是指没有子节点的节点。 示例： 给定二叉树 [3,9,20,null,null,15,7]， 3 / \\ 9 20 / \\ 15 7 返回它的最大深度 3 。 55.2-平衡二叉树 55.2-平衡二叉树 给定一个二叉树，判断它是否是高度平衡的二叉树。本题中，一棵高度平衡二叉树定义为： 一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1。 leetcode 109-有序链表转换二叉搜索树 leetcode109. 有序链表转换二叉搜索树 给定一个单链表，其中的元素按升序排序，将其转换为高度平衡的二叉搜索树。 本题中，一个高度平衡二叉树是指一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1。 示例: 给定的有序链表： [-10, -3, 0, 5, 9], 一个可能的答案是：[0, -3, 9, -10, null, 5], 它可以表示下面这个高度平衡二叉搜索树： 0 / \\ -3 9 / / -10 5 解题方法： 1，将单调递增链表转化为数组，然后分治递归。 2，快慢指针找链表的中间节点，然后递归。 复杂度分析： 时间复杂度: O(n) 空间复杂度: O(n) C++代码： class Solution { public: // 分治递归 TreeNode* sortedListToBST(ListNode* head) { vector vec; for(auto it = head; it!=nullptr ; it=it->next ){ vec.push_back( it->val ); } return recur(vec, 0, vec.size()-1); } TreeNode* recur(vector &arr, int left, int right){ if(left > right) return nullptr; int mid = right + (left-right)/2; // 数组中间位置的索引 TreeNode* node = new TreeNode(arr[mid]); node -> left = recur(arr, left, mid - 1); node -> right = recur(arr, mid + 1, right); return node; } }; 7，图 二，算法 1，递归 10-1. 斐波那契数列 10-1. 斐波那契数列 写一个函数，输入 $n$，求斐波那契（Fibonacci）数列的第 $n$ 项（即 $F(n)$）。斐波那契数列的定义如下： F(0) = 0, F(1) = 1 F(n) = F(n - 1) + F(n - 2), 其中 n > 1. 斐波那契数列由 0 和 1 开始，之后的斐波那契数就是由之前的两数相加而得出。答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1 解题方法： 1，记忆化递归 2，迭代法 C++代码： // 剑指 offer 10-1. 斐波那契数列 class Solution { private: static const int mod = 1e9 + 7; int m = 101; vector vec = vector(101, -1); // c++11 之后，类 private成员初始化方式 public: // 1，直接递归会超出时间限制，需要使用记忆化递归 constexpr int fib(int n) { if (n == 0) return 0; if (n == 1 || n == 2) return 1; if (vec[n] != -1) return vec[n]; vec[n] = (fib(n - 1) + fib(n - 2)) % mod; return vec[n]; } // 2，迭代求解 int fib(int n) { int arr[101]; arr[0] = 0; arr[1] = 1; arr[2] = 1; for (int i = 2; i 2，二分查找 3，排序 4，贪心 63-股票的最大利润 63-股票的最大利润 假设把某股票的价格按照时间先后顺序存储在数组中，请问买卖该股票一次可能获得的最大利润是多少？ 示例 1: 输入: [7,1,5,3,6,4] 输出: 5 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格。 解题方法： 1，贪心法：假设每天的股价都是最低价，每天都计算股票卖出去后的利润。一次 for 循环，时间复杂度：$O(n)$ 2，暴力法：两次 for 循环，时间复杂度 $O(n^2)$ C++代码： # include # include # include using namespace std; class Solution { public: int maxProfit(vector& prices) { // 贪心算法：一次遍历 int inf = 1e9; // 表示“无穷大” int minprice = inf, maxprofit = 0; for(int price: prices){ maxprofit = max(maxprofit, (price-minprice)); // 假设每天都是最低价 minprice = min(minprice, price); } return maxprofit; } }; int main(){ vector prices = {7,1,5,3,6,4}; Solution s1; int max_profit = s1.maxProfit(prices); cout 5，分治 6，回溯 7，动态规划 10.2-青蛙跳台阶问题 剑指offer 10.2-青蛙跳台阶问题 一只青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。 答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 解题方法： 1，动态规划法：以斐波那契数列性质 $f(n + 1) = f(n) + f(n - 1)$ 为转移方程。 状态定义： 设 $dp$ 为一维数组，其中 $dp[i]$ 的值代表斐波那契数列第 $i$ 个数字 。 转移方程： $dp[i + 1] = dp[i] + dp[i - 1]$ ，即对应数列定义 $f(n + 1) = f(n) + f(n - 1)$； 初始状态： $dp[0] = 1, dp[1] = 1$，即初始化前两个数字； 返回值： $dp[n]$，即斐波那契数列的第 $n$ 个数字。 C++代码： class Solution { private: static const int mod = 1e9 + 7; public: // 动态规划法 int numWays(int n) { int dp[n+1]; if( n == 0 || n == 1) return 1; dp[0] = 1; dp[1] = 1; for(int i=2; i 42-连续子数组的最大和 剑指offer 42-连续子数组的最大和 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 示例 1： 输入：nums = [-2,1,-3,4,-1,2,1,-5,4] 输出：6 解释：连续子数组 [4,-1,2,1] 的和最大，为 6 。 解题思路： 动态规划法。 C++代码： class Solution { public: //1, 动态规划算法 int maxSubArray2(vector& nums) { int* dp = new int[nums.size()]; dp[0] = nums[0]; int maxSum = dp[0]; for(int i=1; i & nums) { int sum = nums[0]; int maxSum = nums[0]; for(int i=1; i 47-礼物的最大价值 剑指offer 47-礼物的最大价值 在一个 $m\\ast n$ 的棋盘的每一格都放有一个礼物，每个礼物都有一定的价值（价值大于 0）。你可以从棋盘的左上角开始拿格子里的礼物，并每次向右或者向下移动一格、直到到达棋盘的右下角。给定一个棋盘及其上面的礼物的价值，请计算你最多能拿到多少价值的礼物？ 示例 1: 输入: [ [1,3,1], [1,5,1], [4,2,1] ] 输出: 12 解释: 路径 1→3→5→2→1 可以拿到最多价值的礼物 解题方法： 动态规划-状态转移方程法。 C++代码： class Solution { // 状态转移方程法 private: int minDist(int i, int j, vector >& matrix, vector >& mem) { // 调用minDist(n-1, n-1); if (i == 0 && j == 0) return matrix[0][0]; if (mem[i][j] > 0) return mem[i][j]; int minUp = -10000; if (i - 1 >= 0) minUp = minDist(i - 1, j, matrix, mem); int minLeft = -10000; if (j - 1 >= 0) minLeft = minDist(i, j - 1, matrix, mem); int currMinDist = matrix[i][j] + std::max(minUp, minLeft); mem[i][j] = currMinDist; return currMinDist; } public: int maxValue(vector>& grid) { int m = grid.size(); int n = grid[0].size(); vector > mem(m, vector(n, -1)); return minDist(m - 1, n - 1, grid, mem); } }; 48-最长不含重复字符的子字符串 剑指offer 42-最长不含重复字符的子字符串 请从字符串中找出一个最长的不包含重复字符的子字符串，计算该最长子字符串的长度。 示例 1: 输入: \"abcabcbb\" 输出: 3 解释: 因为无重复字符的最长子串是 \"abc\"，所以其长度为 3。 解题思路： 动态规划。参考这里。 滑动窗口法 + 哈希表结构。 C++代码： class Solution { public: // 动态规划+线性遍历 int lengthOfLongestSubstring(string s) { int res=0, tmp = 0, i=0; for(int j=0; j =0 && s[i] != s[j]) i-= 1; if(tmp seen; int maxLength = 0, l = 0; for(int r=0; r 0) { int last_pos = seen[s[r]]; // 位置判断不可少，重复字符的位置必须是在滑动窗口内！ if(last_pos >= l) l = last_pos + 1; // last_pos 66-构建乘积数组 剑指offer 66-构建乘积数组 给定一个数组 $A[0,1,…,n-1]$，请构建一个数组 $B[0,1,…,n-1]$，其中 $B[i]$ 的值是数组 $A$ 中除了下标 $i$ 以外的元素的积, 即 $B[i]=A[0]×A[1]×…×A[i-1]×A[i+1]×…×A[n-1]$。不能使用除法。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"3-data_structure-algorithm/算法图解笔记.html":{"url":"3-data_structure-algorithm/算法图解笔记.html","title":"算法图解笔记","keywords":"","body":" 前言知识 第一章，算法简介 1.2，二分法查找元素 1.2.1，特殊的二分查找 第二章，选择排序 2.1，内存工作原理 2.2.1，链表 2.2.2，数组 2.2.3，术语 2.3，选择排序 2.4，小结 第三章，递归 3.2，基线条件和递归条件 3.3，栈 3.3.1，调用栈 3.3.2，递归调用栈 3.4，小结 第四章，快速排序 4.1，分而治之 4.2 快速排序 4.3 再谈大Ｏ表示法 4.4，小结 第五章，散列表 5.3，冲突 5.4，性能 5.5，小结 第六章，广度优先搜索 6.1，图是什么 6.2，广度优先搜索 6.3，栈与队列 6.4，代码实现图结构 6.4.1 运行时间 第七章，迪克斯特拉算法 7.1，使用迪克斯特拉算法 7.2，术语 7.3，负权边 7.4，编程实现狄克斯特拉算法 7.5，小结 第八章，贪婪(贪心)算法 8.1，教室调度问题 8.2，背包问题 8.3，集合覆盖问题 8.3.1，近似算法 8.4，NP完全问题 8.4.1，如何识别NP完全问题 8.5，小结 第九章，动态规划 9.1，概念 9.2，背包问题 9.1，最长公共子串 第十章，K最近邻算法 第十一章　接下来如何做 前言知识 十大经典排序算法动画与解析，看我就够了！（配代码完全版） 10 大排序算法时间复杂度及空间复杂度如下图： 第一章，算法简介 1.2，二分法查找元素 二分查找是一种算法，其输入是一个有序的元素列表（必须有序的原因稍后解释）。如果要查找的元素包含在列表中，二分查找返回其位置；否则返回 null，使用二分查找时，每次猜测的是中间的数字，从而将余下的数字排除一半。（仅仅当列表是有序的时候，二分查找才管用） 一般而言，对于包含n个元素的列表查找某个元素，使用二分法最多需要 $log{2}n$ 步(**时间复杂度为 $log{2}n$ **)，简单查找最多需要 n 步。大 O 表示法指出了算法最糟糕情况下的运行时间。二分法实例代码如下： def binary_search(list, item): low = 0 high = len(list)-1 while low item: high = mid - 1 else: low = mid + 1 return None if __name__ == \"__main__\": print(binary_search([1,2,3,4,6,7], 3)) # 输出 2 1.2.1，特殊的二分查找 有序数组中的目标出现多次，利用二分查找返回在最左边出现的目标值或者是最右边出现的目标值，实例代码如下： def binary_search2(arr, target, flag=\"left\"): if not arr: return None left = 0 right = len(arr) - 1 while left target: right = mid -1 else: if flag == \"left\": if mid > 0 and arr[mid-1] == target: right = mid -1 # 不断向最左边逼近 else: return mid elif flag == \"right\": if mid + 1 第二章，选择排序 2.1，内存工作原理 在计算机中，存储多项数据时，有两种基本方式－数组和链表。但它们并非适用于所有情形。 2.2.1，链表 链表中的元素可存储在内存的任何地方。 链表的每个元素都存储了下一个元素的地址，从而使一系列随机的内存地址串在一起。链表结构直观显示如下图所示： 链表的优势在插入元素方面，那数组的优势又是什么呢？ 2.2.2，数组 需要随机地读取元素时，数组的效率很高，因为可迅速找到数组的任何元素。在链表中，元素并非靠在一起的，你无法迅速计算出第五个元素的内存 地址，而必须先访问第一个元素以获取第二个元素的地址，再访问第二个元素以获取第三个元素 的地址，以此类推，直到访问第五个元素。 2.2.3，术语 数组的元素带编号，编号从 0 而不是 １ 开始，几乎所有的编程语言都从０开始对数组元素进行编号，比如C/C++的数组结构和Python的列表结构。 元素的位置称为索引。下面是常见数组和链表操作的运行时间. 2.3，选择排序 选择排序时间复杂度$O(n^{2})$ def findSmallest(arr): smallest = arr[0] # 存储最小的值 smallest_index = 0 # 存储最小元素的索引 for i in range(1, len(arr)): if arr[i] 2.4，小结 计算机内存犹如一大堆抽屉。 需要存储多个元素时，可使用数组或链表。 数组的元素都在一起。 链表的元素是分开的，其中每个元素都存储了下一个元素的地址。 数组的读取速度很快。 链表的插入和删除速度很快. 在同一个数组中，所有元素的类型都必须相同（都为int、 double等）。 第三章，递归 学习如何将问题分成基线条件和递归条件，学习如何使用递归算法，递归算法直观上更好理解，步骤简单。 3.2，基线条件和递归条件 编写递归函数时，必须告诉它何时停止，因此，每个递归函数有两个部分：基线条件(base case)和递归条件(recursive case)。递归条件指的是函数调用自己，而基线条件则 指的是函数不再调用自己，从而避免形成无限循环。 3.3，栈 栈的定义：栈是一种只能从表的一端存取数据且遵循 \"先进后出\" 原则的线性存储结构。 调用栈(call stack) 3.3.1，调用栈 计算机在内部使用被称为调用栈的栈。调用另一个函数时，当前函数暂停 并处于未完成状态。该函数的所有变量的值都还在内存中。栈顶的方框指出了当前执行 到了什么地方。 3.3.2，递归调用栈 栈在递归中扮演着重要角色。使用栈虽然很方便，但是也要付出代价：存储详尽的信息可能占用大量的内存。每个函数调 用都要占用一定的内存，如果栈很高，就意味着计算机存储了大量函数调用的信息。在这种情况 下，你有两种选择。 重新编写代码 使用尾递归 3.4，小结 递归值的是调用自己的函数 每个递归函数都有两个条件：基线条件和递归条件 栈有两种操作：压如和弹出 所有函数调用都进入调用栈 调用栈可能很长，这将占用大量内存 第四章，快速排序 快速排序使用分而治之的策略，分而治之是我们学习的第一种通用的问题解决办法。 分而治之(divide and conquer，D&C)-一种著名的递归式问题解决办法。 4.1，分而治之 D&C算法是递归的。使用D&C解决问题的过程包括两个步骤： 找出基线条件，这种条件必须尽可能简单。 不断将问题分解（或者说缩小规模），直到符合基线条件。 D&C并非可直接用于解决问题的算法，而是一种解决问题的思路。 4.2 快速排序 C语言标准库中的函数qsort实现的就是快速排序。快速排序也是用了D&C思想。 对数组进行快速排序，步骤如下： 随机选择一个基准值； 将数组分成两个子数组：小于基准值的元素和大于基准值额元素； 对这两个子数组进行排序。 在平均情况下，快速排序时间复杂度$O(nlogn)$。快速排序代码如下： def quicksort(array): if len(array) pivot] return quicksort(less) + [pivot] + quicksort(greater) print(quicksort([4, 90, 0, 2, 17, 79, 12])) # [0, 2, 4, 12, 17, 79, 90] 上面的代码空间复杂度很大，真正的快排是原地排序，空间复杂度为O(1)，代码如下： # _*_ coding:utf-8 _*_ def quick_sort(L): return q_sort(L, 0, len(L)-1) def q_sort(L, left, right): if left = pivotkey: right -= 1 L[left] = L[right] while left 4.3 再谈大Ｏ表示法 快速排序的独特之处在于，其速度取决于选择的基准值。在讨论快速排序的运行时间前，我 们再来看看最常见的大O运行时间。 选择排序，其运行时间为 $O(n^2)$，速度非常慢。 还有一种名为合并排序（merge sort）的排序算法，其运行时间为 $O(nlogn)$，比选择排序快得多！ 快速排序的情况比较棘手，在最糟情况下，其运行时间为 $O(n^2)$。与选择排序一样慢！但这是最糟情况。在平均情况下，快速排序的运行时间为 $O(nlogn)$。 由对数的换底公式，$log_a n$ 和 $log_b n$ 只有一个常数因子不同，这个因子在大O记法中被丢弃。因此记作$O(log n)$，而不论对数的底是多少，是对数时间算法的标准记法。 4.4，小结 D&C将问题逐步分解。使用D&C处理列表时，基线条件很可能是空数组或只包含一个元 素的数组。 实现快速排序时，请随机地选择用作基准值的元素。快速排序的平均运行时间为O(n log n)。 大O表示法中的常量有时候事关重大，这就是快速排序比合并排序快的原因所在。 比较简单查找和二分查找时，常量几乎无关紧要，因为列表很长时， O(log n)的速度比O(n) 快得多。 第五章，散列表 数组和链表结构可以用以查找，栈不行。散列表也叫哈希表(Hash table)，散列表有些类似 Python 中的字典 dict 结构。散列表可用以： 模拟映射关系； 防止重复； 缓冲/记住数据，以免服务器再通过处理来生成它们。 5.3，冲突 给两个键分配的位置相同，这被称为冲突（collision）。处理冲突最简单的办法就是：如果两个键映射到了同一个位置，就在这个位置存储一个链表。 5.4，性能 散列表，数组，链表的查找、插入删除元素的时间复杂度，如下表所示： 在平均情况下，散列表的查找（获取给定索引处的值）速度与数组一样快，而插入和删除速 度与链表一样快，因此它兼具两者的优点！但在最糟情况下，散列表的各种操作的速度都很慢。 因此，在使用散列表时，避开最糟情况至关重要。为此，需要避免冲突。而要避免冲突，需要有： 较低的填装因子； 良好的散列函数。 5.5，小结 散列表是一种功能强大的数据结构，其操作速度快，还能让你以不同的方式建立数据模型。 你可能很快会发现自己经常在使用它。 你可以结合散列函数和数组来创建散列表。 冲突很糟糕，你应使用可以最大限度减少冲突的散列函数。 散列表的查找、插入和删除速度都非常快。 散列表适合用于模拟映射关系。 一旦填装因子超过0.7，就该调整散列表的长度。 散列表可用于缓存数据（例如，在Web服务器上）。 散列表非常适合用于防止重复。 第六章，广度优先搜索 图算法：广度优先搜索（breadth-first search, BFS）算法 广度优先搜索让你能够找出两样东西之间的最短距离，不过最短距离的含义有很多！使用广度优先搜索可以： 编写国际跳棋AI，计算最少走多少步就可获胜； 编写拼写检查器，计算最少编辑多少个地方就可将错拼的单词改成正确的单词，如将 READED改为READER需要编辑一个地方； 根据你的人际关系网络找到关系最近的医生。 解决最短路径问题的算法被称为广度有限算法，一般步骤为： 使用图来建立问题模型。 使用广度优先搜索解决问题。 6.1，图是什么 图由节点（node）和边（edge）组成。 6.2，广度优先搜索 在广度优先搜索的执行过程中，搜索范围从起点开始逐渐向外延伸，即先检查一度关系，再检查二度关系。 6.3，栈与队列 栈：后进先出（Last In First Out，LIFO）的数据结构 队列：先进先出（First In First Out，FIFO）的数据结构，支持入队和出对操作。 6.4，代码实现图结构 图中每个节点都与相邻节点相连，散列表结构可以表示这种关系。 图分为有向图（directed graph）和无向图（undirected graph），有向图关系是单向的，无向图没有箭头，直接相连的节点互为邻居。对从自己出发有指向他人的箭头，则有邻居。 6.4.1 运行时间 如果你在你的整个人际关系网中搜索芒果销售商，就意味着你将沿每条边前行（记住，边是从一个人到另一个人的箭头或连接），因此运行时间至少为 $O(边数)$。你还使用了一个队列，其中包含要检查的每个人。将一个人添加到队列需要的时间是固定的，即为 $O(1)$，因此对每个人都这样做需要的总时间为 $O(人数)$。所以，广度优先搜索的运行时间为 $O(人数 + 边数)$，这通常写作 $O(V + E)$，其中 $V$ 为顶点（ vertice）数， $E$ 为边数。 第七章，迪克斯特拉算法 7.1，使用迪克斯特拉算法 迪克斯特拉算法能够找出加权图中前往X的最短路径。对于寻找耗时最少的路径，迪克斯特拉算法包含４个步骤： 找出“最便宜”的节点，即可在最短时间内到达的节点。 更新该节点的邻居的开销，其含义将稍后介绍。 重复这个过程，直到对图中的每个节点都这样做了。 计算最终路径。 每个节点都有开销。开销指的是从起点前往该节点需要多长时间。 7.2，术语 带权重的图称为加权图（ weighted graph），不带权重的图称为非加权图（ unweighted graph）。 要计算非加权图中的最短路径，可使用广度优先搜索。要计算加权图中的最短路径，可使用狄克斯特拉算法。 在无向图中，每条边都是一个环。狄克斯特拉算法只适用于有向无环图（ directed acyclic graph， DAG）。 图可能有环，所谓环，是指由一个节点出发，走一圈后可以又回到原节点，如下图所示： 7.3，负权边 因此， 不能将狄克斯特拉算法用于包含负权边的图。在包含负权边的图中，要找出最短路径，可使用另一种算法——贝尔曼福德算法（ Bellman-Ford algorithm）。 7.4，编程实现狄克斯特拉算法 以下图为例，编程实现耗时最短的路径。 代码如下： # 为了实现带权图，可使用散列表，散列表用Python字典实现 graph = {} # 存储起始节点邻居和前往邻居的开销 graph['start'] = {} graph[\"start\"][\"a\"] = 6 graph[\"start\"][\"b\"] = 2 print(graph[\"start\"].keys()) # 添加其他节点及其邻居 graph[\"a\"] = {} graph[\"a\"][\"fin\"] = 1 graph[\"b\"] = {} graph[\"b\"][\"a\"] = 3 graph[\"b\"][\"fin\"] = 5 # 终点没有任何邻居 graph['fin'] = {} # 创建存储每个节点开销的开销表 infinity = float(\"inf\") costs = {} costs[\"a\"] = 6 costs[\"b\"] = 2 costs[\"fin\"] = infinity # 创建存储父节点的散列表 parents = {} parents[\"a\"] = \"start\" parents[\"b\"] = \"start\" parents[\"fin\"] = None # 创建一个数组，用于记录处理过的节点 processed = [] # 找出开销最低的节点 def find_lowest_cost_node(costs): lowest_cost = float(\"inf\") lowest_cost_node = None for node in costs: cost = costs[node] if cost new_cost: costs[n] = new_cost parents[n] = node # 将当前节点标记为处理过 processed.append(node) # 找出接下来要处理的节点，并循环 node = find_lowest_cost_node(costs) print(\"Cost from the start to each node:\") print(costs) 7.5，小结 广度优先搜索用于在非加权图中查找最短路径。 狄克斯特拉算法用于在加权图中查找最短路径。 仅当权重为正时狄克斯特拉算法才管用。 如果图中包含负权边，请使用贝尔曼福德算法。 第八章，贪婪(贪心)算法 贪婪算法思想很简单：每步都采取最优的做法，专业术语说，就是每步都选择局部最优解，最终得到的就是全局最优解。 8.1，教室调度问题 根据给定课表，尽可能将更多的课程安排在某间教室。解决办法：贪婪算法可找到最优解。 8.2，背包问题 背包重量有限，根据策略使得装入背包的物品价值最高。 在这里， 贪婪策略显然不能获得最优解，但非常接近。在有些情况下，完美是优秀的敌人。有时候，你只需找到一个能够大致解决问题的算法，此时贪婪算法正好可派上用场，因为它们实现起来很容易，得到的结果又与正确结果相当接近。 8.3，集合覆盖问题 每个广播台都覆盖特定区域的州，找出覆盖全美50个州的最小广播集合。 贪婪算法解决这个问题，当广播台数量过多，算法所耗费的时间将激增。 8.3.1，近似算法 集合覆盖问题举例：每个广播台都覆盖特定区域的州，找出覆盖全美50个州的最小广播集合。贪婪算法可以解决这个问题，当广播台数量过多，算法所耗费的时间将激增。 选出这样一个广播台，即它覆盖了最多的未覆盖州。即便这个广播台覆盖了一些已覆盖的州，也没有关系。 重复第一步，直到覆盖了所有的州。 这是一种近似算法（ approximation algorithm） 。在获得精确解需要的时间太长时，可使用近似算法。判断近似算法优劣的标准如下： 速度有多快； 得到的近似解与最优解的接近程度。 代码实例： \"\"\" 准备工作 \"\"\" # 创建一个列表，包含要覆盖的州 states_needed = set([\"mt\", \"wa\", \"or\", \"id\", \"nv\", \"ut\", \"ca\", \"az\"]) # 广播台清单 stations = {} stations[\"kone\"] = set([\"id\", \"nv\", \"ut\"]) stations[\"ktwo\"] = set([\"wa\", \"id\", \"mt\"]) stations[\"kthree\"] = set([\"or\", \"nv\", \"ca\"]) stations[\"kfour\"] = set([\"nv\", \"ut\"]) stations[\"kfive\"] = set([\"ca\", \"az\"]) # 定义一个集合存储最终选择的广播台 final_stations = set() \"\"\" 计算答案 \"\"\" best_station = None while states_needed: best_station = None states_covered = set() for station, states in stations.items(): covered = states_needed & states if len(covered) > len(states_covered): best_station = station states_covered = covered states_needed -= states_covered final_stations.add(best_station) print(final_stations) 程序输出如下： {'kone', 'ktwo', 'kthree', 'kfive'} 贪心算法的实质是每次选出当前的最优解，不管整体，是基于一定假设下的最优解。 8.4，NP完全问题 旅行商问题和集合覆盖问题有一些共同之处：你需要计算所有的解，并从中选出最小/最短的那个。这两个问题都属于NP完全问题。NP完全问题的简单定义是，以难解著称的问题，如旅行商问题和集合覆盖问题。很多非常聪明的人都认为，根本不可能编写出可快速解决这些问题的算法。 8.4.1，如何识别NP完全问题 NP 完全问题无处不在！如果能够判断出要解决的问题属于 NP 完全问题就好了，这样就不用 去寻找完美的解决方案，而是使用近似算法即可。但要判断问题是不是NP完全问题很难，易于解决的问题和 NP 完全问题的差别通常很小。 但如果要找出经由指定几个点的的最短路径，就是旅行商问题——NP完全问题。简言之，没办法判断问题是不是 NP 完全问题，但还是有一些蛛丝马迹可循的。 元素较少时算法的运行速度非常快，但随着元素数量的增加，速度会变得非常慢。 涉及“所有组合”的问题通常是NP完全问题。 不能将问题分成小问题，必须考虑各种可能的情况。这可能是NP完全问题。 如果问题涉及序列（如旅行商问题中的城市序列）且难以解决，它可能就是NP完全问题。 如果问题涉及集合（如广播台集合）且难以解决，它可能就是NP完全问题。 如果问题可转换为集合覆盖问题或旅行商问题，那它肯定是NP完全问题。 8.5，小结 贪婪算法寻找局部最优解，企图以这种方式获得全局最优解。 对于NP完全问题，还没有找到快速解决方案。 面临NP完全问题时，最佳的做法是使用近似算法。 贪婪算法易于实现、运行速度快，是不错的近似算法。 第九章，动态规划 9.1，概念 动态规划算法是通过拆分问题，定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。在学习动态规划之前需要明确掌握几个重要概念，如下： 阶段：对于一个完整的问题过程，适当的切分为若干个相互联系的子问题，每次在求解一个子问题，则对应一个阶段，整个问题的求解转化为按照阶段次序去求解。 状态：状态表示每个阶段开始时所处的客观条件，即在求解子问题时的已知条件。状态描述了研究的问题过程中的状况。 决策：决策表示当求解过程处于某一阶段的某一状态时，可以根据当前条件作出不同的选择，从而确定下一个阶段的状态，这种选择称为决策。 策略：由所有阶段的决策组成的决策序列称为全过程策略，简称策略。 最优策略：在所有的策略中，找到代价最小，性能最优的策略，此策略称为最优策略。 状态转移方程：状态转移方程是确定两个相邻阶段状态的演变过程，描述了状态之间是如何演变的。 9.2，背包问题 学习动态规划，这是一种解决棘手问题的方法，它将问题分成小问题，并先着手解决这些小问题，每个动态规划问题都是从一个网格入手，背包问题的网格如下： 工作原理：动态规划先解决子问题，再逐步解决大问题。从背包问题的网格计算入手，可明白为何计算小背包可装入的商品的最大价值。余下了空间时，你可根据这些子问题的答案来确定余下的空间可装入哪些商品。计算每个单元格的价值时，使用的公式都相同。 这个公式如下： 网格的行顺序发生变化时，最终答案没有变化。各行的排列顺序对最终结果无关紧要。 动态规划功能强大，它能够解决子问题并使用这些答案来解决大问题。 但仅当每个子问题都是离散的，即不依赖于其他子问题时，动态规划才管用。 这意味着使用动态规划算 法解决不了去巴黎玩的问题。 9.1，最长公共子串 通过动态规划问题，得到以下启示： 动态规划可帮助你在给定约束条件下找到最优解在背包问题中，你必须在背包容量给定的情况下，偷到价值最高的商品。 在问题可分解为彼此独立且离散的子问题时，就可使用动态规划来解决。 要设计出动态规划解决方案可能很难，这正是本节要介绍的。下面是一些通用的小贴士： 每种动态规划解决方案都涉及网格。 单元格中的值通常就是你要优化的值。在前面的背包问题中，单元格的值为商品的价值。 每个单元格都是一个子问题，因此你应考虑如何将问题分成子问题，这有助于你找出网格的坐标轴。 第十章，K最近邻算法 第十一章　接下来如何做 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"4-machine_learning/":{"url":"4-machine_learning/","title":"4. 机器学习","keywords":"","body":"前言 本目录内容旨在分享机器学习数学基础笔记、机器学习算法总结和机器学习经典面试题总结知识。 目录 机器学习基础 西瓜书笔记 常见机器学习算法总结 4-机器学习面试题 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"4-machine_learning/3-常见机器学习算法.html":{"url":"4-machine_learning/3-常见机器学习算法.html","title":"3-常见机器学习算法","keywords":"","body":" KNN 算法 k 值的选取 KNN 算法思路 支持向量机算法 支持向量机简述 SVM 基本型 对偶问题求解 K-means 聚类算法 分类与聚类算法 K-means 聚类算法 参考资料 KNN 算法 K 近邻算法（KNN）是一种基本分类和回归方法。KNN 算法的核心思想是如果一个样本在特征空间中的 k 个最相邻的样本中的大多数属于一个类别，那该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分类样本所属的类别。 如下图： 在 KNN 中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离，公式如下： 同时，KNN通过依据 k 个对象中占优的类别进行决策，而不是单一的对象类别决策，这两点就是KNN算法的优势。 k 值的选取 k 值较小，模型会变得复杂，容易发生过拟合 k 值较大，模型比较简单，容易欠拟合 所以 k 值得选取也是一种调参？ KNN 算法思路 KNN 思想就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前 K 个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为： 计算测试数据与各个训练数据之间的距离； 按照距离的递增关系进行排序； 选取距离最小的 K 个点； 确定前 K 个点所在类别的出现频率； 返回前 K 个点中出现频率最高的类别作为测试数据的预测分类。 支持向量机算法 机器学习中的算法(2)-支持向量机(SVM)基础 支持向量机简述 支持向量机（Support Vector Machines, SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使其成为实质上的非线性分类器。 SVM 的学习策略是找到最大间隔（两个异类支持向量到超平面的距离之和 $\\gamma = \\frac{2}{||w}$ 称为“间隔”），可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。 SVM 的最优化算法是求解凸二次规划的最优化算法。 SVM 基本型 $$min \\frac{1}{2}||w||^2$$ $$s.t. y_{i}(w^Tx_i + b) \\geq 1, i = 1,2,...,m$$ SVM 的最优化算法是一个凸二次规划（convex quadratic programming）问题，对上式使用拉格朗日乘子法可转化为对偶问题，并优化求解。 对偶问题求解 对 SVM 基本型公式的每条约束添加拉格朗日乘子 $\\alpha_i \\geq 0$，则式子的拉格朗日函数如下： $$L(w,b,a) = \\frac 1 2||w||^2 - \\sum{i=1}{n} \\alpha_i (y_i(w^Tx_i+b) - 1)$$ 经过推导（参考机器学习西瓜书），可得 SVM 基本型的对偶问题： $$\\max\\limits{\\alpha} \\sum{i=1}^{m}-\\frac{1}{2} \\sum{i=1}^{m} \\sum{j=1}^{m}\\alphai \\alpha_j y_i y_j x^{T}\\{i} x_j$$ $$s.t. \\sum{i=1}^{m} = \\alpha{i}y_{i} = 0$$ $$\\alpha_{i}\\geq 0, i=1,2,...,m$$ 继续优化该问题，有 SMO 方法，SMO 的基本思路是先固定 $\\alpha_i$ 之外的的所有参数，然后求 $\\alpha_i$ 上的极值。 K-means 聚类算法 分类与聚类算法 分类简单来说，就是根据文本的特征或属性，划分到已有的类别中。也就是说，这些类别是已知的，通过对已知分类的数据进行训练和学习，找到这些不同类的特征，再对未分类的数据进行分类。 聚类，就是你压根不知道数据会分为几类，通过聚类分析将数据或者说用户聚合成几个群体，那就是聚类了。聚类不需要对数据进行训练和学习。 分类属于监督学习，聚类属于无监督学习。常见的分类比如决策树分类算法、贝叶斯分类算法等聚类的算法最基本的有系统聚类，K-means 均值聚类。 K-means 聚类算法 聚类的目的是找到每个样本 $x$ 潜在的类别 $y$，并将同类别 $y$ 的样本 $x$ 放在一起。在聚类问题中，假定训练样本是 ${x^1,...,x^m}$，每个 $x^i \\in R^n$，没有 $y$。K-means 算法是将样本聚类成 $k$ 个簇（cluster），算法过程如下： 随机选取 $k$ 个聚类中心（cluster centroids）为 $\\mu_1, \\mu_1,...,\\mu_k \\in R^n$。 重复下面过程，直到质心不变或者变化很小： 对于每一个样例 $i$ ，计算其所属类别：$$c^i = \\underset{j}{argmin}||x^i - \\mu_j||^2$$ 对于每一个类 $j$，重新计算该类的质心：$$\\muj = \\frac {\\sum{i=1}^{m} 1{c^i} = jx^{i}} { \\sum_{i=1}^{m}1 c^{i} = j}$$ $K$ 是我们事先给定的聚类数，$c^i$ 代表样例 $i$ 与 $k$ 个类中距离最近的那个类，$c^i$ 的值是 $1$ 到 $k$ 中的一个。质心 $\\mu_j$ 代表我们对属于同一个类的样本中心点的猜测。 参考资料 K-means聚类算法 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"4-machine_learning/4-机器学习面试题.html":{"url":"4-machine_learning/4-机器学习面试题.html","title":"4-机器学习面试题","keywords":"","body":"前言 等待更新。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"5-deep_learning/":{"url":"5-deep_learning/","title":"5. 深度学习","keywords":"","body":"前言 本目录内容旨在分享深度学习基础、Backbone 网络论文以及深度学习框架使用及解析的技术笔记。 深度学习基础 1-深度学习算法基础 2-使用 CNN 进行图像分类 3-CNN 基本部件 常用激活函数 深度学习框架笔记 MXNet 框架学习 Pytorch 框架学习 Pytorch 基础-张量基本操作 Pytorch 基础-tensor 数据结构 Backbone 网络 1-DenseNet 网络 2-ResNetv2 网络 经典 backbone 网络总结 深度学习面试题 必问深度学习面试题 参考资料 《深度学习》 《机器学习》 《动手学深度学习》 《CNN 解析神经网络》 PyTorch官方教程中文版 eat_pytorch_in_20_days Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"5-deep_learning/backbone论文解读/DenseNet网络理解.html":{"url":"5-deep_learning/backbone论文解读/DenseNet网络理解.html","title":"DenseNet网络理解","keywords":"","body":"目录 目录 摘要 网络结构 优点 代码 问题 参考资料 摘要 ResNet 的工作表面，只要建立前面层和后面层之间的“短路连接”（shortcut），就能有助于训练过程中梯度的反向传播，从而能训练出更“深”的 CNN 网络。DenseNet 网络的基本思路和 ResNet 一致，但是它建立的是前面所有层与后面层的密集连接（dense connection）。传统的 $L$ 层卷积网络有 $L$ 个连接——每一层与它的前一层和后一层相连—，而 DenseNet 网络有 $L(L+1)/2$ 个连接。 在 DenseNet 中，让网络中的每一层都直接与其前面层相连，实现特征的重复利用；同时把网络的每一层设计得特别“窄”（特征图/滤波器数量少），即只学习非常少的特征图（最极端情况就是每一层只学习一个特征图），达到降低冗余性的目的。 网络结构 DenseNet 模型主要是由 DenseBlock 组成的。 用公式表示，传统直连（plain）的网络在 $l$ 层的输出为： $$\\mathrm{x}_l = H_l(\\mathrm{\\mathrm{x}}_l-1)$$ 对于残差块（residual block）结构，增加了一个恒等映射（shortcut 连接）： $$\\mathrm{x}l = H_l(\\mathrm{\\mathrm{x}}_l-1) + \\mathrm{x}{l-1}$$ 而在密集块（DenseBlock）结构中，每一层都会将前面所有层 concate 后作为输入： $$\\mathrm{x}l = H_l([\\mathrm{\\mathrm{x_0},\\mathrm{x_1},...,\\mathrm{x{l-1}}]})$$ $[\\mathrm{\\mathrm{x0},\\mathrm{x_1},...,\\mathrm{x{l-1}}]}$ 表示网络层 $0,...,l-1$ 输出特征图的拼接。这里暗示了，在 DenseBlock 中，每个网络层的特征图大小是一样的。$H_l(\\cdot)$ 是非线性转化函数（non-liear transformation），它由 BN(Batch Normalization)，ReLU 和 Conv 层组合而成。 DenseBlock 的结构图如下图所示。 在 DenseBlock 的设计中，作者重点提到了一个参数 $k$，被称为网络的增长率（growth of the network），其实是 DenseBlock 中任何一个 $3\\times 3$ 卷积层的滤波器个数（输出通道数）。如果每个 $H_l(\\cdot)$ 函数都输出 $k$ 个特征图，那么第 $l$ 层的输入特征图数量为 $k_0 + k\\times (l-1)$，$k_0$ 是 DenseBlock 的输入特征图数量（即第一个卷积层的输入通道数）。DenseNet 网络和其他网络最显著的区别是，$k$ 值可以变得很小，比如 $k=12$，即网络变得很“窄”，但又不影响精度。如表 4 所示。 为了在 DenseNet 网络中，保持 DenseBlock 的卷积层的 feature map 大小一致，作者在两个 DenseBlock 中间插入 transition 层。其由 $2\\times 2$ average pool, stride=2，和 $1\\times 1$ conv 层组合而成，具体为 BN + ReLU + 1x1 Conv + 2x2 AvgPooling。transition 层完成降低特征图大小和降维的作用。 CNN 网络一般通过 Pooling 层或者 stride>1 的卷积层来降低特征图大小（比如 stride=2 的 3x3 卷积层）， 下图给出了一个 DenseNet 的网路结构，它共包含 3 个（一半用 4 个）DenseBlock，各个 DenseBlock 之间通过 Transition 连接在一起。 和 ResNet 一样，DenseNet 也有 bottleneck 单元，来适应更深的 DenseNet。Bottleneck 单元是 BN-ReLU-Conv(1x1)-BN-ReLU-Conv(3x3)这样连接的结构，作者将具有 bottleneck 的密集单元组成的网络称为 DenseNet-B。 Bottleneck 译为瓶颈，一端大一端小，对应着 1x1 卷积通道数多，3x3 卷积通道数少。 对于 ImageNet 数据集，图片输入大小为 $224\\times 224$ ，网络结构采用包含 4 个 DenseBlock 的DenseNet-BC，网络第一层是 stride=2 的 $7\\times 7$卷积层，然后是一个 stride=2 的 $3\\times 3$ MaxPooling 层，而后是 DenseBlock。ImageNet 数据集所采用的网络配置参数表如表 1 所示： 网络中每个阶段卷积层的 feature map 数量都是 32。 优点 省参数 省计算 抗过拟合 注意，后续的 VoVNet 证明了，虽然 DenseNet 网络参数量少，但是其推理效率却不高。 在 ImageNet 分类数据集上达到同样的准确率，DenseNet 所需的参数量和计算量都不到 ResNet 的一半。对于工业界而言，小模型（参数量少）可以显著地节省带宽，降低存储开销。 参数量少的模型，计算量肯定也少。 作者通过实验发现，DenseNet 不容易过拟合，这在数据集不是很大的情况下表现尤为突出。在一些图像分割和物体检测的任务上，基于 DenseNet 的模型往往可以省略在 ImageNet 上的预训练，直接从随机初始化的模型开始训练，最终达到相同甚至更好的效果。 对于 DenseNet 抗过拟合的原因，作者给出的比较直观的解释是：神经网络每一层提取的特征都相当于对输入数据的一个非线性变换，而随着深度的增加，变换的复杂度也逐渐增加（更多非线性函数的复合）。相比于一般神经网络的分类器直接依赖于网络最后一层（复杂度最高）的特征，DenseNet 可以综合利用浅层复杂度低的特征，因而更容易得到一个光滑的具有更好泛化性能的决策函数。 DenseNet 的泛化性能优于其他网络是可以从理论上证明的：去年的一篇几乎与 DenseNet 同期发布在 arXiv 上的论文（AdaNet: Adaptive Structural Learning of Artificial Neural Networks）所证明的结论（见文中 Theorem 1）表明类似于 DenseNet 的网络结构具有更小的泛化误差界。 代码 作者开源的 DenseNet 提高内存效率版本的代码如下。 # This implementation is based on the DenseNet-BC implementation in torchvision # https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py import math import torch import torch.nn as nn import torch.nn.functional as F import torch.utils.checkpoint as cp from collections import OrderedDict def _bn_function_factory(norm, relu, conv): def bn_function(*inputs): concated_features = torch.cat(inputs, 1) bottleneck_output = conv(relu(norm(concated_features))) return bottleneck_output return bn_function class _DenseLayer(nn.Module): def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, efficient=False): super(_DenseLayer, self).__init__() self.add_module('norm1', nn.BatchNorm2d(num_input_features)), self.add_module('relu1', nn.ReLU(inplace=True)), self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)), self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)), self.add_module('relu2', nn.ReLU(inplace=True)), self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)), self.drop_rate = drop_rate self.efficient = efficient def forward(self, *prev_features): bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1) if self.efficient and any(prev_feature.requires_grad for prev_feature in prev_features): bottleneck_output = cp.checkpoint(bn_function, *prev_features) else: bottleneck_output = bn_function(*prev_features) new_features = self.conv2(self.relu2(self.norm2(bottleneck_output))) if self.drop_rate > 0: # 加入 dropout 增加模型泛化能力 new_features = F.dropout(new_features, p=self.drop_rate, training=self.training) return new_features class _Transition(nn.Sequential): def __init__(self, num_input_features, num_output_features): super(_Transition, self).__init__() self.add_module('norm', nn.BatchNorm2d(num_input_features)) self.add_module('relu', nn.ReLU(inplace=True)) self.add_module('conv', nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)) self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2)) class _DenseBlock(nn.Module): def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, efficient=False): super(_DenseBlock, self).__init__() for i in range(num_layers): layer = _DenseLayer( num_input_features + i * growth_rate, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate, efficient=efficient, ) self.add_module('denselayer%d' % (i + 1), layer) def forward(self, init_features): features = [init_features] for name, layer in self.named_children(): new_features = layer(*features) features.append(new_features) return torch.cat(features, 1) class DenseNet(nn.Module): r\"\"\"Densenet-BC model class, based on `\"Densely Connected Convolutional Networks\" ` Args: growth_rate (int) - how many filters to add each layer (`k` in paper) block_config (list of 3 or 4 ints) - how many layers in each pooling block num_init_features (int) - the number of filters to learn in the first convolution layer bn_size (int) - multiplicative factor for number of bottle neck layers (i.e. bn_size * k features in the bottleneck layer) drop_rate (float) - dropout rate after each dense layer num_classes (int) - number of classification classes small_inputs (bool) - set to True if images are 32x32. Otherwise assumes images are larger. efficient (bool) - set to True to use checkpointing. Much more memory efficient, but slower. \"\"\" def __init__(self, growth_rate=12, block_config=(16, 16, 16), compression=0.5, num_init_features=24, bn_size=4, drop_rate=0, num_classes=10, small_inputs=True, efficient=False): super(DenseNet, self).__init__() assert 0 问题 1，这么多的密集连接，是不是全部都是必要的，有没有可能去掉一些也不会影响网络的性能？ 作者回答：论文里面有一个热力图（heatmap），直观上刻画了各个连接的强度。从图中可以观察到网络中比较靠后的层确实也会用到非常浅层的特征。 注意，后续的改进版本 VoVNet 设计的 OSP 模块，去掉中间层的密集连接，只有最后一层聚合前面所有层的特征，并做了同一个实验。热力图的结果表明，去掉中间层的聚集密集连接后，最后一层的连接强度变得更好。同时，在 CIFAR-10 上和同 DenseNet 做了对比实验，OSP 的精度和 DenseBlock 相近，但是 MAC 减少了很多，这说明 DenseBlock 的这种密集连接会导致中间层的很多特征冗余的。 参考资料 CVPR 2017最佳论文作者解读：DenseNet 的“what”、“why”和“how”｜CVPR 2017 https://github.com/gpleiss/efficient_densenet_pytorch Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 04:23:59 "},"5-deep_learning/backbone论文解读/ResNetv2论文解读.html":{"url":"5-deep_learning/backbone论文解读/ResNetv2论文解读.html","title":"ResNetv2论文解读","keywords":"","body":"目录 [toc] 前言 本文的主要贡献在于通过理论分析和大量实验证明使用恒等映射（identity mapping）作为快捷连接（skip connection）对于残差块的重要性。同时，将 BN/ReLu 这些 activation 操作挪到了 Conv（真正的weights filter操作）之前，提出“预激活“操作，并通过与”后激活“操作做对比实验，表明对于多层网络，使用了预激活残差单元（Pre-activation residual unit） 的 resnet v2 都取得了比 resnet v1（或 resnet v1.5）更好的结果。 摘要 近期已经涌现出很多以深度残差网络（deep residual network）为基础的极深层的网络架构，在准确率和收敛性等方面的表现都非常引人注目。本文主要分析残差网络基本构件（residual building block）中的信号传播，本文发现当使用恒等映射（identity mapping）作为快捷连接（skip connection）并且将激活函数移至加法操作后面时，前向-反向信号都可以在两个 block 之间直接传播而不受到任何变换操作的影响。同时大量实验结果证明了恒等映射的重要性。本文根据这个发现重新设计了一种残差网络基本单元（unit），使得网络更易于训练并且泛化性能也得到提升。 注意这里的实验是深层 ResNet（$\\geq$ 110 layers） 的实验，所以我觉得，应该是对于深层 ResNet，使用”预激活”残差单元（Pre-activation residual unit）的网络（ResNet v2）更易于训练并且精度也更高。 1、介绍 深度残差网络（ResNets）由残差单元（Residual Units）堆叠而成。每个残差单元（图1 (a)）可以表示为： 其中，$xl$ 和 $x{l+1}$ 是 第 $l$ 个残差单元的输入和输出，$F$ 是残差函数。在 ResNet 中，$h(x{l})= x{l}$ 是恒等映射（identity），$f$ 是 ReLU 激活函数。在 ImageNet 数据集和 COCO 数据集上，超过 1000 层的残差网络都取得了当前最优的准确率。残差网络的核心思想是在 $h(x{l})$ 的基础上学习附加的残差函数 $F$，其中很关键的选择就是使用恒等映射 $h(x{l})= x_{l}$，这可以通过在网络中添加恒等快捷连接（skip connection) shortcut 来实现。 本文中主要在于分析在深度残差网络中构建一个信息“直接”传播的路径——不只是在残差单元直接，而是在整个网络中信息可以“直接”传播。如果 $h(x{l})$ 和 $f(y{l})$ 都是恒等映射，那么信号可以在单元间直接进行前向-反向传播。实验证明基本满足上述条件的网络架构一般更容易训练。本文实验了不同形式的 $h(x{l})$，发现使用恒等映射的网络性能最好，误差减小最快且训练损失最低。这些实验说明“干净”的信息通道有助于优化。各种不同形式的 $h(x{l})$ 见论文中的图 1、图2 和 图4 中的灰色箭头所示。 为了构建 $f(y_l)=y_l$ 的恒等映射，本文将激活函数（ReLU 和 BN）移到权值层（Conv）之前，形成一种“预激活（pre-activation）”的方式，而不是常规的“后激活（post-activation）”方式，这样就设计出了一种新的残差单元（见图 1(b)）。基于这种新的单元我们在 CIFAR-10/100 数据集上使用1001 层残差网络进行训练，发现新的残差网络比之前（ResNet）的更容易训练并且泛化性能更好。同时还考察了 200 层新残差网络在 ImageNet 上的表现，原先的残差网络在这个层数之后开始出现过拟合的现象。这些结果表明网络深度这个维度还有很大探索空间，毕竟深度是现代神经网络成功的关键。 2、深度残差网络的分析 原先 ResNets 的残差单元的可以表示为： 在 ResNet 中，函数 $h$ 是恒等映射，即 $h(x{l}) = x{l}$。公式的参数解释见下图： 如果函数 $f$ 也是恒等映射，即 $y{l}\\equiv y{l}$，公式 (1)(2) 可以合并为： 那么任意深层的单元 $L$ 与浅层单元 $l$之间的关系为： 公式 (4) 有两个特性： 深层单元的特征可以由浅层单元的特征和残差函数相加得到； 任意深层单元的特征都可以由起始特征 $x_0$ 与先前所有残差函数相加得到，这与普通（plain）网络不同，普通网络的深层特征是由一系列的矩阵向量相乘得到。残差网络是连加，普通网络是连乘。 公式 (4) 也带来了良好的反向传播特性，用 $\\varepsilon $ 表示损失函数，根据反向传播的链式传导规则，反向传播公式如下： 从公式 (5) 中可以看出，反向传播也是两条路径，其中之一直接将信息回传，另一条会经过所有的带权重层。另外可以注意到第二项的值在一个 mini-batch 中不可能一直是 -1，也就是说回传的梯度不会消失，不论网络中的权值的值再小都不会发生梯度消失现象。 3、On the Importance of Identity Skip Connection 考虑恒等映射的重要性。假设将恒等映射改为 $h(x{l}) = \\lambda{l}x_{l})$，则： 像公式 (4) 一样递归的调用公式 (3)，得： 其中，$\\hat{F}$ 表示将标量合并到残差函数中，与公式 (5) 类似，反向传播公式如下： 与公式 (5) 不同，公式 (8) 的第一个加法项由因子 $\\prod{i=l}^{L-1}\\lambda{i}$ 进行调节。对于一个极深的网络($L$ 极大)，考虑第一个连乘的项，如果所有的 $\\lambda$ 都大于 1，那么这一项会指数级增大；如果所有 $\\lambda$ 都小于 1，那么这一项会很小甚至消失，会阻断来自 shortcut 的反向传播信号，并迫使其流过权重层。本文通过实验证明这种方式会对模型优化造成困难。 另外其他不同形式的变换映射也都会阻碍信号的有效传播，进而影响训练进程。 4、On the Usage of Activation Functions 第 3 章考察使用不同形式映射（见图 2）来验证函数 $h$ 是恒等映射的重要性，这章讨论公式(2)中的 $f$，如果 $f$ 也是恒等映射，网络的性能会不会有所提升。通过调节激活函数 (ReLU and/or BN) 的位置，来使 $f$ 是恒等映射。图 4 展示了激活函数在不同位置的残差单元结构图去。 图 4(e) 的”预激活“操作是本文提出的一种对于深层残差网络能够更有效训练的网络结构（ResNet v2）。 4.1、Experiments on Activation 本章，我们使用 ResNet-110 和 164 层瓶颈结构(称为 ResNet-164)来进行实验。瓶颈残差单元包含一个 $1\\times 1$ 的层来降维，一个 $3\\times 3$ 的层，还有一个 $1\\times 1$ 的层来恢复维度。如 ResNet 论文中描述的那样，它的计算复杂度和包含两个 $3\\times 3$ 卷积层的残差单元相似。 BN after addition 效果比基准差，BN 层移到相加操作后面会阻碍信号传播，一个明显的现象就是训练初期误差下降缓慢。 ReLU before addition 这样组合的话残差函数分支的输出就一直保持非负，这会影响到模型的表示能力，而实验结果也表明这种组合比基准差。 Post-activation or pre-activation 原来的设计中相加操作后面还有一个 ReLU 激活函数，这个激活函数会影响到残差单元的两个分支，现在将它移到残差函数分支上，快捷连接分支不再受到影响。具体操作如图 5 所示。 根据激活函数与相加操作的位置关系，我们称之前的组合方式为“后激活（post-activation）”，现在新的组合方式称之为“预激活（pre-activation）”。原来的设计与预激活残差单元之间的性能对比见表 3。预激活方式又可以分为两种：只将 ReLU 放在前面，或者将 ReLU 和 BN都放到前面，根据表 2 中的结果可以看出 full pre-activation 的效果要更好。 4.2、Analysis 使用预激活有两个方面的优点：1) $f$ 变为恒等映射，使得网络更易于优化；2)使用 BN 作为预激活可以加强对模型的正则化。 Ease of optimization 这在训练 1001 层残差网络时尤为明显，具体见图 1。使用原来设计的网络在起始阶段误差下降很慢，因为 $f$ 是 ReLU 激活函数，当信号为负时会被截断，使模型无法很好地逼近期望函数；而使用预激活网络中的 $f$ 是恒等映射，信号可以在不同单元直接直接传播。本文使用的 1001层网络优化速度很快，并且得到了最低的误差。 $f$ 为 ReLU 对浅层残差网络的影响并不大，如图 6-right 所示。本文认为是当网络经过一段时间的训练之后权值经过适当的调整，使得单元输出基本都是非负，此时 $f$ 不再对信号进行截断。但是截断现象在超过 1000层的网络中经常发生。 Reducing overfitting 观察图 6-right，使用了预激活的网络的训练误差稍高，但却得到更低的测试误差，本文推测这是 BN 层的正则化效果所致。在原始残差单元中，尽管BN 对信号进行了标准化，但是它很快就被合并到捷径连接(shortcut)上，组合的信号并不是被标准化的。这个非标准化的信号又被用作下一个权重层的输入。与之相反，本文的预激活（pre-activation）版本的模型中，权重层的输入总是标准化的。 5、Results 表 4、表 5 分别展示了不同深层网络在不同数据集上的表现。使用的预激活单元的且更深层的残差网络（ResNet v2）都取得了最好的精度。 6、结论 恒等映射形式的快捷连接和预激活对于信号在网络中的顺畅传播至关重要。 参考资料 [DL-架构-ResNet系] 002 ResNet-v2 Identity Mappings in Deep Residual Networks（译） Identity Mappings in Deep Residual Networks Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 04:23:59 "},"5-deep_learning/backbone论文解读/经典backbone总结.html":{"url":"5-deep_learning/backbone论文解读/经典backbone总结.html","title":"经典backbone总结","keywords":"","body":"目录 VGG ResNet Inceptionv3 Resnetv2 ResNeXt Darknet53 DenseNet CSPNet VoVNet 一些结论 参考资料 VGG VGG网络结构参数表如下图所示。 ResNet ResNet 模型比 VGG 网络具有更少的滤波器数量和更低的复杂性。 比如 Resnet34 的 FLOPs 为 3.6G，仅为 VGG-19 19.6G 的 18%。 注意，论文中算的 FLOPs，把乘加当作 1 次计算。 ResNet 和 VGG 的网络结构连接对比图，如下图所示。 不同层数的 Resnet 网络参数表如下图所示。 看了后续的 ResNeXt、ResNetv2、Densenet、CSPNet、VOVNet 等论文，越发觉得 ResNet 真的算是 Backone 领域划时代的工作了，因为它让深层神经网络可以训练，基本解决了深层神经网络训练过程中的梯度消失问题，并给出了系统性的解决方案（两种残差结构），即系统性的让网络变得更“深”了。而让网络变得更“宽”的工作，至今也没有一个公认的最佳方案（Inception、ResNeXt 等后续没有广泛应用），难道是因为网络变得“宽”不如“深”更重要，亦或是我们还没有找到一个更有效的方案。 Inceptionv3 常见的一种 Inception Modules 结构如下： Resnetv2 作者总结出恒等映射形式的快捷连接和预激活对于信号在网络中的顺畅传播至关重要的结论。 ResNeXt ResNeXt 的卷积block 和 Resnet 对比图如下所示。 ResNeXt 和 Resnet 的模型结构参数对比图如下图所示。 Darknet53 Darknet53 模型结构连接图，如下图所示。 DenseNet 作者 Gao Huang 于 2018 年发表的论文 Densely Connected Convolutional Networks。 在密集块（DenseBlock）结构中，每一层都会将前面所有层 concate 后作为输入。DenseBlock（类似于残差块的密集块结构）结构的 3 画法图如下所示： 可以看出 DenseNet 论文更侧重的是 DenseBlock 内各个卷积层之间的密集连接（dense connection）关系，另外两个则是强调每层的输入是前面所有层 feature map 的叠加，反映了 feature map 数量的变化。 CSPNet CSPDenseNet 的一个阶段是由局部密集块和局部过渡层组成（a partial dense block and a partial transition layer）。 CSP 方法可以减少模型计算量和提高运行速度的同时，还不降低模型的精度，是一种更高效的网络设计方法，同时还能和 Resnet、Densenet、Darknet 等 backbone 结合在一起。 VoVNet One-Shot Aggregation（只聚集一次）是指 OSA 模块的 concat 操作只进行一次，即只有最后一层($1\\times 1$ 卷积)的输入是前面所有层 feature map 的 concat（叠加）。OSA 模块的结构图如图 1(b) 所示。 在 OSA module 中，每一层产生两种连接，一种是通过 conv 和下一层连接，产生 receptive field 更大的 feature map，另一种是和最后的输出层相连，以聚合足够好的特征。通过使用 OSA module，5 层 43 channels 的 DenseNet-40 的 MAC 可以被减少 30%（3.7M -> 2.5M）。 基于 OSA 模块构建的各种 VoVNet 结构参数表如下。 作者认为 DenseNet 用更少的参数与 Flops 而性能却比 ResNet 更好，主要是因为concat 比 add 能保留更多的信息。但是，实际上 DenseNet 却比 ResNet要慢且消耗更多资源。 GPU 的计算效率： GPU 特性是擅长 parallel computation，tensor 越大，GPU 使用效率越高。 把大的卷积操作拆分成碎片的小操作将不利于 GPU 计算。 设计 layer 数量少的网络是更好的选择。 1x1 卷积可以减少计算量，但不利于 GPU 计算。 在 CenterMask 论文提出了 VoVNetv2，其卷积模块结构图如下： 一些结论 当卷积层的输入输出通道数相等时，内存访问代价（MAC）最小。 影响 CNN 功耗的主要因素在于内存访问代价 MAC，而不是计算量 FLOPs。 GPU 擅长并行计算，Tensor 越大，GPU 使用效率越高，把大的卷积操作拆分成碎片的小操作不利于 GPU 计算。 1x1 卷积可以减少计算量，但不利于 GPU 计算。 参考资料 VGG/ResNet/Inception/ResNeXt/CSPNet 论文 深度学习论文: An Energy and GPU-Computation Efficient Backbone Network for Object Detection及其PyTorch Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 04:23:59 "},"5-deep_learning/ml-dl-框架笔记/MXNet学习笔记.html":{"url":"5-deep_learning/ml-dl-框架笔记/MXNet学习笔记.html","title":"MXNet学习笔记","keywords":"","body":" 制作MXNet数据集 数据操作-NDArray 创建NDArray NDArray实例运算 NDArray其他操作 autograd模块 简单使用 训练模式和预测模式 MXNet用于构建CNN的模块-Symbol、gluon Symbol模块学习 基本神经网络构建 深度网络模块化构建 mxnet.sym.Group mxnet.io.DataIter mxnet.io.MXDataIter mxnet.image.ImageIter MXNet读取图像数据总结 制作MXNet数据集 与tf不同，MXNet也有自己的专属图像数据格式，MXNet读取图像有两种方式： 读.rec格式文件，包含文件路径、标签和图像信息 读.lst和图像结合方式，.lst文件其实就是图像路径和标签的对应列表，有点类似.csv文件 数据操作-NDArray NDArray 功能类似 numpy 库的多维数组操作，NDArray 提供了 GPU 计算和自动求梯度等更多应用于深度学习的功能，用法如下： from mcnet import nd # 导入NDArray(ndarray, nd)模块 x = arange(10) # arange函数创建一个行向量 创建NDArray 这里的操作和numpy类似，创建零元素张量、1元素张量、改变张量形状等 x = nd.arrange(12) x.size # 12 x.reshape((3,4)) x.zeros((3, 4, 5)) # 创建元素为0，形状为(3, 4, 5)的张量 x.ones((3, 4, 5)) NDArray实例运算 和numpy类似，矩阵乘法np.dot(X, Y.T)，矩阵连结操作（concatenate）。 下面分别在行上（维度0，即形状中的最左边元素）和列上（维度1，即形状中左起第二个元素）连结两个矩阵。可以看到，输出的第一个NDArray在维度0的长度（ 6 ）为两个输入矩阵在维度0的长度之和（ 3+3 ），而输出的第二个NDArray在维度1的长度（ 8 ）为两个输入矩阵在维度1的长度之和（ 4+4 ） X = nd.arrange(12).reshape(3,4) Y = nd.arange(12).reshape(3,4) Z1 = nd.concat(X, Y, dim=0) # Z1.shape (6,4) Z2 = nd.concat(X, Y, dim=1) # Z1.shape (3,8) NDArray其他操作 常用的还有广播机制、索引、运算的内存开销和NDArray和Numpy相互互换。NDArray实例和NumPy实例互换如下： import numpy as np from mxnet import nd # 将NumPy实例变换成NDArray实例 P = np.ones((3,4)) D = nd.array(P) # 将NDArray实例变换成NumPy实例 D.asnumpy autograd模块 autograd模块实现对函数求梯度(gradient)。 from mxnet import autograd, nd 简单使用 主要是两个函数： attach_grad()：申请存储梯度所需内存 record()：求MXNet记录与求梯度有关的计算 backward()：自动求梯度 训练模式和预测模式 调用record函数后，MXNet会记录并计算梯度。此外，默认情况下autograd还会将运行模式从预测模式转为训练模式。这可以通过调用is_training函数来查看。 MXNet用于构建CNN的模块-Symbol、gluon 涉及使用MXNet框架完成模型构造、参数的访问和初始化、自定义层构建、模型读取、加载和使用GPU等。 Symbol模块学习 MXNet 提供了符号接口，用于符号编程的交互。它和一步步的解释的命令式编程不同，我们首先要定义一个计算图。这个图包括了输入的占位符和设计的输出。之后编译这个图，产生一个可以绑定到NDArray s并运行的函数。Symbol API类似于caffe中的网络配置或者Theano中的符号编程。 symbol 构建神经网络有点类似于 tensorflow 的静态图 api。 基本神经网络构建 除了基本操作符，symbol 模块提供了丰富的神经网络api，以下示例构建了一个两层的全连接层，通过给定输入数据大小实例化该结构： import mxnet as mx net = mx.sym.Variable('data') net = mx.sym.FullyConnected(data=net, num_hidden=128, name='fc1') net = mx.sym.relu(net, name='relu1', act_type='relu') net = mx.sym.FullyConnected(data=net, name='fc2', num_hidden=2) net = mx.sym.SoftmaxOutput(data=net, name='out') mx.viz.plot_network(net, shape={'data': (100, 200)}) print(net.list_arguments()) # 遍历参数 输出如下： ['data', 'fc1_weight', 'fc1_bias', 'fc2_weight', 'fc2_bias', 'out_label'] 注意：FullyConnected 层有三个输入：数据，权值，偏置。任何，任何没有指定输入的将自动生成一个变量。这里一般权值w，偏置b参数不用指定。 深度网络模块化构建 对于一些深度的CNN网络，可用模块化的方式将其构建，下面一个示例将卷积层、批标准化层和relu激活层捆绑在一起形成一个新的函数。 def Conv(data, num_filter, kernel=(1, 1), stride=(1, 1), pad=(0, 0), name=None, suffix=''): conv = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad, no_bias=True, name='%s%s_conv2d' %(name, suffix)) bn = mx.sym.BatchNorm(data=conv, name='%s%s_batchnorm' %(name, suffix), fix_gamma=False) act = mx.sym.Activation(data=bn, act_type='relu', name='%s%s_relu' %(name, suffix)) return act 注意这里的mx.sym.Convolution函数需要自己指定data输入张量、num_filter滤波器（卷积核）数量、kernel卷积核尺寸、stride移动步长、pad填充shape。 对于2-D convolution, 各参数对应shape是： data: (batch_size, channel, height, width) weight: (num_filter, channel, kernel[0], kernel[1]) bias: (num_filter,) out: (batch_size, num_filter, out_height, out_width).mxnet.sym.Group 组合两个输出层： net = mx.sym.Variable('data') fc1 = mx.sym.FullyConnected(data=net, name='fc1', num_hidden=128) net = mx.sym.Activation(data=fc1, name='relu1', act_type=\"relu\") out1 = mx.sym.SoftmaxOutput(data=net, name='softmax') out2 = mx.sym.LinearRegressionOutput(data=net, name='regression') group = mx.sym.Group([out1, out2]) print(group.list_outputs()) 输出如下： ['softmax_output', 'regression_output'] MXNet数据读取类：基础类mxnet.io.DataIter、高级类mxnet.io.MXDataIter、高级类mxnet.image.ImageIter mxnet.io.DataIter mxnet.io.DataIter是MXNet 框架中构造数据迭代器的基础类，在MXNet框架下只要和数据读取相关的接口基本上都继承该类，比如我们常用的图像算法相关的 mxnet.io.ImageRecordIter 类或 mxnet.image.ImageIter 类都直接或间接继承 mxnet.io.DataIter 类进行封装。 mxnet.io.MXDataIter 初始化一个mxnet.io.ImageRecordIter类时会得到一个MXDataIter实例，然后当你调用该实例的时候就会调用MXDataIter类的底层C++数据迭代器读取数据（后面会介绍是通过next方法实现的）。 mxnet.image.ImageIter Imageter类是纯python实现，继承自DataIter类，与mxnet.io.imageRecordIter类不同，该接口是Python代码实现的图像数据迭代器，既可以读取.rec文件，也可以以图像+.lst方式来读取数据。 由于mxnet.image.ImageIter接口在以原图像+.lst文件形式读取数据时是基于python代码实现的，因此在速度上会比基于C++代码实现的mxnet.io.ImageRecordIter接口效率低，尤其是当数据是存储在机械硬盘上时。 MXNet读取图像数据总结 MXNet的图像数据导入模块主要有mxnet.io.ImageRecordIter和mxnet.image.ImageIter两个类，前者主要用来读取.rec格式的数据，后者既可以读.rec格式文件，也可以读原图像数据。 注意：在MXNet框架中，数据存储为NDArray格式，图像数据也是如此，因此mxnet.image中的很多函数的输入输出都是NDArray格式 当我们使用mxnet.io.ImageRecordIter这个类读取图像时，必须先用im2rec.py生成lst和rec文件，然后采用mxnet.io.ImageRecordIter类读取rec文件。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"5-deep_learning/ml-dl-框架笔记/Pytorch基础-tensor数据结构.html":{"url":"5-deep_learning/ml-dl-框架笔记/Pytorch基础-tensor数据结构.html","title":"Pytorch基础-tensor数据结构","keywords":"","body":" torch.Tensor Tensor 数据类型 Tensor 的属性 view 和 reshape 的区别 Tensor 与 ndarray 创建 Tensor 传入维度的方法 参考资料 torch.Tensor torch.Tensor 是一种包含单一数据类型元素的多维矩阵，类似于 numpy 的 array。 可以使用使用 torch.tensor() 方法将 python 的 list 或序列数据转换成 Tensor 数据，生成的是dtype 默认是 torch.FloatTensor。 注意 torch.tensor() 总是拷贝 data。如果你有一个 tensor data 并且仅仅想改变它的 requires_grad 属性，可用 requires_grad_() 或者 detach() 来避免拷贝。如果你有一个 numpy 数组并且想避免拷贝，请使用 torch.as_tensor()。 1，指定数据类型的 tensor 可以通过传递参数 torch.dtype 和/或者 torch.device 到构造函数生成： 注意为了改变已有的 tensor 的 torch.device 和/或者 torch.dtype, 考虑使用 to() 方法. >>> torch.ones([2,3], dtype=torch.float64, device=\"cuda:0\") tensor([[1., 1., 1.], [1., 1., 1.]], device='cuda:0', dtype=torch.float64) >>> torch.ones([2,3], dtype=torch.float32) tensor([[1., 1., 1.], [1., 1., 1.]]) 2，Tensor 的内容可以通过 Python 索引或者切片访问以及修改： >>> matrix = torch.tensor([[2,3,4],[5,6,7]]) >>> print(matrix[1][2]) tensor(7) >>> matrix[1][2] = 9 >>> print(matrix) tensor([[2, 3, 4], [5, 6, 9]]) 3，使用 torch.Tensor.item() 或者 int() 方法从只有一个值的 Tensor中获取 Python Number： >>> x = torch.tensor([[4.5]]) >>> x tensor([[4.5000]]) >>> x.item() 4.5 >>> int(x) 4 4，Tensor可以通过参数 requires_grad=True 创建, 这样 torch.autograd 会记录相关的运算实现自动求导： >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True) >>> out = x.pow(2).sum() >>> out.backward() >>> x.grad tensor([[ 2.0000, -2.0000], [ 2.0000, 2.0000]]) 5，每一个 tensor都有一个相应的 torch.Storage 保存其数据。tensor 类提供了一个多维的、strided 视图, 并定义了数值操作。 Tensor 数据类型 Torch 定义了七种 CPU Tensor 类型和八种 GPU Tensor 类型： torch.Tensor 是默认的 tensor 类型（torch.FloatTensor）的简称，即 32 位浮点数数据类型。 Tensor 的属性 Tensor 有很多属性，包括数据类型、Tensor 的维度、Tensor 的尺寸。 数据类型：可通过改变 torch.tensor() 方法的 dtype 参数值，来设定不同的 Tensor 数据类型。 维度：不同类型的数据可以用不同维度(dimension)的张量来表示。标量为 0 维张量，向量为 1 维张量，矩阵为 2 维张量。彩色图像有 rgb 三个通道，可以表示为 3 维张量。视频还有时间维，可以表示为 4 维张量，有几个中括号 [ 维度就是几。可使用 dim() 方法 获取 tensor 的维度。 尺寸：可以使用 shape属性或者 size()方法查看张量在每一维的长度，可以使用 view()方法或者reshape() 方法改变张量的尺寸。Pytorch 框架中四维张量形状的定义是 (N, C, H, W)。 关于如何理解 Pytorch 的 Tensor Shape 可以参考 stackoverflow 上的这个 回答。 样例代码如下： matrix = torch.tensor([[[1,2,3,4],[5,6,7,8]], [[5,4,6,7], [5,6,8,9]]], dtype = torch.float64) print(matrix) # 打印 tensor print(matrix.dtype) # 打印 tensor 数据类型 print(matrix.dim()) # 打印 tensor 维度 print(matrix.size()) # 打印 tensor 尺寸 print(matrix.shape) # 打印 tensor 尺寸 matrix2 = matrix.view(4, 2, 2) # 改变 tensor 尺寸 print(matrix2) 程序输出结果如下： view 和 reshape 的区别 两个方法都是用来改变 tensor 的 shape，view() 只适合对满足连续性条件（contiguous）的 tensor 进行操作，而 reshape() 同时还可以对不满足连续性条件的 tensor 进行操作。 在满足 tensor 连续性条件（contiguous）时，a.reshape() 返回的结果与a.view() 相同，都不会开辟新内存空间；不满足 contiguous 时， 直接使用 view() 方法会失败，reshape() 依然有用，但是会重新开辟内存空间，不与之前的 tensor 共享内存，即返回的是 ”副本“（等价于先调用 contiguous() 方法再使用 view() 方法）。 更多理解参考这篇文章 Tensor 与 ndarray 1，张量和 numpy 数组。可以用 .numpy() 方法从 Tensor 得到 numpy 数组，也可以用 torch.from_numpy 从 numpy 数组得到Tensor。这两种方法关联的 Tensor 和 numpy 数组是共享数据内存的。可以用张量的 clone方法拷贝张量，中断这种关联。 arr = np.random.rand(4,5) print(type(arr)) tensor1 = torch.from_numpy(arr) print(type(tensor1)) arr1 = tensor1.numpy() print(type(arr1)) \"\"\" \"\"\" 2，item() 方法和 tolist() 方法可以将张量转换成 Python 数值和数值列表 # item方法和tolist方法可以将张量转换成Python数值和数值列表 scalar = torch.tensor(5) # 标量 s = scalar.item() print(s) print(type(s)) tensor = torch.rand(3,2) # 矩阵 t = tensor.tolist() print(t) print(type(t)) \"\"\" 1.0 [[0.8211846351623535, 0.20020723342895508], [0.011571824550628662, 0.2906131148338318]] \"\"\" 创建 Tensor 创建 tensor ，可以传入数据或者维度，torch.tensor() 方法只能传入数据，torch.Tensor() 方法既可以传入数据也可以传维度，强烈建议 tensor() 传数据，Tensor() 传维度，否则易搞混。 传入维度的方法 方法名 方法功能 备注 torch.rand(*sizes, out=None) → Tensor 返回一个张量，包含了从区间 [0, 1) 的均匀分布中抽取的一组随机数。张量的形状由参数sizes定义。 推荐 torch.randn(*sizes, out=None) → Tensor 返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义。 不推荐 torch.normal(means, std, out=None) → Tensor 返回一个张量，包含了从指定均值 means 和标准差 std 的离散正态分布中抽取的一组随机数。标准差 std 是一个张量，包含每个输出元素相关的正态分布标准差。 多种形式，建议看源码 torch.rand_like(a) 根据数据 a 的 shape 来生成随机数据 不常用 torch.randint(low=0, high, size) 生成指定范围(low, hight)和 size 的随机整数数据 常用 torch.full([2, 2], 4) 生成给定维度，全部数据相等的数据 不常用 torch.arange(start=0, end, step=1, *, out=None) 生成指定间隔的数据 易用常用 torch.ones(*size, *, out=None) 生成给定 size 且值全为1 的矩阵数据 简单 zeros()/zeros_like()/eye() 全 0 的 tensor 和 对角矩阵 简单 样例代码： >>> torch.rand([1,1,3,3]) tensor([[[[0.3005, 0.6891, 0.4628], [0.4808, 0.8968, 0.5237], [0.4417, 0.2479, 0.0175]]]]) >>> torch.normal(2, 3, size=(1, 4)) tensor([[3.6851, 3.2853, 1.8538, 3.5181]]) >>> torch.full([2, 2], 4) tensor([[4, 4], [4, 4]]) >>> torch.arange(0,10,2) tensor([0, 2, 4, 6, 8]) >>> torch.eye(3,3) tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) 参考资料 PyTorch：view() 与 reshape() 区别详解 torch.rand和torch.randn和torch.normal和linespace() Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"5-deep_learning/ml-dl-框架笔记/Pytorch基础-张量基本操作.html":{"url":"5-deep_learning/ml-dl-框架笔记/Pytorch基础-张量基本操作.html","title":"Pytorch基础-张量基本操作","keywords":"","body":" 一，张量的基本操作 二，维度变换 2.1，squeeze vs unsqueeze 维度增减 2.2，transpose vs permute 维度交换 三，索引切片 3.1，规则索引切片方式 3.2，gather 和 torch.index_select 算子 四，合并分割 4.1，torch.cat 和 torch.stack 4.2，torch.split 和 torch.chunk 五，卷积相关算子 5.1，上采样方法总结 5.2，F.interpolate 采样函数 5.3，nn.ConvTranspose2d 反卷积 参考资料 授人以鱼不如授人以渔，原汁原味的知识才更富有精华，本文只是对张量基本操作知识的理解和学习笔记，看完之后，想要更深入理解，建议去 pytorch 官方网站，查阅相关函数和操作，英文版在这里，中文版在这里。本文的代码是在 pytorch1.7 版本上测试的，其他版本一般也没问题。 一，张量的基本操作 Pytorch 中，张量的操作分为结构操作和数学运算，其理解就如字面意思。结构操作就是改变张量本身的结构，数学运算就是对张量的元素值完成数学运算。 常使用的张量结构操作：维度变换（tranpose、view 等）、合并分割（split、chunk等）、索引切片（index_select、gather 等）。 常使用的张量数学运算：标量运算、向量运算、矩阵运算。二，维度变换 2.1，squeeze vs unsqueeze 维度增减 squeeze()：对 tensor 进行维度的压缩，去掉维数为 1 的维度。用法：torch.squeeze(a) 将 a 中所有为 1 的维度都删除，或者 a.squeeze(1) 是去掉 a中指定的维数为 1 的维度。 unsqueeze()：对数据维度进行扩充，给指定位置加上维数为 1 的维度。用法：torch.unsqueeze(a, N)，或者 a.unsqueeze(N)，在 a 中指定位置 N 加上一个维数为 1 的维度。 squeeze 用例程序如下： a = torch.rand(1,1,3,3) b = torch.squeeze(a) c = a.squeeze(1) print(b.shape) print(c.shape) 程序输出结果如下： torch.Size([3, 3]) torch.Size([1, 3, 3]) unsqueeze 用例程序如下： x = torch.rand(3,3) y1 = torch.unsqueeze(x, 0) y2 = x.unsqueeze(0) print(y1.shape) print(y2.shape) 程序输出结果如下： torch.Size([1, 3, 3]) torch.Size([1, 3, 3]) 2.2，transpose vs permute 维度交换 torch.transpose() 只能交换两个维度，而 .permute() 可以自由交换任意位置。函数定义如下： transpose(dim0, dim1) → Tensor # See torch.transpose() permute(*dims) → Tensor # dim(int). Returns a view of the original tensor with its dimensions permuted. 在 CNN 模型中，我们经常遇到交换维度的问题，举例：四个维度表示的 tensor：[batch, channel, h, w]（nchw），如果想把 channel 放到最后去，形成[batch, h, w, channel]（nhwc），如果使用 torch.transpose() 方法，至少要交换两次（先 1 3 交换再 1 2 交换），而使用 .permute() 方法只需一次操作，更加方便。例子程序如下： import torch input = torch.rand(1,3,28,32) # torch.Size([1, 3, 28, 32] print(b.transpose(1, 3).shape) # torch.Size([1, 32, 28, 3]) print(b.transpose(1, 3).transpose(1, 2).shape) # torch.Size([1, 28, 32, 3]) print(b.permute(0,2,3,1).shape) # torch.Size([1, 28, 28, 3] 三，索引切片 3.1，规则索引切片方式 张量的索引切片方式和 numpy、python 多维列表几乎一致，都可以通过索引和切片对部分元素进行修改。切片时支持缺省参数和省略号。实例代码如下： >>> t = torch.randint(1,10,[3,3]) >>> t tensor([[8, 2, 9], [2, 5, 9], [3, 9, 9]]) >>> t[0] # 第 1 行数据 tensor([8, 2, 9]) >>> t[2][2] tensor(9) >>> t[0:3,:] # 第1至第3行，全部列 tensor([[8, 2, 9], [2, 5, 9], [3, 9, 9]]) >>> t[0:2,:] # 第1行至第2行 tensor([[8, 2, 9], [2, 5, 9]]) >>> t[1:,-1] # 第2行至最后行，最后一列 tensor([9, 9]) >>> t[1:,::2] # 第1行至最后行，第0列到最后一列每隔两列取一列 tensor([[2, 9], [3, 9]]) 以上切片方式相对规则，对于不规则的切片提取,可以使用 torch.index_select, torch.take, torch.gather, torch.masked_select。 3.2，gather 和 torch.index_select 算子 gather 算子的用法比较难以理解，在翻阅了官方文档和网上资料后，我有了一些自己的理解。 1，gather 是不规则的切片提取算子（Gathers values along an axis specified by dim. 在指定维度上根据索引 index 来选取数据）。函数定义如下： torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor 参数解释： input (Tensor) – the source tensor. dim (int) – the axis along which to index. index (LongTensor) – the indices of elements to gather. gather 算子的注意事项： 输入 input 和索引 index 具有相同数量的维度，即 input.shape = index.shape 对于任意维数，只要 d != dim，index.size(d) d 上的全部数据。 输出 out 和 索引 index 具有相同的形状。输入和索引不会相互广播。 对于 3D tensor，output 值的定义如下： gather 的官方定义如下： out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 通过理解前面的一些定义，相信读者对 gather 算子的用法有了一个基本了解，下面再结合 2D 和 3D tensor 的用例来直观理解算子用法。 （1），对于 2D tensor 的例子： >>> import torch >>> a = torch.arange(0, 16).view(4,4) >>> a tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) >>> index = torch.tensor([[0, 1, 2, 3]]) # 选取对角线元素 >>> torch.gather(a, 0, index) tensor([[ 0, 5, 10, 15]]) output 值定义如下： # 按照 index = tensor([[0, 1, 2, 3]])顺序作用在行上索引依次为0,1,2,3 a[0][0] = 0 a[1][1] = 5 a[2][2] = 10 a[3][3] = 15 （2），索引更复杂的 2D tensor 例子： >>> t = torch.tensor([[1, 2], [3, 4]]) >>> t tensor([[1, 2], [3, 4]]) >>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]])) tensor([[ 1, 1], [ 4, 3]]) output 值的计算如下： output[i][j] = input[i][index[i][j]] # if dim = 1 output[0][0] = input[0][index[0][0]] = input[0][0] = 1 output[0][1] = input[0][index[0][1]] = input[0][0] = 1 output[1][0] = input[1][index[1][0]] = input[1][1] = 4 output[1][1] = input[1][index[1][1]] = input[1][0] = 3 总结：可以看到 gather 是通过将索引在指定维度 dim 上的值替换为 index 的值，但是其他维度索引不变的情况下获取 tensor 数据。直观上可以理解为对矩阵进行重排，比如对每一行(dim=1)的元素进行变换，比如 torch.gather(a, 1, torch.tensor([[1,2,0], [1,2,0]])) 的作用就是对 矩阵 a 每一行的元素，进行 permtute(1,2,0) 操作。 2，理解了 gather 再看 index_select 就很简单，函数作用是返回沿着输入张量的指定维度的指定索引号进行索引的张量子集。函数定义如下： torch.index_select(input, dim, index, *, out=None) → Tensor 函数返回一个新的张量，它使用数据类型为 LongTensor 的 index 中的条目沿维度 dim 索引输入张量。返回的张量具有与原始张量（输入）相同的维数。 维度尺寸与索引长度相同； 其他尺寸与原始张量中的尺寸相同。实例代码如下： >>> x = torch.randn(3, 4) >>> x tensor([[ 0.1427, 0.0231, -0.5414, -1.0009], [-0.4664, 0.2647, -0.1228, -1.1068], [-1.1734, -0.6571, 0.7230, -0.6004]]) >>> indices = torch.tensor([0, 2]) >>> torch.index_select(x, 0, indices) tensor([[ 0.1427, 0.0231, -0.5414, -1.0009], [-1.1734, -0.6571, 0.7230, -0.6004]]) >>> torch.index_select(x, 1, indices) tensor([[ 0.1427, -0.5414], [-0.4664, -0.1228], [-1.1734, 0.7230]]) 四，合并分割 4.1，torch.cat 和 torch.stack 可以用 torch.cat 方法和 torch.stack 方法将多个张量合并，也可以用 torch.split方法把一个张量分割成多个张量。torch.cat 和 torch.stack 有略微的区别，torch.cat 是连接，不会增加维度，而 torch.stack 是堆叠，会增加一个维度。两者函数定义如下： # Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty. torch.cat(tensors, dim=0, *, out=None) → Tensor # Concatenates a sequence of tensors along **a new** dimension. All tensors need to be of the same size. torch.stack(tensors, dim=0, *, out=None) → Tensor torch.cat 和 torch.stack 用法实例代码如下： >>> a = torch.arange(0,9).view(3,3) >>> b = torch.arange(10,19).view(3,3) >>> c = torch.arange(20,29).view(3,3) >>> cat_abc = torch.cat([a,b,c], dim=0) >>> print(cat_abc.shape) torch.Size([9, 3]) >>> print(cat_abc) tensor([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [10, 11, 12], [13, 14, 15], [16, 17, 18], [20, 21, 22], [23, 24, 25], [26, 27, 28]]) >>> stack_abc = torch.stack([a,b,c], axis=0) # torch中dim和axis参数名可以混用 >>> print(stack_abc.shape) torch.Size([3, 3, 3]) >>> print(stack_abc) tensor([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[20, 21, 22], [23, 24, 25], [26, 27, 28]]]) >>> chunk_abc = torch.chunk(cat_abc, 3, dim=0) >>> chunk_abc (tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]]), tensor([[10, 11, 12], [13, 14, 15], [16, 17, 18]]), tensor([[20, 21, 22], [23, 24, 25], [26, 27, 28]])) 4.2，torch.split 和 torch.chunk torch.split() 和 torch.chunk() 可以看作是 torch.cat() 的逆运算。split() 作用是将张量拆分为多个块，每个块都是原始张量的视图。split() 函数定义如下： \"\"\" Splits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size. If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_or_sections. \"\"\" torch.split(tensor, split_size_or_sections, dim=0) chunk() 作用是将 tensor 按 dim（行或列）分割成 chunks 个 tensor 块，返回的是一个元组。chunk() 函数定义如下： torch.chunk(input, chunks, dim=0) → List of Tensors \"\"\" Splits a tensor into a specific number of chunks. Each chunk is a view of the input tensor. Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks. Parameters: input (Tensor) – the tensor to split chunks (int) – number of chunks to return dim (int) – dimension along which to split the tensor \"\"\" 实例代码如下： >>> a = torch.arange(10).reshape(5,2) >>> a tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) >>> torch.split(a, 2) (tensor([[0, 1], [2, 3]]), tensor([[4, 5], [6, 7]]), tensor([[8, 9]])) >>> torch.split(a, [1,4]) (tensor([[0, 1]]), tensor([[2, 3], [4, 5], [6, 7], [8, 9]])) >>> torch.chunk(a, 2, dim=1) (tensor([[0], [2], [4], [6], [8]]), tensor([[1], [3], [5], [7], [9]])) 五，卷积相关算子 5.1，上采样方法总结 上采样大致被总结成了三个类别： 基于线性插值的上采样：最近邻算法（nearest）、双线性插值算法（bilinear）、双三次插值算法（bicubic）等，这是传统图像处理方法。 基于深度学习的上采样（转置卷积，也叫反卷积 Conv2dTranspose2d等） Unpooling 的方法（简单的补零或者扩充操作） 计算效果：最近邻插值算法 双线性插值 > 双三次插值。 5.2，F.interpolate 采样函数 Pytorch 老版本有 nn.Upsample 函数，新版本建议用 torch.nn.functional.interpolate，一个函数可实现定制化需求的上采样或者下采样功能，。 F.interpolate() 函数全称是 torch.nn.functional.interpolate()，函数定义如下： def interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None): # noqa: F811 # type: (Tensor, Optional[int], Optional[List[float]], str, Optional[bool], Optional[bool]) -> Tensor pass 参数解释如下： input(Tensor)：输入张量数据； size： 输出的尺寸，数据类型为 tuple： ([optional D_out], [optional H_out], W_out)，和 scale_factor 二选一。 scale_factor：在高度、宽度和深度上面的放大倍数。数据类型既可以是 int——表明高度、宽度、深度都扩大同一倍数；也可是tuple`——指定高度、宽度、深度等维度的扩大倍数。 mode： 上采样的方法，包括最近邻（nearest），线性插值（linear），双线性插值（bilinear），三次线性插值（trilinear），默认是最近邻（nearest）。 align_corners： 如果设为 True，输入图像和输出图像角点的像素将会被对齐（aligned），这只在 mode = linear, bilinear, or trilinear 才有效，默认为 False。 例子程序如下： import torch.nn.functional as F x = torch.rand(1,3,224,224) y = F.interpolate(x * 2, scale_factor=(2, 2), mode='bilinear').squeeze(0) print(y.shape) # torch.Size([3, 224, 224) 5.3，nn.ConvTranspose2d 反卷积 转置卷积（有时候也称为反卷积，个人觉得这种叫法不是很规范），它是一种特殊的卷积，先 padding 来扩大图像尺寸，紧接着跟正向卷积一样，旋转卷积核 180 度，再进行卷积计算。 参考资料 pytorch演示卷积和反卷积运算 torch.Tensor PyTorch学习笔记(10)——上采样和PixelShuffle 反卷积 Transposed convolution PyTorch中的转置卷积详解——全网最细 4-1,张量的结构操作 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"5-deep_learning/深度学习基础/1-深度学习算法基础.html":{"url":"5-deep_learning/深度学习基础/1-深度学习算法基础.html","title":"1-深度学习算法基础","keywords":"","body":" 1, 基本概念 1.1，余弦相似度 1.2，欧式距离 1.3，余弦相似度和欧氏距离的区别 2，容量、欠拟合和过拟合 3，正则化方法 4，超参数和验证集 5，估计、偏差和方差 6，随机梯度下降算法 1, 基本概念 一，欧氏距离与余弦相似度（cos距离） 专题-机器学习实践 余弦相似度 | 文本分析：基础 1.1，余弦相似度 通过对两个文本分词，TF-IDF 算法向量化，利用空间中两个向量的夹角，来判断这两个向量的相似程度：(计算夹角的余弦，取值 0-1) 当两个向量夹角越大，距离越远，最大距离就是两个向量夹角 180°； 夹角越小，距离越近，最小距离就是两个向量夹角 0°，完全重合。 夹角越小相似度越高，但由于有可能一个文章的特征向量词特别多导致整个向量维度很高，使得计算的代价太大不适合大数据量的计算。 计算两个向量a、b的夹角余弦： 我们知道，余弦定理：$cos(\\theta) = \\frac {a^2+b^2+c^2}{2ab}$ ，由此推得两个向量夹角余弦的计算公式如下： $$cos(\\theta) = \\frac {ab}{||a|| \\times ||b||} = \\frac {x{1}x{2}+y_1y_2}{\\sqrt{x^2_1+y^2_1}\\sqrt{x^2_2+y^2_2}}$$ （分子就是两个向量的内积，分母是两个向量的模长乘积） 1.2，欧式距离 在欧几里得空间中，欧式距离其实就是向量空间中两点之间的距离。点 $x = (x{1}, ..., x{n})$ 和 $y = (y{1}, ..., y{n})$ 之间得欧氏距离计算公式如下： $$d(x,y) = \\sqrt {((x{1}-y{1})^{2} + (x{2}-y{2})^{2} + ... + (x{n}-y{n})^{2})}$$ 1.3，余弦相似度和欧氏距离的区别 欧式距离和余弦相似度都能度量 2 个向量之间的相似度 放到向量空间中看，欧式距离衡量两点之间的直线距离，而余弦相似度计算的是两个向量之间的夹角 没有归一化时，欧式距离的范围是 [0, +∞]，而余弦相似度的范围是 [-1, 1]；余弦距离是计算相似程度，而欧氏距离计算的是相同程度（对应值的相同程度） 归一化的情况下，可以将空间想象成一个超球面（三维），欧氏距离就是球面上两点的直线距离，而向量余弦值等价于两点的球面距离，本质是一样。 2，容量、欠拟合和过拟合 模型容量是指模型拟合各种函数的能力，决定了模型是欠拟合还是过拟合。 欠拟合就是指模型的训练误差过大，即偏差过大，表现为模型不够”准“，优化算法目的在于解决欠拟合问题。 过拟合就是指训练误差和测试误差间距过大，即方差过大，表现为模型不够”稳“，正则化目的在于解决过拟合问题。 机器学习模型的目的是解决欠拟合和过拟合的问题，这也是机器学习算法的两个挑战。 训练误差 train error，泛化误差 generalization error，也叫测试误差(test error)。 3，正则化方法 正则化是指我们修改学习算法，使其降低泛化误差而非训练误差。 正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相媲。 正则化一个学习函数为 $f(x; θ)$ 的模型，我们可以给代价函数（损失函数）添加被称为正则化项（regularizer）的惩罚。 正则化是一种思想（策略），给代价函数添加惩罚只是其中一种方法。另外一种最常用的正则化技术是权重衰减，通过加入的正则项对参数数值进行衰减，得到更小的权值。当 $\\lambda$ 较大时，会使得一些权重几乎衰减到零，相当于去掉了这一项特征，类似于减少特征维度。 4，超参数和验证集 普通参数指算法权重 $w$ 的值，是可以通过学习算法本身学习得到。超参数的值不是通过学习算法本身学习出来的，可通过验证集人为选择合适的超参数。 将训练数据划分为两个不相交的子集，即训练集和验证集，训练集用于学习普通参数，验证集用于估计训练中或训练后的泛化误差，更新超参数（“训练超参数”）。通常，80% 的训练数据用于训练，20% 用于验证。 交叉验证方法适合小规模数据集（例如几百上千张图片）训练模型的情况。 5，估计、偏差和方差 统计领域的基本概念，例如参数估计、偏差和方差，对于正式地刻画泛化、欠拟合和过拟合都非常有帮助。偏差和方差的关系和机器学习容量、欠拟合和过拟合的概念紧密相联。 偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或参数的误差期望。而方差度量着数据上任意特定采样可能导致的估计期望的偏差。 6，随机梯度下降算法 随机梯度下降算法是目前最为广泛应用的一种优化算法，形式为 $θ=θ − ϵg$，$ϵ$ 是学习率，$g$ 是梯度，$θ$ 是权重。 随机梯度下降优化算法不一定能保证在合理的时间内达到一个局部最小值，但它通常能及时地找到代价函数一个很小的值，并且是有用的。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"5-deep_learning/深度学习基础/2-使用CNN进行图像分类.html":{"url":"5-deep_learning/深度学习基础/2-使用CNN进行图像分类.html","title":"2-使用CNN进行图像分类","keywords":"","body":" 1，图像分类 2，类别不平衡问题 3，图像分类模型 4，提升分类模型精度的方法 5，数据扩充（数据增强） 6，参考资料 图像分类是计算机视觉中最基础的任务，基本上深度学习模型的发展史就是图像分类任务提升的发展历史，但是图像分类并不是那么简单，也没有被完全解决。 1，图像分类 图像分类顾名思义就是一个模式分类问题，它的目标是将不同的图像，划分到不同的类别，实现最小的分类误差。 1，单标签分类：总体来说，对于单标签的图像分类问题，它可以分为跨物种语义级别的图像分类(cifar10)，子类细粒度图像分类(Caltech-UCSD Birds-200-2011)，以及实例级图像分类(人脸识别)三大类别。 虽然基本的图像分类任务，尤其是比赛趋近饱和，但是现实中的图像任务仍然有很多的困难和挑战。如类别不均衡的分类任务，类内方差非常大的细粒度分类任务，以及包含无穷负样本的分类任务。 2，多标签分类：多标签分类问题，通常有两种解决方案，即转换为多个单标签分类问题，或者直接联合研究。前者，可以训练多个分类器，来判断该维度属性的是否，损失函数常使用softmax loss。后者，则直接训练一个多标签的分类器，所使用的标签为0,1,0,0…这样的向量，使用hanmming距离等作为优化目标。 2，类别不平衡问题 在很多情况下，可能会遇到数据不平衡问题。数据不平衡是什么意思呢？举一个简单的例子：假设你正在训练一个网络模型，该模型用来预测视频中是否有人持有致命武器。但是训练数据中只有 50 个持有武器的视频，而有 1000 个没有持有武器的视频。如果使用这个数据集完成训练的话，模型肯定倾向于预测视频中没有持有武器。针对这个问题，可以做一些事情来解决： 在损失函数中使用权重：对数据量小的类别在损失函数中添加更高的权重，使得对于该特定类别的任何未正确分类将导致损失函数输出非常高的错误。 过采样：重复包含代表性不足类别的一些训练实例有助于提升模型精度。 欠采样：对数据量大的类别进行采样，降低二者的不平衡程度。 数据扩充：对数据量小的类别进行扩充。 3，图像分类模型 近些年来用于分类（backbone）的经典 CNN 模型，它们之间计算量和精度关系的变化图如下所示。 4，提升分类模型精度的方法 5，数据扩充（数据增强） 深度学习依赖于大数据，使用更多的数据已被证明可以进一步提升模型的精度。随着扩充的处理，将会免费获得更多的数据，使用的扩充方法取决于具体任务，比如，你在做自动驾驶汽车任务，可能不会有倒置的树、汽车和建筑物，因此对图像进行竖直翻转是没有意义的，然而，当天气变化和整个场景变化时，对图像进行光线变化和水平翻转是有意义的。 6，参考资料 不懂得如何优化CNN图像分类模型？这有一份综合设计指南请供查阅 【技术综述】你真的了解图像分类吗？ Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"5-deep_learning/深度学习基础/CNN基本部件-常用激活函数.html":{"url":"5-deep_learning/深度学习基础/CNN基本部件-常用激活函数.html","title":"CNN基本部件-常用激活函数","keywords":"","body":" 背景知识 反向传播算法 梯度消失与梯度爆炸 激活函数的作用 七种常用激活函数 Sigmoid型函数 tanh(x)型函数 修正线性单元（ReLU） Leaky ReLU 参数化 ReLU 总结 参考资料 我们在项目中可能实际上就常用relu作为卷积层和全连接层的激活函数，但是，其他激活函数的特性和relu激活函数的问题及优点在哪也是我们需要知道的。本文为学习笔记，主要参考魏秀参的《CNN卷积神经网络》这本书和网上部分资料，加以自己理解，然后写的文章。 背景知识 反向传播算法 一文搞懂反向传播 梯度消失与梯度爆炸 左边是生物神经元，右边是数学模型。 激活函数的作用 激活函数实现去线性化。神经元的结构的输出为所有输入的加权和，这导致神经网络是一个线性模型。如果将每一个神经元（也就是神经网络的节点）的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性的了，这个非线性函数就是激活函数。 常见的激活函数有：ReLU函数、sigmoid函数、tanh函数。 七种常用激活函数 神经网络中常用的激活函数有七种：Sigmoid 型函数、 tanh(x) 型函数、修正线性单元（ReLU）、Leaky ReLu、参数化 ReLU、随机化 ReLU 和指数化线性单元（ELU）。 Sigmoid型函数 Sigmoid型函数也称Logistic函数：$\\sigma (x) = \\frac{1}{1+exp(-x)}$。函数形状和函数梯度图，如下图所示： 从上图可以看出，经过sigmoid型函数作用后，输出响应的值被压缩到 [0,1] 之间，而 0 对应了生物神经元的“抑制状态”，1 则恰好对应了“兴奋状态”。但对于Sigmoid 梯度函数两端大于 5（或小于 −5）的区域，这部分输出会被压缩到 1（或 0）。这样的处理会带来梯度的“饱和效应”（saturation effect），也就是梯度消失现象。所谓梯度消失现象，可以对照 Sigmoid型函数的梯度图观察，会发现大于 5（或小于 −5）部分的梯度接近 0，这会导致在误差反向传播过程中导数处于该区域的误差很难甚至无法传递至前层，进而导致整个网络无法正常训练。 另外，还有一个问题是，Sigmoid型函数值域的均值并非为 0 而是全为正，这样的结果实际上并不符合我们对神经网络内权重参数数值的期望（均值）应为0的设想。 tanh(x)型函数 tanh(x)型函数是在 Sigmoid 型函数基础上为解决均值问题提出的激活函数： $$tanh(x) = 2\\sigma(2x)-1$$ 函数形状如下图所示： tanh 型函数又称作双曲正切函数（hyperbolic tangent function）`，其函数范围是 (−1, +1)，输出响应的均值为 y。但由于 tanh(x) 型函数仍基于Sigmoid型函数，使用 tanh(x) 型函数依然会发生 “梯度消失” 现象。 修正线性单元（ReLU） ReLU是Nair和Hinton于 2010 年引入神经网络的，是截止目前为止深度卷积神经网络中最为常用的激活函数之一，其定义如下： $$ReLU(x) = max{0,x} = \\left{\\begin{matrix} x & if x\\geqslant 0 \\ 0 & if x 函数及函数梯度形状如下图所示： 与前两个激活函数相比： ReLU 函数的梯度在x >= 0 时为 1，反之则为 0；对 x >= 0 部分完全消除了sigmoid型函数的梯度消失现象。 计算复杂度上，ReLU 函数也相对前两者更简单。 同时，实验中还发现 ReLU 函数有助于随机梯度下降方法收敛，收敛速度约快 6 倍左右。 但是，ReLU函数也有自身缺陷，即在 x 0。换句话说，对于小于 0 的这部分卷积结果响应，它们一旦变为负值将再无法影响网络训练——这种现象被称作“死区\"。 Leaky ReLU 为缓解死区现象，研究者将 ReLU 函数中 x $$Leaky ReLU(x) = \\left{\\begin{matrix} x & if x>=0\\ \\alpha\\cdot x & if x 可以发现原始 ReLU 函数实际上是 Leaky ReLU 函数的一个特例，即$\\alpha=0$，但是实际项目中发现，由于 Leaky ReLU 中 $\\alpha$ 为超参数，合适的值较难设定且很敏感，所以 Leaky ReLU函数在实际使用中性能并不十分稳定。 参数化 ReLU 参数化 ReLU 直接将 $\\alpha$ 也作为一个网络中可以学习的变量融入模型的整体训练过程。具体怎么做，可参考相关文献。万事具有两面性，参数化 ReLU 在带来更大自由度的同时，也增加了网络模型过拟合的风险，在实际使用中需要格外注意。 总结 建议首先使用目前最常用的 ReLU 激活函数，但需注意模型参数初始化和学习率的设置； 为了进一步提高模型精度，可尝试 Leaky ReLU、参数化 ReLU、随机化 ReLU 和 ELU。但四者之间实际性能优劣并无一致性结论，需具体问题具体对待 参考资料 《魏秀参-CNN解析神经网络》 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"5-deep_learning/深度学习面试题.html":{"url":"5-deep_learning/深度学习面试题.html","title":"深度学习面试题","keywords":"","body":"目录 目录 一，滤波器与卷积核 二，卷积层和池化输出大小计算 2.1，CNN 中术语解释 2.2，卷积输出大小计算（简化型） 2.3，理解边界效应与填充 padding 参考资料 三，深度学习框架的张量形状格式 四，Pytorch 、Keras 的池化层函数理解 4.1，torch.nn.MaxPool2d 4.2，keras.layers.MaxPooling2D 五，Pytorch 和 Keras 的卷积层函数理解 5.1，torch.nn.Conv2d 5.2，keras.layers.Conv2D 5.3，总结 六，softmax 回归 七，交叉熵损失函数 7.1，为什么交叉熵可以用作代价函数 7.2，优化算法理解 八，感受野理解 8.1，感受野大小计算 九，卷积和池化操作的作用 参考资料 十，卷积层与全连接层的区别 十一，CNN 权值共享问题 十二，CNN 结构特点 Reference 十三，深度特征的层次性 十四，什么样的数据集不适合深度学习 十五，什么造成梯度消失问题 十六，Overfitting 和 Underfitting 问题 16.1，过拟合问题怎么解决 16.2，如何判断深度学习模型是否过拟合 16.3，欠拟合怎么解决 16.4，如何判断模型是否欠拟合 十七，L1 和 L2 区别 十八，TensorFlow计算图概念 十九，BN（批归一化）的作用 二十，什么是梯度消失和爆炸 梯度消失和梯度爆炸产生的原因 如何解决梯度消失和梯度爆炸问题 二十一，RNN循环神经网络理解 二十二，训练过程中模型不收敛，是否说明这个模型无效，导致模型不收敛的原因 二十三，VGG 使用 2 个 3*3 卷积的优势 23.1，1*1 卷积的主要作用 二十四，Relu比Sigmoid效果好在哪里？ 参考链接 二十五，神经网络中权值共享的理解 参考资料 二十六，对 fine-tuning(微调模型的理解)，为什么要修改最后几层神经网络权值？ 参考资料 二十七，什么是 dropout? 27.1，dropout具体工作流程 27.2，dropout在神经网络中的应用 27.3，如何选择dropout 的概率 参考资料 二十八，HOG 算法原理描述 HOG特征原理 HOG特征检测步骤 参考资料 二十九，激活函数 29.1，激活函数的作用 29.2，常见的激活函数 29.3，激活函数理解及函数梯度图 三十，卷积层和池化层有什么区别 三十一，卷积层和池化层参数量计算 三十二，神经网络为什么用交叉熵损失函数 三十三，数据增强方法有哪些 33.1，离线数据增强和在线数据增强有什么区别? Reference 三十四，ROI Pooling替换为ROI Align的效果，及各自原理 ROI Pooling原理 ROI Align原理 RoiPooling 和 RoiAlign 总结 Reference 三十五，CNN的反向传播算法推导 三十六，Focal Loss 公式 三十七，快速回答 37.1，为什么 Faster RCNN、Mask RCNN 需要使用 ROI Pooling、ROI Align? 37.2，softmax公式 37.3，上采样方法总结 37.4，移动端深度学习框架知道哪些，用过哪些？ 37.5，如何提升网络的泛化能力 37.6，BN算法，为什么要在后面加伽马和贝塔，不加可以吗？ 37.7，验证集和测试集的作用 三十八，交叉验证的理解和作用 三十九，介绍一下NMS和IOU的原理 Reference 一，滤波器与卷积核 在只有一个通道的情况下，“卷积核”（“kernel”）就相当于滤波器（“filter”），这两个概念是可以互换的。一个 “Kernel” 更倾向于是 2D 的权重矩阵。而 “filter” 则是指多个 kernel 堆叠的 3D 结构。如果是一个 2D 的 filter，那么两者就是一样的。但是一个3D filter，在大多数深度学习的卷积中，它是包含 kernel 的。每个卷积核都是独一无二的，主要在于强调输入通道的不同方面。 二，卷积层和池化输出大小计算 不管是 TensorFlow、Keras、Caffe 还是 Pytorch，其卷积层和池化层的参数默认值可能有所不同，但是最终的卷积输出大小计算公式是一样的。 2.1，CNN 中术语解释 卷积层主要参数有下面这么几个： 卷积核 Kernal 大小（在 Tensorflow/keras 框架中也称为filter）； 填充 Padding ； 滑动步长 Stride； 输出通道数 Channels。 2.2，卷积输出大小计算（简化型） 1，在 Pytorch 框架中，图片（feature map）经卷积 Conv2D 后输出大小计算公式如下：$\\left \\lfloor N = \\frac{W-F+2P}{S}+1 \\right \\rfloor$，其中 $\\lfloor \\rfloor$ 是向下取整符号，用于结果不是整数时进行向下取整（Pytorch 的 Conv2d 卷积函数的默认参数 ceil_mode = False，即默认向下取整, dilation = 1）。 输入图片大小 W×W（默认输入尺寸为正方形） Filter 大小 F×F 步长 S padding的像素数 P 输出特征图大小 N×N 2，特征图经反卷积（也叫转置卷积） keras-Conv2DTranspose（pytorch-ConvTranspose2d） 后得到的特征图大小计算方式：$out = (in - 1) s -2p + k$，还有另外一个写法：$W = (N - 1)S - 2P + F$，可由卷积输出大小计算公式反推得到。$in$ 是输入大小， $k$ 是卷积核大小， $s$ 是滑动步长， padding 的像素数 $p$，$out$ 是输出大小。 反卷积也称为转置卷积，一般主要用来还原 feature map 的尺寸大小，在 cnn 可视化，fcn 中达到 pixel classification，以及 gan 中从特征生成图像都需要用到反卷积的操作。反卷积输出结果计算实例。例如，输入：2x2， 卷积核大小：4x4， 滑动步长：3，填充像素为 0， 输出：7x7 ，其计算过程就是， (2 - 1) * 3 + 4 = 7。 3，池化层如果设置为不填充像素（对于 Pytorch，设置参数padding = 0，对于 Keras/TensorFlow，设置参数padding=\"valid\"），池化得到的特征图大小计算方式: $N=(W-F)/S+1$，这里公式表示的是除法结果向下取整再加 1。 总结：对于Pytorch 和 tensorflow 的卷积和池化函数，卷积函数 padding 参数值默认为 0/\"valid\"（即不填充），但在实际设计的卷积神经网络中，卷积层一般会填充像素(same)，池化层一般不填充像素(valid)，输出 shape 计算是向下取整。注意：当 stride为 1 的时候，kernel为 3、padding为 1 或者 kernel为 5、padding为 2，这两种情况可直接得出卷积前后特征图尺寸不变。 注意不同的深度学习框架，卷积/池化函数的输出 shape 计算会有和上述公式有所不同，我给出的公式是简化版，适合面试题计算，实际框架的计算比这复杂，因为参数更多。 2.3，理解边界效应与填充 padding 如果希望输出特征图的空间维度Keras/TensorFlow 设置卷积层的过程中可以设置 padding 参数值为 “valid” 或 “same”。“valid” 代表只进行有效的卷积，对边界数据不处理。“same” 代表 TensorFlow 会自动对原图像进行补零（表示卷积核可以停留在图像边缘），也就是自动设置 padding 值让输出与输入形状相同。 参考资料 CNN中的参数解释及计算 CNN基础知识——卷积（Convolution）、填充（Padding）、步长(Stride) 三，深度学习框架的张量形状格式 图像张量的形状有两种约定，通道在前（channel-first）和通道在后（channel-last）的约定，常用深度学习框架使用的数据张量形状总结如下： Pytorch/Caffe: (N, C, H, W)； TensorFlow/Keras: (N, H, W, C)。 举例理解就是Pytorch 的卷积层和池化层的输入 shape 格式为 (N, C, H, W)，Keras 的卷积层和池化层的输入 shape 格式为 (N, H, W, C)。 值得注意的是 OpenCV 读取图像后返回的矩阵 shape 的格式是 （H, W, C）格式。当 OpenCV 读取的图像为彩色图像时，返回的多通道的 BGR 格式的矩阵（HWC），在内存中的存储如下图： 四，Pytorch 、Keras 的池化层函数理解 注意：对于 Pytorch、Keras 的卷积层和池化层函数，其 padding 参数值都默认为不填充像素，默认值为 0和 valid。 4.1，torch.nn.MaxPool2d class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) 二维池化层，默认输入的尺度是(N, C_in,H,W)，输出尺度（N,C_out,H_out,W_out）。池化层输出尺度的 Width 默认计算公式如下（ceil_mode= True 时是向上取整，Height 计算同理）: $$\\left\\lfloor \\frac{W_{in} + 2 * \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel_size}[0] - 1) - 1}{\\text{stride[0]}} + 1 \\right\\rfloor$$ 主要参数解释： kernel_size(int or tuple)：max pooling 的窗口大小。 stride(int or tuple, optional)：max pooling的窗口移动的步长。默认值是kernel_size`。 padding(int or tuple, optional)：默认值为 0，即不填充像素。输入的每一条边补充 0 的层数。 dilation：滑动窗中各元素之间的距离。 ceil_mode：默认值为 False，即上述公式默认向下取整，如果设为 True，计算输出信号大小的时候，公式会使用向上取整。 Pytorch 中池化层默认ceil mode = false，而 Caffe 只实现了 ceil mode= true 的计算方式。 示例代码： import torch import torch.nn as nn import torch.autograd as autograd # 大小为3，步幅为2的正方形窗口池 m = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # pool of non-square window input = autograd.Variable(torch.randn(20, 16, 50, 32)) output = m(input) print(output.shape) # torch.Size([20, 16, 25, 16]) 4.2，keras.layers.MaxPooling2D keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None) 对于 2D 空间数据的最大池化。默认输入尺寸是 (batch_size, rows, cols, channels)/(N, H, W, C_in) 的 4D 张量，默认输出尺寸是 (batch_size, pooled_rows, pooled_cols, channels) 的 4D 张量。 padding = valid：池化层输出的特征图大小为：$N=(W-F)/S+1$，这里表示的是向下取整再加 1。 padding = same: 池化层输出的特征图大小为 $N = W/S$，这里表示向上取整。 主要参数解释： pool_size: 整数，或者 2 个整数表示的元组， 沿（垂直，水平）方向缩小比例的因数。（2，2）会把输入张量的两个维度都缩小一半。 如果只使用一个整数，那么两个维度都会使用同样的窗口长度。 strides: 整数，2 个整数表示的元组，或者是 None。 表示步长值。 如果是 None，那么默认值是 pool_size。 padding: \"valid\" 或者 \"same\"（区分大小写）。 data_format: 字符串，channels_last (默认)或 channels_first 之一。 表示输入各维度的顺序。 channels_last 代表尺寸是 (batch, height, width, channels) 的输入张量， 而 channels_first 代表尺寸是 (batch, channels, height, width) 的输入张量。 默认值根据 Keras 配置文件 ~/.keras/keras.json 中的 image_data_format 值来设置。如果还没有设置过，那么默认值就是 \"channels_last\"。 五，Pytorch 和 Keras 的卷积层函数理解 5.1，torch.nn.Conv2d class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) 二维卷积层, 输入的尺度是(N, C_in, H, W)，输出尺度（N,C_out,H_out,W_out）。卷积层输出尺度的 Weight 计算公式如下（Height 同理）： $$\\left\\lfloor \\frac{W_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor$$ Pytorch/Caffe 框架输入输出数据的尺寸都是 （(N, C, H, W)），常规卷积的卷积核权重 shape 都为（C_out, C_in, kernel_height, kernel_width），常规卷积是这样，但是分组卷积的卷积核权重 shape 为（C_out, C_in/g, kernel_height, kernel_width）和 DW 卷积的卷积核 权重shape 为（C_in, 1, kernel_height, kernel_width）。 kernel_size, stride, padding, dilation 参数可以是以下两种形式( Maxpool2D 也一样)： a single int：同样的参数值被应用与 height 和 width 两个维度。 a tuple of two ints：第一个 int 值应用于 height 维度，第二个 int 值应用于 width 维度，也就是说卷积输出后的 height 和 width 值是不同的，要分别计算。 主要参数解释： in_channels(int) – 输入信号的通道。 out_channels(int) – 卷积产生的通道。 kerner_size(int or tuple) - 卷积核的尺寸。 stride(int or tuple, optional) - 卷积步长，默认值为 1 。 padding(int or tuple, optional) - 输入的每一条边补充 0 的层数，默认不填充。 dilation(int or tuple, optional) – 卷积核元素之间的间距，默认取值 1 。 groups(int, optional) – 从输入通道到输出通道的阻塞连接数。 bias(bool, optional) - 如果 bias=True，添加偏置。 示例代码： ###### Pytorch卷积层输出大小验证 import torch import torch.nn as nn import torch.autograd as autograd # With square kernels and equal stride # output_shape: height = (50-3)/2+1 = 24.5，卷积向下取整，所以 height=24. m = nn.Conv2d(16, 33, 3, stride=2) # # non-square kernels and unequal stride and with padding # m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) # 输出shape: torch.Size([20, 33, 28, 100]) # # non-square kernels and unequal stride and with padding and dilation # m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) # 输出shape: torch.Size([20, 33, 26, 100]) input = autograd.Variable(torch.randn(20, 16, 50, 100)) output = m(input) print(output.shape) # 输出shape: torch.Size([20, 16, 24, 49]) 5.2，keras.layers.Conv2D keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 2D 卷积层 (例如对图像的空间卷积)。输入输出尺寸格式要求和池化层函数一样。输入尺寸：(N, H, W, C)，卷积核尺寸：（K, K, C_in, C_out）。 当使用该层作为模型第一层时，需要提供 input_shape 参数（整数元组，不包含 batch 轴），例如，input_shape=(128, 128, 3) 表示 128x128 的 RGB 图像，在 data_format=\"channels_last\" 时。 主要参数解释： filters: 整数，输出空间的维度 （即卷积中滤波器的输出数量）。 kernel_size: 一个整数，或者 2 个整数表示的元组或列表，指明 2D 卷积窗口的宽度和高度。 可以是一个整数，为所有空间维度指定相同的值。 strides: 一个整数，或者 2 个整数表示的元组或列表，指明卷积核模板沿宽度和高度方向的移动步长。 可以是一个整数，为所有空间维度指定相同的值。 指定任何 stride 值 != 1 与指定 dilation_rate 值 != 1 两者不兼容，默认取值 1，即代表会不遗漏的滑过输入图片（Feature Map）的每一个点。 padding: \"valid\" 或 \"same\" (大小写敏感)，默认valid，这里的 \"same\" 代表给边界加上 Padding 让卷积的输出和输入保持同样（\"same\"）的尺寸（即填充像素）。 data_format: 字符串， channels_last (默认) 或 channels_first 之一，表示输入中维度的顺序。 channels_last 对应输入尺寸为 (batch_size, height, width, channels)， channels_first 对应输入尺寸为 (batch_size, channels, height, width)。 它默认为从 Keras 配置文件 ~/.keras/keras.json 中 找到的 image_data_format 值。 如果你从未设置它，将使用 channels_last。 dilation_rate: 一个整数或 2 个整数的元组或列表， 指定膨胀卷积（空洞卷积 dilated convolution）的膨胀率。 可以是一个整数，为所有空间维度指定相同的值。 当前，指定任何 dilation_rate 值 != 1 与 指定 stride 值 != 1 两者不兼容。 5.3，总结 Pytorch 的 Conv2d 函数不要求提供 输入数据的大小 (H,W)，但是要提供输入深度，Keras 的 Conv2d 函数第一层要求提供 input_shape 参数 (H,W, C)，其他层不需要。 六，softmax 回归 分类问题中，直接使用输出层的输出有两个问题： 神经网络输出层的输出值的范围不确定，我们难以直观上判断这些值的意义 由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量 softmax 回归解决了以上两个问题，它将输出值变换为值为正且和为 1 的概率分布，公式如下： $$ softmax(y){i} = y{i}^{'} = \\frac{e^{yi}}{\\sum_{j=1}^{n}e^{yj}} $$ 七，交叉熵损失函数 交叉熵刻画了两个概率分布之间的距离，它是分类问题中使用比较广泛的一种损失函数，交叉熵一般会与 softmax 回归一起使用，公式如下： $$L = -\\sum{c=1}^{M}y{c}log(p_{c})或者H(p,q)=-\\sum p(x)logq(x)$$ $p$ ——代表正确答案； $q$ ——代表预测值； $M$ ——类别的数量； $y_{c}$ ——指示变量（ 0 或 1），如果该类别和样本的类别相同就是 1，否则是 0； $p_{c}$ ——对于观测样本属于类别 $c$ 的预测概率。 7.1，为什么交叉熵可以用作代价函数 从数学上来理解就是，为了让学到的模型分布更接近真实数据的分布，我们需要最小化模型数据分布与训练数据之间的 KL 散度，而因为训练数据的分布是固定的，因此最小化 KL 散度等价于最小化交叉熵，而且交叉熵计算更简单，所以机器/深度学习中常用交叉熵 cross-entroy 作为分类问题的损失函数。 7.2，优化算法理解 Adam、AdaGrad、RMSProp优化算法具有自适应性。 八，感受野理解 感受野理解(Receptive Field)是指后一层神经元在前一层神经元的感受空间，也可以定义为卷积神经网络中每层的特征图（Feature Map）上的像素点在原始图像中映射的区域大小，即如下图所示： 注意：感受野在 CNN 中是呈指数级增加的。小卷积核（如 3*3）通过多层叠加可取得与大卷积核（如 7*7）同等规模的感受野，此外采用小卷积核有两个优势： 小卷积核需多层叠加，加深了网络深度进而增强了网络容量(model capacity)和复杂度（model complexity）。 增强了网络容量的同时减少了参数个数。 8.1，感受野大小计算 计算感受野时，我们需要知道： 参考 感受野（receptive file）计算 第一层卷积层的输出特征图像素的感受野的大小等于滤波器的大小； 深层卷积层的感受野大小和它之前所有层的滤波器大小和步长有关系； 计算感受野大小时，忽略了图像边缘的影响。 感受野的计算方式有两种：自底向上和自顶向下（top to down），这里只讲解后者。正常卷积（且不带 padding）感受野计算公式如下： $$F(i, j-1) = (F(i, j)-1)*stride + kernel_size$$ 其中 $F(i, j)$ 表示第 i 层对第 j 层的局部感受野，所以这个公式是从上层向下层计算感受野的。仔细看这个公式会发现和反卷积输出大小计算公式一模一样，实际上感受野计算公式就是 feature_map 计算公式的反向推导。 以下 Python 代码可以实现计算 Alexnet zf-5 和 VGG16 网络每层输出 feature map 的感受野大小，卷积核大小和输入图像尺寸默认定义好了，代码如下： # !/usr/bin/env python # [filter size, stride, padding] net_struct = {'alexnet': {'net':[[11,4,0],[3,2,0],[5,1,2],[3,2,0],[3,1,1],[3,1,1],[3,1,1],[3,2,0]], 'name':['conv1','pool1','conv2','pool2','conv3','conv4','conv5','pool5']}, 'vgg16': {'net':[[3,1,1],[3,1,1],[2,2,0],[3,1,1],[3,1,1],[2,2,0],[3,1,1],[3,1,1],[3,1,1], [2,2,0],[3,1,1],[3,1,1],[3,1,1],[2,2,0],[3,1,1],[3,1,1],[3,1,1],[2,2,0]], 'name':['conv1_1','conv1_2','pool1','conv2_1','conv2_2','pool2','conv3_1','conv3_2', 'conv3_3', 'pool3','conv4_1','conv4_2','conv4_3','pool4','conv5_1','conv5_2','conv5_3','pool5']}, 'zf-5':{'net': [[7,2,3],[3,2,1],[5,2,2],[3,2,1],[3,1,1],[3,1,1],[3,1,1]], 'name': ['conv1','pool1','conv2','pool2','conv3','conv4','conv5']}} def outFromIn(isz, net, layernum): \"\"\" 计算feature map大小 \"\"\" totstride = 1 insize = isz # for layer in range(layernum): fsize, stride, pad = net[layernum] outsize = (insize - fsize + 2*pad) / stride + 1 insize = outsize totstride = totstride * stride return outsize, totstride def inFromOut(net, layernum): \"\"\" 计算感受野receptive file大小 \"\"\" RF = 1 for layer in reversed(range(layernum)): # reversed 函数返回一个反向的迭代器 fsize, stride, pad = net[layer] RF = ((RF -1)* stride) + fsize return RF if __name__ == '__main__': imsize = 224 feature_size = imsize print (\"layer output sizes given image = %dx%d\" % (imsize, imsize)) for net in net_struct.keys(): feature_size = imsize print ('************net structrue name is %s**************'% net) for i in range(len(net_struct[net]['net'])): feature_size, stride = outFromIn(feature_size, net_struct[net]['net'], i) rf = inFromOut(net_struct[net]['net'], i+1) print (\"Layer Name = %s, Output size = %3d, Stride = % 3d, RF size = %3d\" % (net_struct[net]['name'][i], feature_size, stride, rf)) 程序输出结果如下： layer output sizes given image = 224x224 ************net structrue name is alexnet************** Layer Name = conv1, Output size = 54, Stride = 4, RF size = 11 Layer Name = pool1, Output size = 26, Stride = 2, RF size = 19 Layer Name = conv2, Output size = 26, Stride = 1, RF size = 51 Layer Name = pool2, Output size = 12, Stride = 2, RF size = 67 Layer Name = conv3, Output size = 12, Stride = 1, RF size = 99 Layer Name = conv4, Output size = 12, Stride = 1, RF size = 131 Layer Name = conv5, Output size = 12, Stride = 1, RF size = 163 Layer Name = pool5, Output size = 5, Stride = 2, RF size = 195 ************net structrue name is vgg16************** Layer Name = conv1_1, Output size = 224, Stride = 1, RF size = 3 Layer Name = conv1_2, Output size = 224, Stride = 1, RF size = 5 Layer Name = pool1, Output size = 112, Stride = 2, RF size = 6 Layer Name = conv2_1, Output size = 112, Stride = 1, RF size = 10 Layer Name = conv2_2, Output size = 112, Stride = 1, RF size = 14 Layer Name = pool2, Output size = 56, Stride = 2, RF size = 16 Layer Name = conv3_1, Output size = 56, Stride = 1, RF size = 24 Layer Name = conv3_2, Output size = 56, Stride = 1, RF size = 32 Layer Name = conv3_3, Output size = 56, Stride = 1, RF size = 40 Layer Name = pool3, Output size = 28, Stride = 2, RF size = 44 Layer Name = conv4_1, Output size = 28, Stride = 1, RF size = 60 Layer Name = conv4_2, Output size = 28, Stride = 1, RF size = 76 Layer Name = conv4_3, Output size = 28, Stride = 1, RF size = 92 Layer Name = pool4, Output size = 14, Stride = 2, RF size = 100 Layer Name = conv5_1, Output size = 14, Stride = 1, RF size = 132 Layer Name = conv5_2, Output size = 14, Stride = 1, RF size = 164 Layer Name = conv5_3, Output size = 14, Stride = 1, RF size = 196 Layer Name = pool5, Output size = 7, Stride = 2, RF size = 212 ************net structrue name is zf-5************** Layer Name = conv1, Output size = 112, Stride = 2, RF size = 7 Layer Name = pool1, Output size = 56, Stride = 2, RF size = 11 Layer Name = conv2, Output size = 28, Stride = 2, RF size = 27 Layer Name = pool2, Output size = 14, Stride = 2, RF size = 43 Layer Name = conv3, Output size = 14, Stride = 1, RF size = 75 Layer Name = conv4, Output size = 14, Stride = 1, RF size = 107 Layer Name = conv5, Output size = 14, Stride = 1, RF size = 139 九，卷积和池化操作的作用 卷积核池化的定义核过程理解是不难的，但是其作用却没有一个标准的答案，我在网上看了众多博客和魏秀参博士的书籍，总结了以下答案。 卷积层和池化层的理解可参考魏秀参的《解析卷积神经网络》书籍，卷积（convolution ）操作的作用如下： 局部感知，参数共享 的特点大大降低了网络参数，保证了网络的稀疏性。 通过卷积核的组合以及随着网络后续操作的进行，卷积操作可获取图像不同区域的不同类型特征；模型靠近底部的层提取的是局部的、高度通用的特征图，而更靠近顶部的层提取的是更加抽象的语义特征。 池化/汇合（pooling ）操作作用如下： 特征不变性（feature invariant）。汇合操作使模型更关注是否存在某些特征而不是特征具体的位置可看作是一种很强的先验，使特征学习包含某种程度自由度，能容忍一些特征微小的位移。 特征降维。由于汇合操作的降采样作用，汇合结果中的一个元素对应于原输入数据的一个子区域（sub-region），因此汇合相当于在空间范围内做了维度约减（spatially dimension reduction），从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减小计算量和参数个数。 在一定程度上能防止过拟合（overfitting），更方便优化。 参考资料 魏秀参-《解析卷积神经网络》 十，卷积层与全连接层的区别 卷积层学习到的是局部模式（对于图像，学到的就是在输入图像的二维小窗口中发现的模式） 全连接层学习到的是全局模式（全局模式就算设计所有像素） 十一，CNN 权值共享问题 首先权值共享就是滤波器共享，滤波器的参数是固定的，即是用相同的滤波器去扫一遍图像，提取一次特征特征，得到feature map。在卷积网络中，学好了一个滤波器，就相当于掌握了一种特征，这个滤波器在图像中滑动，进行特征提取，然后所有进行这样操作的区域都会被采集到这种特征，就好比上面的水平线。 十二，CNN 结构特点 典型的用于分类的CNN主要由卷积层+激活函数+池化层组成，最后用全连接层输出。卷积层负责提取图像中的局部特征；池化层用来大幅降低参数量级(降维)；全连接层类似传统神经网络的部分，用来输出想要的结果。 CNN 具有局部连接、权值共享、池化操作(简单说就是下采样)和多层次结构的特点。 局部连接使网络可以提取数据的局部特征。 权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积。 池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。 卷积神经网络学到的模式具有平移不变性（translation invariant），且可以学到模式的空间层次结构。 Reference (二)计算机视觉四大基本任务(分类、定位、检测、分割 十三，深度特征的层次性 卷积操作可获取图像区域不同类型特征，而汇合等操作可对这些特征进行融合和抽象，随着若干卷积、汇合等操作的堆叠，各层得到的深度特征逐渐从泛化特征（如边缘、纹理等）过渡到高层语义表示（躯干、头部等模式）。 十四，什么样的数据集不适合深度学习 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。 十五，什么造成梯度消失问题 神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。 梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0，造成学习停止。 十六，Overfitting 和 Underfitting 问题 16.1，过拟合问题怎么解决 首先所谓过拟合，指的是一个模型过于复杂之后，它可以很好地“记忆”每一个训练数据中随机噪音的部分而忘记了去“训练”数据中的通用趋势。训练好后的模型过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。解决办法如下： 数据增强, 增加数据多样性; 正则化策略：如 Parameter Norm Penalties(参数范数惩罚), L1, L2正则化; dropout; 模型融合, 比如Bagging 和其他集成方法; BN ,batch normalization; Early Stopping(提前终止训练)。 16.2，如何判断深度学习模型是否过拟合 首先将训练数据划分为训练集和验证集，80% 用于训练集，20% 用于验证集（训练集和验证集一定不能相交）；训练都时候每隔一定 Epoch 比较验证集但指标和训练集是否一致，如果不一致，并且变坏了，那么意味着过拟合。 用学习曲线 learning curve 来判别过拟合，参考此博客。 16.3，欠拟合怎么解决 underfitting 欠拟合的表现就是模型不收敛，原因有很多种，这里以神经网络拟合能力不足问题给出以下参考解决方法： 寻找最优的权重初始化方案：如 He正态分布初始化 he_normal，深度学习框架都内置了很多权重初始化方法； 使用适当的激活函数：卷积层的输出使用的激活函数一般为 ReLu，循环神经网络中的循环层使用的激活函数一般为 tanh，或者 ReLu； 选择合适的优化器和学习速率：SGD 优化器速度慢但是会达到最优. 16.4，如何判断模型是否欠拟合 神级网络欠拟合的特征就是模型训练了足够长但时间后, loss 值依然很大甚至与初始值没有太大区别，且精度很低，测试集亦如此。根据我的总结，原因一般有以下几种： 神经网络的拟合能力不足； 网络配置的问题； 数据集配置的问题； 训练方法出错（初学者经常碰到，原因千奇百怪）。 十七，L1 和 L2 区别 L1 范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 比如 向量 A=[1，-1，3]， 那么 A 的 L1 范数为 |1|+|-1|+|3|。简单总结一下就是： L1 范数: 为向量 x 各个元素绝对值之和。 L2 范数: 为向量 x 各个元素平方和的 1/2 次方，L2 范数又称 Euclidean 范数或 Frobenius 范数 Lp 范数: 为向量 x 各个元素绝对值 $p$ 次方和的 $1/p$ 次方. 在支持向量机学习过程中，L1 范数实际是一种对于成本函数求解最优的过程，因此，L1 范数正则化通过向成本函数中添加 L1 范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。 L1 范数可以使权值参数稀疏，方便特征提取。 L2 范数可以防止过拟合，提升模型的泛化能力。 十八，TensorFlow计算图概念 Tensorflow 是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow 中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。 十九，BN（批归一化）的作用 在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。 (1). 可以使用更高的学习率。如果每层的 scale 不一致，实际上每层需要的学习率是不一样的，同一层不同维度的 scale 往往也需要不同大小的学习率，通常需要使用最小的那个学习率才能保证损失函数有效下降，Batch Normalization 将每层、每维的 scale 保持一致，那么我们就可以直接使用较高的学习率进行优化。 (2). 移除或使用较低的 dropout。 dropout 是常用的防止 overfitting 的方法，而导致 overfitting 的位置往往在数据边界处，如果初始化权重就已经落在数据内部，overfitting现象就可以得到一定的缓解。论文中最后的模型分别使用10%、5%和0%的dropout训练模型，与之前的 40%-50% 相比，可以大大提高训练速度。 (3). 降低 L2 权重衰减系数。 还是一样的问题，边界处的局部最优往往有几维的权重（斜率）较大，使用 L2 衰减可以缓解这一问题，现在用了 Batch Normalization，就可以把这个值降低了，论文中降低为原来的 5 倍。 (4). 代替Local Response Normalization层。 由于使用了一种 Normalization，再使用 LRN 就显得没那么必要了。而且 LRN 实际上也没那么 work。 (5). Batch Normalization调整了数据的分布，不考虑激活函数，它让每一层的输出归一化到了均值为0方差为1的分布，这保证了梯度的有效性，可以解决反向传播过程中的梯度问题。目前大部分资料都这样解释，比如 BN 的原始论文认为的缓解了 Internal Covariate Shift(ICS) 问题。 关于训练阶段和推理阶段 BN 的不同可以参考 Batch Normalization详解 。 二十，什么是梯度消失和爆炸 梯度消失是指在深度学习训练的过程中，梯度随着 BP 算法中的链式求导逐层传递逐层减小，最后趋近于0，导致对某些层的训练失效； 梯度爆炸与梯度消失相反，梯度随着 BP 算法中的链式求导逐层传递逐层增大，最后趋于无穷，导致某些层无法收敛； 在反向传播过程中需要对激活函数进行求导，如果导数大于 1，那么随着网络层数的增加，梯度更新将会朝着指数爆炸的方式增加这就是梯度爆炸。同样如果导数小于 1，那么随着网络层数的增加梯度更新信息会朝着指数衰减的方式减少这就是梯度消失。 梯度消失和梯度爆炸产生的原因 出现梯度消失和梯度爆炸的问题主要是因为参数初始化不当以及激活函数选择不当造成的。其根本原因在于反向传播训练法则，属于先天不足。当训练较多层数的模型时，一般会出现梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）。注意在反向传播中，当网络模型层数较多时，梯度消失和梯度爆炸是不可避免的。 深度神经网络中的梯度不稳定性，根本原因在于前面层上的梯度是来自于后面层上梯度的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。前面的层比后面的层梯度变化更小，故变化更慢，故引起了梯度消失问题。前面层比后面层梯度变化更快，故引起梯度爆炸问题。 如何解决梯度消失和梯度爆炸问题 常用的有以下几个方案： 预训练模型 + 微调 梯度剪切 + 正则化 relu、leakrelu 等激活函数 BN 批归一化 参数标准初始化函数如 xavier CNN 中的残差结构 LSTM 结构 二十一，RNN循环神经网络理解 循环神经网络（recurrent neural network, RNN）, 主要应用在语音识别、语言模型、机器翻译以及时序分析等问题上。 在经典应用中，卷积神经网络在不同的空间位置共享参数，循环神经网络是在不同的时间位置共享参数，从而能够使用有限的参数处理任意长度的序列。 RNN 可以看做作是同一神经网络结构在时间序列上被复制多次的结果，这个被复制多次的结构称为循环体，如何设计循环体的网络结构是 RNN 解决实际问题的关键。RNN 的输入有两个部分，一部分为上一时刻的状态，另一部分为当前时刻的输入样本。 二十二，训练过程中模型不收敛，是否说明这个模型无效，导致模型不收敛的原因 不一定。导致模型不收敛的原因有很多种可能，常见的有以下几种： 没有对数据做归一化。 没有检查过你的结果。这里的结果包括预处理结果和最终的训练测试结果。 忘了做数据预处理。 忘了使用正则化。 Batch Size 设的太大。 学习率设的不对。 最后一层的激活函数用的不对。 网络存在坏梯度。比如 ReLu 对负值的梯度为 0，反向传播时，0 梯度就是不传播。 参数初始化错误。 网络太深。隐藏层神经元数量错误。 更多回答，参考此链接。 二十三，VGG 使用 2 个 3*3 卷积的优势 (1). 减少网络层参数。用两个 3*3 卷积比用 1 个 5*5 卷积拥有更少的参数量，只有后者的 2∗3∗3/5∗5=0.72。但是起到的效果是一样的，两个 3×3 的卷积层串联相当于一个 5×5 的卷积层，感受野的大小都是 5×5，即 1 个像素会跟周围 5×5 的像素产生关联。把下图当成动态图看，很容易看到两个 3×3 卷积层堆叠（没有空间池化）有 5×5 的有效感受野。 (2). 更多的非线性变换。2 个 3×3 卷积层拥有比 1 个 5×5 卷积层更多的非线性变换（前者可以使用两次 ReLU 激活函数，而后者只有一次），使得卷积神经网络对特征的学习能力更强。 paper中给出的相关解释：三个这样的层具有 7×7 的有效感受野。那么我们获得了什么？例如通过使用三个 3×3 卷积层的堆叠来替换单个 7×7 层。首先，我们结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。其次，我们减少参数的数量：假设三层 3×3 卷积堆叠的输入和输出有 C 个通道，堆叠卷积层的参数为 3×(3×3C) = 27C 个权重；同时，单个 7×7 卷积层将需要 7×7×C = 49C 个参数，即参数多 81％。这可以看作是对 7×7 卷积滤波器进行正则化，迫使它们通过 3×3 滤波器（在它们之间注入非线性）进行分解。 此回答可以参考 TensorFlow 实战 p110，网上很多回答都说的不全。 23.1，1*1 卷积的主要作用 降维（ dimension reductionality ）。比如，一张500 * 500且厚度depth为100 的图片在20个filter上做1*1的卷积，那么结果的大小为500*500*20。 加入非线性。卷积层之后经过激励层，1*1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力； 二十四，Relu比Sigmoid效果好在哪里？ Sigmoid 函数公式如下： $\\sigma (x)=\\frac{1}{1+exp(-x)}$ ReLU激活函数公式如下： $$f(x) = max(x, 0) = \\begin{cases} x, & \\text{if $x$ $\\geq$ 0} \\ 0, & \\text{if $x$ ReLU 的输出要么是 0, 要么是输入本身。虽然方程简单，但实际上效果更好。在网上看了很多版本的解释，有从程序实例分析也有从数学上分析，我找了个相对比较直白的回答，如下： ReLU 函数计算简单，可以减少很多计算量。反向传播求误差梯度时，涉及除法，计算量相对较大，采用 ReLU 激活函数，可以节省很多计算量； 避免梯度消失问题。对于深层网络，sigmoid 函数反向传播时，很容易就会出现梯度消失问题（在sigmoid接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成信息丢失），从而无法完成深层网络的训练。 可以缓解过拟合问题的发生。ReLU 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。 相比 sigmoid 型函数，ReLU 函数有助于随机梯度下降方法收敛。 参考链接 ReLU为什么比Sigmoid效果好 二十五，神经网络中权值共享的理解 权值(权重)共享这个词是由 LeNet5 模型提出来的。以 CNN 为例，在对一张图偏进行卷积的过程中，使用的是同一个卷积核的参数。 比如一个 3×3×1 的卷积核，这个卷积核内 9 个的参数被整张图共享，而不会因为图像内位置的不同而改变卷积核内的权系数。说的再直白一些，就是用一个卷积核不改变其内权系数的情况下卷积处理整张图片（当然CNN中每一层不会只有一个卷积核的，这样说只是为了方便解释而已）。 参考资料 如何理解CNN中的权值共享 二十六，对 fine-tuning(微调模型的理解)，为什么要修改最后几层神经网络权值？ 使用预训练模型的好处，在于利用训练好的SOTA模型权重去做特征提取，可以节省我们训练模型和调参的时间。 至于为什么只微调最后几层神经网络权重，是因为： (1). CNN 中更靠近底部的层（定义模型时先添加到模型中的层）编码的是更加通用的可复用特征，而更靠近顶部的层（最后添加到模型中的层）编码的是更专业业化的特征。微调这些更专业化的特征更加有用，它更代表了新数据集上的有用特征。 (2). 训练的参数越多，过拟合的风险越大。很多SOTA模型拥有超过千万的参数，在一个不大的数据集上训练这么多参数是有过拟合风险的，除非你的数据集像Imagenet那样大。 参考资料 Python深度学习p127. 二十七，什么是 dropout? dropout可以防止过拟合，dropout简单来说就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率 p 停止工作，这样可以使模型的泛化性更强，因为它不会依赖某些局部的特征。 dropout效果跟bagging效果类似（bagging是减少方差variance，而boosting是减少偏差bias） 加入dropout会使神经网络训练时间边长，模型预测时不需要dropout，记得关掉。 27.1，dropout具体工作流程 以 标准神经网络为例，正常的流程是：我们首先把输入数据x通过网络前向传播，然后把误差反向传播一决定如何更新参数让网络进行学习。使用dropout之后，过程变成如下： 1，首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元）； 2，然后把输入x通过修改后的网络进行前向传播计算，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）； 3，然后重复这一过程： 恢复被删掉的神经元（此时被删除的神经元保持原样没有更新w参数，而没有被删除的神经元已经有所更新）; 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（同时备份被删除神经元的参数）; 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。 27.2，dropout在神经网络中的应用 (1). 在训练模型阶段 不可避免的，在训练网络中的每个单元都要添加一道概率流程，标准网络和带有dropout网络的比较图如下所示： (2). 在测试模型阶段 预测模型的时候，输入是当前输入，每个神经单元的权重参数要乘以概率p。 27.3，如何选择dropout 的概率 input 的 dropout 概率推荐是 0.8， hidden layer 推荐是0.5， 但是也可以在一定的区间上取值。（All dropout nets use p = 0.5 for hidden units and p = 0.8 for input units.） 参考资料 [Dropout:A Simple Way to Prevent Neural Networks from Overfitting] 深度学习中Dropout原理解析 二十八，HOG 算法原理描述 方向梯度直方图（Histogram of Oriented Gradient, HOG）特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。在深度学习取得成功之前，Hog特征结合SVM分类器被广泛应用于图像识别中，在行人检测中获得了较大的成功。 HOG特征原理 HOG 的核心思想是所检测的局部物体外形能够被光强梯度或边缘方向的分布所描述。通过将整幅图像分割成小的连接区域（称为cells），每个 cell 生成一个方向梯度直方图或者 cell 中pixel 的边缘方向，这些直方图的组合可表示（所检测目标的目标）描述子。 为改善准确率，局部直方图可以通过计算图像中一个较大区域(称为block)的光强作为 measure 被对比标准化，然后用这个值(measure)归一化这个 block 中的所有 cells 。这个归一化过程完成了更好的照射/阴影不变性。与其他描述子相比，HOG 得到的描述子保持了几何和光学转化不变性（除非物体方向改变）。因此HOG描述子尤其适合人的检测。 HOG 特征提取方法就是将一个image： 灰度化（将图像看做一个x,y,z（灰度）的三维图像） 划分成小 cells（2*2） 计算每个 cell 中每个 pixel 的 gradient（即 orientation） 统计每个 cell 的梯度直方图（不同梯度的个数），即可形成每个 cell 的 descriptor。 HOG特征检测步骤 总结：颜色空间归一化——>梯度计算——>梯度方向直方图——>重叠块直方图归一化———>HOG特征 参考资料 HOG特征检测－简述 二十九，激活函数 29.1，激活函数的作用 激活函数实现去线性化。激活函数的引入是为了增加整个网络的表达能力（引入非线性因素），否则若干线形层的堆叠仍然只能起到线性映射的作用，无法形成复杂的函数。如果将每一个神经元（也就是神经网络的节点）的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性的了，这个非线性函数就是激活函数。 激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。 激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。 29.2，常见的激活函数 激活函数也称非线性映射函数，常见的激活函数有：ReLU 函数、sigmoid 函数、tanh 函数等，其计算公式如下： ReLU函数：$f(x)=max(x,0)$ sigmoid函数：$f(x)=\\frac{1}{1+e^{-x}}$ tanh函数：$f(x)=\\frac{1+e^{-2x}}{1+e^{-2x}}=\\frac{2}{1+e^{-2x}}-1 = 2sigmoid(2x)-1$ 29.3，激活函数理解及函数梯度图 Sigmoid 激活函数，可用作分类任务的输出层。可以看出经过sigmoid激活函数后，模型输出的值域被压缩到 0到1 区间（这和概率的取值范围一致），这正是分类任务中sigmoid很受欢迎的原因。 tanh(x) 型函数是在 sigmoid 型函数基础上为解决均值问题提出的激活函数。tanh 的形状和 sigmoid 类似，只不过tanh将“挤压”输入至区间(-1, 1)。至于梯度，它有一个大得多的峰值1.0（同样位于z = 0处），但它下降得更快，当|x|的值到达 3 时就已经接近零了。这是所谓梯度消失（vanishing gradients）问题背后的原因，会导致网络的训练进展变慢。 ReLU 处理了sigmoid、tanh中常见的梯度消失问题，同时也是计算梯度最快的激励函数。但是，ReLU函数也有自身缺陷，即在 x 三十，卷积层和池化层有什么区别 卷积层有参数，池化层没有参数； 经过卷积层节点矩阵深度会改变。池化层不会改变节点矩阵的深度，但是它可以缩小节点矩阵的大小。 三十一，卷积层和池化层参数量计算 参考 神经网络模型复杂度分析 文章。 三十二，神经网络为什么用交叉熵损失函数 判断一个输出向量和期望的向量有多接近，交叉熵（cross entroy）是常用的评判方法之一。交叉熵刻画了两个概率分布之间的距离，是分类问题中使用比较广泛的一种损失函数。给定两个概率分布 p 和 q ，通过 q 来表示 p 的交叉熵公式为：$H(p,q)=−∑p(x)logq(x)$ 三十三，数据增强方法有哪些 常用数据增强方法： 翻转：Fliplr,Flipud。不同于旋转180度，这是类似镜面的翻折，跟人在镜子中的映射类似，常用水平、上下镜面翻转。 旋转：rotate。顺时针/逆时针旋转，最好旋转 90-180 度，否则会出现边缘缺失或者超出问题，如旋转 45 度。 缩放：zoom。图像可以被放大或缩小，imgaug库可用Scal函数实现。 裁剪：crop。一般叫随机裁剪，操作步骤是：随机从图像中选择一部分，然后降这部分图像裁剪出来，然后调整为原图像的大小。根本上理解，图像crop就是指从图像中移除不需要的信息，只保留需要的部分 平移：translation。平移是将图像沿着x或者y方向（或者两个方向）移动。我们在平移的时候需对背景进行假设，比如说假设为黑色等等，因为平移的时候有一部分图像是空的，由于图片中的物体可能出现在任意的位置，所以说平移增强方法十分有用。 放射变换：Affine。包含：平移(Translation)、旋转(Rotation)、放缩(zoom)、错切(shear)。 添加噪声：过拟合通常发生在神经网络学习高频特征的时候，为消除高频特征的过拟合，可以随机加入噪声数据来消除这些高频特征。imgaug 图像增强库使用 GaussianBlur函数。 亮度、对比度增强：这是图像色彩进行增强的操作 锐化：Sharpen。imgaug库使用Sharpen函数。 33.1，离线数据增强和在线数据增强有什么区别? 数据增强分两类，一类是离线增强，一类是在线增强： 离线增强 ： 直接对硬盘上的数据集进行处理，并保存增强后的数据集文件。数据的数目会变成增强因子 x 原数据集的数目 ，这种方法常常用于数据集很小的时候 在线增强 ： 这种增强的方法用于，获得 batch 数据之后，然后对这个batch的数据进行增强，如旋转、平移、翻折等相应的变化，由于有些数据集不能接受线性级别的增长，这种方法长用于大的数据集，很多机器学习框架已经支持了这种数据增强方式，并且可以使用GPU优化计算。 `Reference 深度学习中的数据增强 三十四，ROI Pooling替换为ROI Align的效果，及各自原理 faster rcnn 将 roi pooling 替换为 roi align 效果有所提升。 ROI Pooling原理 RPN　生成的 ROI 区域大小是对应与输入图像大小（每个roi区域大小各不相同），为了能够共享权重，所以需要将这些 ROI　映射回特征图上，并固定大小。ROI Pooling操作过程如下图： ROI Pooling 具体操作如下： Conv layers 使用的是 VGG16，feat_stride=32(即表示，经过网络层后图片缩小为原图的 1/32),原图 $800\\times 800$,最后一层特征图 feature map 大小: $25\\times 25$ 假定原图中有一 region proposal，大小为 $665\\times 665$，这样，映射到特征图中的大小：$665/32=20.78$，即大小为 $20.78\\times 20.78$，源码中，在计算的时候会进行取整操作，于是，进行所谓的第一次量化，即映射的特征图大小为20*20； 假定 pooled_w=7、pooled_h=7，即 pooling 后固定成 $7\\times 7$ 大小的特征图，所以，将上面在 feature map上映射的 $20\\times 20$ 的 region proposal 划分成 49个同等大小的小区域，每个小区域的大小 $20/7=2.86$,，即 $2.86\\times 2.86$，此时，进行第二次量化，故小区域大小变成 $2\\times 2$； 每个 $2\\times 2$ 的小区域里，取出其中最大的像素值，作为这一个区域的‘代表’，这样，49 个小区域就输出 49 个像素值，组成 $7\\times 7$ 大小的 feature map。 所以，通过上面可以看出，经过两次量化，即将浮点数取整，原本在特征图上映射的 $20\\times 20$ 大小的 region proposal，偏差成大小为 $7\\times 7$ 的，这样的像素偏差势必会对后层的回归定位产生影响。所以，产生了后面的替代方案: RoiAlign。 ROI Align原理 ROI Align 的工作原理如下图。 ROI Align 是在 Mask RCNN 中使用以便使生成的候选框region proposal 映射产生固定大小的 feature map 时提出的，根据上图，有着类似的映射： Conv layers 使用的是 VGG16，feat_stride=32 (即表示，经过网络层后图片缩小为原图的1/32),原图 $800\\times 800$，最后一层特征图feature map大小: $25*\\times 25$; 假定原图中有一region proposal，大小为 $665*\\times 665$，这样，映射到特征图中的大小：$665/32=20.78$，即大小为 $20.78\\times 20.78$，此时，没有像 RoiPooling 那样就行取整操作，保留浮点数; 假定 pooled_w=7,pooled_h=7，即 pooling 后固定成 $7\\times 7$ 大小的特征图，所以，将在 feature map 上映射的 $20.78\\times 20.78$ 的 region proposal 划分成 49 个同等大小的小区域，每个小区域的大小 $20.78/7=2.97$,即 $2.97\\times 2.97$; 假定采样点数为 4，即表示，对于每个 $2.97\\times 2.97$ 的小区域，平分四份，每一份取其中心点位置，而中心点位置的像素，采用双线性插值法进行计算，这样，就会得到四个点的像素值，如下图: 上图中，四个红色叉叉‘×’的像素值是通过双线性插值算法计算得到的。 最后，取四个像素值中最大值作为这个小区域(即：$2.97\\times 2.97$ 大小的区域)的像素值，如此类推，同样是 49 个小区域得到 49 个像素值，组成 $7\\times 7$ 大小的 feature map。 RoiPooling 和 RoiAlign 总结 总结：知道了 RoiPooling 和 RoiAlign 实现原理，在以后的项目中可以根据实际情况进行方案的选择；对于检测图片中大目标物体时，两种方案的差别不大，而如果是图片中有较多小目标物体需要检测，则优先选择 RoiAlign，更精准些。 Reference RoIPooling、RoIAlign笔记 三十五，CNN的反向传播算法推导 四张图彻底搞懂CNN反向传播算法（通俗易懂） 反向传播算法推导过程（非常详细） 三十六，Focal Loss 公式 为了解决正负样本不平衡的问题，我们通常会在交叉熵损失的前面加上一个参数 $\\alpha$ ，即: $$ CE = \\left{\\begin{matrix} -\\alpha log(p), & if \\quad y=1\\ -(1-\\alpha)log(1-p), & if\\quad y=0 \\end{matrix}\\right. $$ 尽管 $\\alpha$ 平衡了正负样本，但对难易样本的不平衡没有任何帮助。因此，Focal Loss 被提出来了，即： $$ CE = \\left{\\begin{matrix} -\\alpha (1-p)^\\gamma log(p), & if \\quad y=1\\ -(1-\\alpha) p^\\gamma log(1-p), & if\\quad y=0 \\end{matrix}\\right. $$ 实验表明 $\\gamma$ 取 2, $\\alpha$ 取 0.25 的时候效果最佳。 Focal loss 成功地解决了在单阶段目标检测时，正负样本区域极不平衡而目标检测 loss 易被大批量负样本所左右的问题。 RetinaNet 达到更高的精度的原因不是网络结构的创新，而是损失函数的创新！ 三十七，快速回答 37.1，为什么 Faster RCNN、Mask RCNN 需要使用 ROI Pooling、ROI Align? 为了使得最后面的两个全连接层能够共享 conv layers(VGG/ResNet) 权重。在所有的 RoIs 都被 pooling 成（512×7×7）的feature map后，将它 reshape 成一个一维的向量，就可以利用 VGG16 的预训练的权重来初始化前两层全连接。 37.2，softmax公式 $$softmax(y){i} = \\frac{e^{y{i}}}{\\sum_{j=1}^{n}e^{y_j}}$$ 37.3，上采样方法总结 上采样大致被总结成了三个类别： 基于线性插值的上采样：最近邻算法（nearest）、双线性插值算法（bilinear）、双三次插值算法（bicubic）等，这是传统图像处理方法。 基于深度学习的上采样（转置卷积，也叫反卷积 Conv2dTranspose2d等） Unpooling 的方法（简单的补零或者扩充操作） 计算效果：最近邻插值算法 双线性插值 > 双三次插值 37.4，移动端深度学习框架知道哪些，用过哪些？ 知名的有TensorFlow Lite、小米MACE、腾讯的 ncnn 等，目前都没有用过。 37.5，如何提升网络的泛化能力 和防止模型过拟合的方法类似，另外还有模型融合方法。 37.6，BN算法，为什么要在后面加伽马和贝塔，不加可以吗？ 最后的 scale and shift 操作则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入。不加也可以。 37.7，验证集和测试集的作用 验证集是在训练过程中用于检验模型的训练情况，从而确定合适的超参数； 测试集是在模型训练结束之后，测试模型的泛化能力。 三十八，交叉验证的理解和作用 参考知乎文章 N折交叉验证的作用（如何使用交叉验证），我的理解之后补充。 三十九，介绍一下NMS和IOU的原理 NMS全称是非极大值抑制，顾名思义就是抑制不是极大值的元素。在目标检测任务中，通常在解析模型输出的预测框时，预测目标框会非常的多，其中有很多重复的框定位到了同一个目标，NMS 的作用就是用来除去这些重复框，从而获得真正的目标框。而 NMS 的过程则用到了 IOU，IOU 是一种用于衡量真实和预测之间相关度的标准，相关度越高，该值就越高。IOU 的计算是两个区域重叠的部分除以两个区域的集合部分，简单的来说就是交集除以并集。 在 NMS 中，首先对预测框的置信度进行排序，依次取置信度最大的预测框与后面的框进行 IOU 比较，当 IOU 大于某个阈值时，可以认为两个预测框框到了同一个目标，而置信度较低的那个将会被剔除，依次进行比较，最终得到所有的预测框。 Reference 《Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift》阅读笔记与实现 详解机器学习中的梯度消失、爆炸原因及其解决方法 N折交叉验证的作用（如何使用交叉验证） 5分钟理解Focal Loss与GHM——解决样本不平衡利器 深度学习CV岗位面试问题总结（目标检测篇） Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/":{"url":"6-computer_vision/","title":"6. 计算机视觉","keywords":"","body":" 一，传统数字图像处理 二，计算机视觉 2.1，目标检测基础 2.2，二阶段目标检测算法 2.3，一阶段目标检测算法 2.4，3D 视觉算法 2.5，工业视觉 2.6，项目实践 参考资料 一，传统数字图像处理 0-数字图像处理笔记 1-数字图像处理基础 2-OpenCV3 图像处理笔记 二，计算机视觉 基于深度学习算法的计算机视觉领域知识。 2.1，目标检测基础 0-目标检测模型的基础 1-目标检测模型的评价标准-AP与mAP 2.2，二阶段目标检测算法 Faster RCNN网络详解 FPN网络详解 Mask RCNN详解 Cascade RCNN论文解读 2.3，一阶段目标检测算法 RetinaNet网络详解 YOLOv1-v5论文解读 Scaled-YOLOv4论文解读 2.4，3D 视觉算法 3D视觉算法初学概述 2.5，工业视觉 Halcon快速入门 2.6，项目实践 车牌检测识别 参考资料 《cs231 课程》 resnet、faster rcnn、mask rcnn 等论文 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/数字图像处理/0-数字图像处理笔记.html":{"url":"6-computer_vision/数字图像处理/0-数字图像处理笔记.html","title":"0-数字图像处理笔记","keywords":"","body":" 一，绪论 1.1, 什么是数字图像处理 1.2，数字图像处理的起源 1.3，数字图像处理技术应用实例 1.4，数字图像处理的基本步骤 1.5，图像处理系统的组成 二，数字图像基础 2.1，视觉感知要素 2.1.1，人眼的结构 2.1.2，人眼中图像的形成 2.1.3，亮度适应与辨别 2.2，电磁波谱 2.3，图像感知与获取 2.3.1，一个简单的成像模型 2.4，图像取样和量化 2.4.1，取样和量化的概念 2.4.2，数字图像表示 2.5，像素间的一些基本关系 2.6，数字图像处理所用的基本数学工具 2.6.1，对应元素运算和矩阵运算 2.6.2，线性运算和非线性运算（一般两个图像之间） 2.6.3，数字图像处理数学工具-算术运算（一般两个图像之间） 2.6.4，集合运算和逻辑运算（一般两个图像之间） 2.6.5，空间运算（单幅图像） 2.6.5.1，单像素操作 2.6.5.2，领域运算 2.6.5.3，几何变换 三，灰度变换与空间滤波 3.1，背景 3.2，一些基本的灰度变换函数 3.2.1，图像反转 3.2.2，对数变换 3.2.3，幂律变换（伽玛变换） 3.2.4，分段线性变换函数 3.3，直方图处理 3.3.1，直方图均衡化 3.3.2，直方图匹配（规定化） 3.3.3，局部直方图处理 3.3.4，使用直方图统计增强图像 3.4，空间滤波基础 3.4.1，线性空间滤波的原理 3.4.2，空间相关与卷积 3.5，平滑（低通）空间滤波器 3.5.2，低通高斯滤波器核 3.5.3，低通滤波代码示例 3.6，锐化（高通）空间滤波器 3.6.1，基础 3.6.2，使用二阶导数锐化图像-拉普拉斯 3.6.3，钝化掩蔽和高提升滤波 3.6.4，使用一阶导数锐化图像-梯度 3.7，低通、高通、带阻核带通滤波器 3.8，组合使用空间增强方法 四，频率域滤波 4.1，背景 4.2，基本概念 4.3，取样和取样函数的傅里叶变换 4.8，使用低通频率域滤波器平滑图像 五，图像复原与重建 六，彩色图像处理 6.1，彩色基础 6.2，彩色模型 6.2.1，RGB 彩色模型 6.2.2，HSI 彩色模型 6.4，全彩色图像处理基础 6.5，彩色变换 6.5.1，公式 6.5.4，色调和彩色校正 6.5.5，彩色图像的直方图处理 6.6，彩色图像平滑和锐化 6.6.1，彩色图像的平滑 6.6.2，彩色图像的锐化 八，图像压缩和水印 8.1，基础 8.1.7，图像格式、存储器（容器）和压缩标准 8.2，霍夫曼编码 8.12，数字图像水印 参考资料 本文章是对《数字图像处理》书的学习笔记，不涉及具体代码，主要是原理概述和公式描述，及概念理解。学习数字图像处理能让我们更深入理解计算机视觉领域的内容。 一，绪论 1.1, 什么是数字图像处理 一副图像可以定义为一个二维函数 $f(x,y)$，其中 $x$ 和 $y$ 是空间（平面）坐标，任意一对空间坐标 $(x,y)$ 处的幅度值 $f$ 称为图像在该坐标点的强度或灰度。当 $x,y$ 和灰度值 $f$ 都是有限的离散量时，我们称该图像为数字图像。数字图像处理是指借助于数字计算机来处理数字图像。注意，数字图像由有限数量的元素组成，每个元素都有一定的位置和数值，这些元素称为像素。 图像处理止于何处或其他相关领域（如图像分析和计算机视觉）始于何处，目前人们的看法并不一致。在本书中，将数字图像处理定义为其输入和输出都是图像的处理。 1.2，数字图像处理的起源 略。 1.3，数字图像处理技术应用实例 目前数字图像处理已经应用于各行各业，在本书中，为了更简单表述，将数字图像处理的应用领域根据数字图像源分类（如可见光或 X 射线等）。目前最主要等图像源是电磁波谱，其他重要的图像源还有声波、超声波和电子（电子显微镜中所用的电子束）。 基于电磁波谱辐射的图像是我们比较数字的图像，比如 X 射线和可见光谱段的图像。电磁波可定义为以各种波长传播的正弦波，或可视为无质量的粒子流，每个粒子流以波的形式传播并以光速运动。每个无质量的粒子都包含一定的能量（或一束能量），每束能量称为一个光子。根据每个光子的能量对光谱波段进行分组，得到图 1.5 所示的光谱，其范围从伽玛射线（最高能量）到无线电波（最低能量）。 根据以上电磁波谱，可得到以下电磁波谱成像： 伽玛射线成像：伽玛射线成像的主要用途包括核医学和天文观测。在核医学中，方法是将放射性同位素注入人体，同位素衰变时会发射伽玛射线，图像则由伽玛射线检测器收集到的放射线产生。下图展示了一幅使用伽玛射线成像得到的人体骨骼扫描图像。 X 射线成像：X 射线成像除了应用于医学诊断，还被广泛应用于工业和其他领域，如天文学。 紫外波段成像。 可见光和红外波段成像：应用领域最广，如光学显微镜、遥感图像、气象监测图像、工业自动视觉检测等。 微波波段成像：主要应用是雷达。 无线电波成像：应用领域如医学等磁共振成像（MRI）。 一些图像实例效果图如下图所示。 1.4，数字图像处理的基本步骤 本书中的数字图像处理步骤如下图所示。 1.5，图像处理系统的组成 略。 二，数字图像基础 相机成像的原理：针孔相机( Pinhole Camera )通过投影变换，可以将三维相机（Camera）坐标转换为二维的图像坐标，这个变换矩阵是相机的内在属性，称为相机内参（Camera Intrinsic） K。 yaw 航向角，pitch 俯仰角，roll 翻滾角。 2.1，视觉感知要素 通过这节内容了解图像被人类感知的基本原理及人类视觉的物理限制。 2.1.1，人眼的结构 下图显示了人眼的简化剖面图。 眼镜最靠内部的膜是视网膜，它布满了整个后部的内壁。眼睛聚焦时，来自物体的光在视网膜上成像。模式视觉由分布在视网膜表面上的哥哥分立光感受器提供，分为两类：锥状体和杆状体。锥状体视觉称为明视觉或亮视觉。杆状体视觉称为暗视觉或微光视觉。 2.1.2，人眼中图像的形成 数码相机中，既有固定焦距的镜头，也有可变焦距的镜头，不同距离的聚焦时通过改变镜头和成像平面之间的距离来实现的。在人眼中，晶状体和成像区域（视网膜）之间的距离是固定的，正确的聚焦是通过改变晶状体的形状得到（远离压扁晶状体，接近目标则加厚晶状体），晶状体中心和沿视轴的视网膜之间的距离约为 17mm，焦距范围为 14~17 mm。下图所示的几何关系说明了在视网膜上所形成的图像的尺寸。令 $h$ 表示视网膜图像中物体的高度， 根据几何关系：$15/100 = h/17$，得到 h = 2.5 mm。 2.1.3，亮度适应与辨别 以下两种现象表明人眼的感知亮度不是十几灰度的简单函数： “下冲”或“上冲”现象（马赫带效应）。 同时对比现象。 2.2，电磁波谱 更详细的电磁波谱图如下图所示。波长常用的单位是米（m），常用的单位是微米（表示为 $\\mu m$，$1\\mu m=10{^-6}m$）。 感知的物体颜色由物体反射的光的性质决定。 没有颜色的光称为单色光或无色光。 彩色光源的三个属性：频率、辐射、光通量和亮度。 2.3，图像感知与获取 将照射能量转换为数字图像主要由三种传感器配置： 使用单个传感器获取图像 使用条带传感器获取图像：如磁共振成像（MRI）和正电子发射断层成像（PET）等。 使用阵列传感器获取图像: 如单反相机和手机相机。 2.3.1，一个简单的成像模型 如 1.1 节所述，我们用形如 $f(x,y)$ 的二维函数来表示图像，在空间坐标 (x,y) 处 $f$ 的值是一个标量，其范围 $0\\leq f(x,y) 2.4，图像取样和量化 多少传感器的输出都是连续的电压被判刑，这些波形的幅度和空间特性都与正被感测的物理现象相关。要产生一幅数字图像，就需要把感测得到的连续数据转化为数字形式，这包括两个步骤：取样和量化。 2.4.1，取样和量化的概念 一幅连续图像 $f$，对坐标值进行数字化称为取样（或采样），对幅度值进行数字化称为量化。 2.4.2，数字图像表示 在计算机中，数字图像可用一个 $M\\times N$矩阵表示，图像长为 $M$，宽为 $N$，矩阵中的每个元素即为图像的像素。 2.5，像素间的一些基本关系 像素的相邻像素 邻接、连通、区域和边界 距离测度：两个像素的距离，通过欧几里得（欧式）距离计算：$D(p,q) = \\sqrt{[(x-u)^2 + (y-v)^2]}$2.6，数字图像处理所用的基本数学工具 2.6.1，对应元素运算和矩阵运算 涉及一幅或多幅图像的对应元素运算是逐个像素操作的，有因为在数字图像处理中，图像可以等效为矩阵，所以图像之间的运算是可以用矩阵理论执行的。 2.6.2，线性运算和非线性运算（一般两个图像之间） 线性运算更为重要，包含了大量适用于图像处理的理论与实践成果；非线形运算范围比较有限。 2.6.3，数字图像处理数学工具-算术运算（一般两个图像之间） 算术运算常用在特定的天文、医学等领域，将两幅图像经过算术运算从而得到更为清晰的图像，两幅图像的算术运算表示如下： 这些加减乘除运算都是对应的像素运算，算术运算一般有以下应用： 使用图像相加（平均）降低噪声。 使用图像相减比较图像。 使用图像相乘/相除校正阴影和模板。 3 种算法运算的实际应用效果对比图如下所示： 2.6.4，集合运算和逻辑运算（一般两个图像之间） 注意这里的集合运算针对的是二值图像，或者图像中所有像素具有相同的灰度值且。 假设有两个集合 A 和，在数字图像处理中常见的集合运算有： 交集运算: $C = A\\cap B$，满足交换律、结合律和分配律。 并集运算: $C = A\\cup B$。 如果想要知道一幅二值图像中的两个目标 A 和 B 是否重叠，可通过计算 A\\cap B$。如果结果不是空集，则可确定两个目标的某些元素是有重叠的。 2.6.5，空间运算（单幅图像） 空间运算是直接对单幅图像的像素执行数学操作，分为三类：（1）单像素运算；（2）领域运算；（3）几何空间运算。 2.6.5.1，单像素操作 用一个变化函数$T$改变图像中各个像素的灰度: $$s = T(z)$$ 上述公式对应单像素操作，$z$是原图像中像素的灰度，$s$是处理后图像中对应像素的（映射）的灰度。 2.6.5.2，领域运算 令 $S{xy}$ 代表图像 $f$中以任意一点 $(x,y)$ 为中心的一个邻域的做标集，领域处理后，输出图像$g$中的相同坐标处会生成一个新的像素，该像素的值由输入图像中邻域像素的规定运算和集合$S{xy}$中的坐标确定。假设领域运算对应的是计算大小为$m\\times n$、中心为$(x,y)$的矩形领域中像素的平均值，且这个区域中的像素坐标是集合$S_{xy}$的元素，那么其对应的领域运算公式如下： $$g(x,y) = \\frac{1}{mn} \\sum{(r,c)\\in S{xy}}f(r,c)$$ 上述公式中，$r$和$c$是像素的行和列坐标，属于集合$S_{xy}$，图像$g$是通过移动坐标$(x,y)$，使得领域的中心逐个移过图像$f$中的所有像素，然后在每个新位置都重复这一领域运算得到，对应的示意图如下： 简单理解所谓的领域运算就是对特定 roi 区域的所有像素，做特定操作，而这个操作就是以指定位置 $(x,y)$ 为中心，邻域范围为 $m\\times n$，对这个范围内的像素取平均/求和/最大值/等。 典型的就是 CNN 模型中卷积层的滤波器操作。 2.6.5.3，几何变换 几何变换即改变图像中像素的空间排列，由两种基本运算组成： 坐标的空间变换； 灰度内插，即为变换后的像素赋灰度值（灰度图）。 坐标变换公式可表示为： 最为重要的是放射变换，它包括缩放变换、平移变换、旋转变换和剪切变换。式(2.44)无法表示平移变换（需要在公式右侧添加一个常数二维向量），所以需将上式升级为，如下所示的齐次坐标变换。 常见图像几何操作对应的仿射矩阵 A 、变换坐标公式以及示意图如下表所示： 三，灰度变换与空间滤波 空间域指的是图像平面本身，空间域中的图像处理方法是直接对图像中的像素进行处理。空间域图像处理的两个主要类别是: 灰度变换: 如对比度处理和图像阈值处理等任务，直接对图像的给个像素进行操作。 空间滤波: 如图像平滑和锐化，对图像中的每个像素的邻域进行操作。 3.1，背景 本章中讨论的所有图像处理技术都是在空间域中实现的，所谓的空间域即包含图像中像素的平面。空间域技术直接操作图像中的像素，而频率域技术操作的是图像的傅立叶变换而非图像本身。 由图像的坐标张成的实平面部分称为空间域，$x$和$y$称为空间变量或空间坐标。 尽管灰度变换和空间滤波的应用范围广泛，但本书中的大多数例子是关于图像增强的。所谓图像增强技术，是为了某些特定应用对原图像进行加工的技术，不具备通用性。 3.2，一些基本的灰度变换函数 通过灰度变换函数$T$将原来的像素值$r$映射为像素值$s$。灰度变换中常用的 3 类基本函数是线性（反转和恒等变换: 输入灰度和输出灰度相同）函数、对数（对数和反对数变换）函数和幂律（n次幂或n次根）函数。 3.2.1，图像反转 假设原图像像素值为$r$，灰度级在区间为 [0, L-1]，则起反转后的图像形式为 $$s=L-1-r$$ 图像反转实例效果图如下所示： 3.2.2，对数变换 对数变换的形式如下： $$s = clog(1+r)$$ 图3.3中对数曲线的形状表明，对数变换会将输入中范围较窄的低灰度值映射为输出中范围较宽的灰度级。例如区间 [0, L/4]中的输入灰度级映射到 [0, 3L/4] 中的输出灰度级；相反输入中的高灰度级被映射为输出中范围较窄的灰度级。 3.2.3，幂律变换（伽玛变换） 幂律变换形式如下： $$s = cr^\\gamma $$ 3.2.4，分段线性变换函数 对比度拉伸 对比度拉伸可以拓展图像中的灰度级范围，使其覆盖记录介质或显示设备的整个理想灰度范围。 灰度级分层 有些图像增强应用的目的是为了突出图像中的特定灰度空间，比如增强卫星图像中的特征、增强 X 射线图像中的缺陷等。灰度级分层可以基于两个基本方法及其变体来实现。 一种方法是将感兴趣范围内的所有灰度值显示为一个值（如白色），而将其他范围的灰度值显示为另一个值（黑色），这种变换会得到一个二值图像。 另一种方法是基于图3.11(b)中的变换，使期望的灰度范围变亮（或变暗），但保持图像中的其他灰度级不变。 灰度级分层的实际应用例子如下图所示 我个人感觉这个应用得根据实际专业场景结合起来使用，难点在于灰度级范围的选择。 比特平面分层 略。 3.3，直方图处理 令 $r_k(k = 0,1,2...L-1)$ 表示一幅$L$级灰度数字图像 $f(x,y)$ 的灰度。 $f$ 的非归一化直方图定义为 $$h(r{k}) = n{k}, k = 0,1,2...L-1$$ 式中,$n_k$是$f$中灰度为$r_k$的像素的数量，并且细分的灰度级称为直方图容器。类似地，归一化直方图定义为 $$p(r{k}) = \\frac{h(r{k})}{MN} = \\frac{n_{k}}{MN}$$ 式中，$M$ 和 $N$ 分别是图像的行数和列数。对$k$的所有值，$p(r_{k})$的和总是 1. 下显示了具有 4 个基本灰度特性的图像: 从上图的分析我么可以得出这样一个结论: 即像素占据整个灰度级范围并且均匀分布的图像，将具有高对比度的外观和多种灰色调。最终结果将是显示了大量灰度细节并具有高动态范围的一副图像。 3.3.1，直方图均衡化 1，直方图均衡化所用的变换函数如下（推导过程复杂，跳过，感兴趣的可以阅读原书了解过程） 2，直方图均衡化的目的是为了生成一幅具有均匀直方图的输出图像。 3，直方图均衡化效果示例如下图所示: 直方图均衡化效果总结分析：尽管 4 个直方图都不同，但直方图均衡化后的图像是很相似的，因为原来的 4 个图的基本区别是对比度而非内容。 3.3.2，直方图匹配（规定化） 直方图匹配（规定化）定义：用于生成具有规定直方图的图像的方法称为直方图匹配（规定化）。 直方图规定化的推导过程较为复杂，请参考原书。本笔记中，直接看特定的一幅图像经过直方图均衡化和规定化的对比结果。 1，直方图均衡化后的效果图如下所示（有噪声）： 2，直方图规定化后的效果图如下所示（无噪声）: 3.3.3，局部直方图处理 直方图均衡化和直方图规定化的直方图处理方法都是全局性的，因为像素是由基于整个图像的灰度分布的变换函数修改的。当目的是为了解决增强图像中几个小区域的细节时，解决方法是设计基于像素邻域的灰度分布的变换函数。 局部直方图的处理过程: 定义一个邻域，并将其中心在水平方向或垂直方向上从一个像素移动到另一个像素。 在每个位置，计算邻域中的各点的直方图，得到直方图均衡化或直方图规定化变换函数，将这个函数映射于邻域中心点像素的灰度。 然后将邻域的中心移到一个相邻像素位置，并重复上述过程。 局部直方图均衡化效果如下图所示: 3.3.4，使用直方图统计增强图像 直接从图像直方图得到的统计量信息可用于增强图像。令$r$是一个离散型随机变量，它表示区间$[0,L-1]$内的灰度值；令$p(r_i)$是相对于灰度值$r_i$的归一化直方图分量。即$p(r_i)$可视为灰度$r_i$的概率密度函数，并可得到图像的直方图。 1，均值是平均灰度的测度，图像像素灰度的均值$m$计算公式如下: $$m = \\sum{i=0}^{L-1}r{i}p(r_i)$$ 2，方差（或标准差$\\sigma$）是图像对比度的测度，方差公式如下: $$\\sigma^2 = \\mu2 = \\sum{i=0}^{L-1}(r_{i}-m)^{2}p(r_i)$$ 简单理解图像灰度均值和方差的意义就是，均值越大，图像越亮；方差越大，图像对比度越高。 上述公式是针对图像全局的，其同样可应用于一个规定大小的邻域空间。结合以上公式和概念可以使用直方图统计增强局部图像，示例如下: 令$f(x,y)$表示原图在图像坐标$(x,y)$处的灰度值，令$g(x,y)$表示增强后的图像在这些坐标处的灰度值，具体的图像增强公式如下: 更详细的具体操作方法细节参考原书，本文略过。 3.4，空间滤波基础 本节内容讨论如何使用空间滤波器进行图像处理。滤波有时要分多个阶段完成。 滤波器一词来自频率域处理（第四章），滤波的意思是指通过修改或抑制图像的规定分量。例如，通过低频的滤波器称为低通滤波器。低通滤波器的作用是通过模糊图像来平滑图像，使用空间滤波器可以直接对图像本身进行类似效果的平滑处理。 3.4.1，线性空间滤波的原理 线性空间滤波定义: 指图像$f$与滤波器核$w$进行乘积之和（卷机）运算。核是一个阵列，其大小定义了运算的邻域，其系数决定了该滤波器（也称模板、窗口滤波器）的性质。 下图3.28说明了使用$3\\times3$核进行线性空间滤波的原理。在图像任何一点$(x,y)$处，滤波器的响应$g(x,y)$是核系数核核所覆盖图像像素的乘积之和: $$g(x,y) = w(-1,-1)f(x-1,y-1)+w(-1,0)f(x-1,y)+...+w(0,0)f(x,y)+...+w(1,1)f(x+1,y+1) \\tag{3.30}$$ 核的中心系数值 $w(0,0)$ 对应于 $(x,y)$ 处的像素。对应大小为 $m\\times n$ 的核，假设 $m=2a+1$ 和 $n=2b+1$，其中$a$和$b$是非负整数。一般来说，大小为 $m\\times n$ 的核对大小为 $M\\times N$ 的图像的线性空间滤波可表示为: $$g(x,y) = \\sum{s=-a}^{a}\\sum{t=-b}^{b} w(s,t)f(x+s, y+t) \\tag{3.31}$$ 上式中 $x$ 和 $y$ 发生变化，使得滤波器核的中心（原点）能够遍历完图像 $f$ 中的每个像素。 3.4.2，空间相关与卷积 图3.28以图形方式说明了空间相关，式(3.31)给出了其数学描述。相关的运算过程如下：在图像上移动核中心，并且在每个位置计算乘积之和。 以一维例子开始，则式(3.31)变为 $$g(x) = \\sum_{s=-1}^{a}w(s)f(x+s)$$ 式中，卷机核为$w$，图像函数为$f$。 在本书中，当我们使用线性空间滤波这个术语时，指的是滤波器核与图像进行卷机（乘积和）运算。 3.5，平滑（低通）空间滤波器 平滑（也称平均）空间滤波器用于降低灰度的急剧过渡，因为随机噪声通常就是由灰度的急剧过渡组成，所以平滑的一个明显应用就是降噪。 本节介绍基于可分离盒式核和高斯核的低通滤波器。 1，基于可分离盒式核的低通滤波器 最简单的可分离低通滤波器核是盒式核，其系数的值相同（通常为1）。 下图3.31(a)中显示了一个大小为$3\\times 3$的盒式滤波器，即一个大小为$m\\times n$的盒式滤波器且元素值为 1 的一个$m\\times n$阵列，器前面有一个归一化的常数，它的值是 1 除以系数值之和（当所有系数都为1时，这个常数为$1/mn$）。 使用不同盒式核对图像进行低通滤波的效果图如下所示: 盒式滤波器较为简单，适合快速实验，其会产生视觉上能够接受的平滑效果。盒式滤波器有一些局限: 对透镜模糊特性的近视能力较差。 盒式滤波器往往沿垂直方向模糊图像，不适合精细细节或具有强几何分量的图像应用。 对应的 opencv 函数如下。 void boxFilter( InputArray f, OutputArray dst, int ddepth, Size ksize, Point anchor=Point(-1,-1), bool normalize=true, int borderType=BORDER_DEFAULT ); 参数 6 的解释： 当 normalize = true 时，盒式滤波就变成了均值滤波。也就是说，均值滤波是盒式滤波归一化（normalized）后的特殊情况。其中，归一化就是把要处理的量都缩放到一个范围内，比如(0,1)，以便统一处理和直观量化。 当 normalize = false 时，为非归一化的盒式滤波，用于计算每个像素邻域内的积分特性，比如密集光流算法（dense optical flow algorithms）中用到的图像倒数的协方差矩阵（covariance matrices of image derivatives）。 均值滤波，是最简单的一种线性滤波操作，输出图像的每一个像素是核窗口内输入图像对应像素的像素的平均值( 所有像素加权系数相等)，实质就是归一化后的方框滤波。均值滤波算法比较简单，计算速度快，但是均值滤波本身存在着固有的缺陷，即它不能很好地保护图像细节，在图像去噪的同时，也破坏了图像的细节部分，从而使图像变得模糊，不能很好地去除噪声点。但均值滤波对周期性的干扰噪声有很好的抑制作用。 3.5.2，低通高斯滤波器核 实际应用中要求卷积核是各向同性的（圆对称），其响应与方向无关。高斯核是唯一可分离的圆对称核，因此非常适合图像处理，对于抑制服从正态分布的噪声非常有效。高斯核定义: $$w(s,t) = G(s,t) = Ke^{-\\frac{s^2+t^2}{2\\sigma^2}}$$ 令$r=[s^2+t^2]^{1/2}$，上式可写为 $$G(r) = Ke^{-\\frac{r^2}{2\\sigma^2}}$$ 这个函数依然是圆对称的，变量$r$表示从中心到函数$G$上任意一点的距离，它必须是正数且是奇数。下图显示了不同大小的核的$r$值。 希望产生更均匀的平滑结果时，通常使用高斯核平滑；盒式核平滑则是硬过渡。 高斯核和盒式核平滑特性的比较对比图如下: 3.5.3，低通滤波代码示例 盒式滤波、均值滤波（归一化后的盒式滤波）、高斯滤波的 python-opencv 应用代码如下。 import cv2 as cv import numpy as np from matplotlib import pyplot as plt img = cv.imread('test.png') blur1 = cv.boxFilter(img, -1 ,(3,3),normalize = False) blur2 = cv.boxFilter(img, -1 ,(3,3),normalize = True) blur3 = cv.GaussianBlur(img,(5,5),0) plt.figure(figsize=(20,20)) #设置窗口大小 plt.subplot(221),plt.imshow(img),plt.title('Original') plt.xticks([]), plt.yticks([]) plt.subplot(222),plt.imshow(blur1),plt.title('boxFilter_normalize_false') plt.xticks([]), plt.yticks([]) plt.subplot(223),plt.imshow(blur2),plt.title('boxFilter_normalize_true') plt.xticks([]), plt.yticks([]) plt.subplot(224),plt.imshow(blur3),plt.title('Gaussian') plt.xticks([]), plt.yticks([]) plt.show() 程序运行后输出如下： 非归一化的时候，得到图像就是一片白色，对源图像毁坏太大，根本无法使用。而归一化的时候，得到图像是一种模糊的效果，此时与均值滤波一样。 3.6，锐化（高通）空间滤波器 锐化的作用是突出灰度中的过渡。前面讲的平滑（低通）滤波（图像模糊）通过积分运算实现的，那可以推断出图像锐化可以用微分实现。 3.6.1，基础 导数（英语：derivative）是微积分学中的一个概念。函数在某一点的导数是指这个函数在这一点附近的变化率。导数的本质是通过极限的概念对函数进行局部的线性逼近。当函数$f$的自变量在一点 $x{0}$ 上产生一个增量 $h$ 时，函数输出值的增量与自变量增量 $h$ 的比值在 $h$ 趋于 0 时的极限如果存在，即为 $f$ 在 $x{0}$ 处的导数，记作 $f'(x{0})$ 或 $\\frac{\\mathrm{d} f}{\\mathrm{d} x}x_0$ 或 $\\frac{\\mathrm{d} f}{\\mathrm{d} x}\\mid{x=x_0}$。例如在运动学中，物体的位移对于时间的导数就是物体的瞬时速度。 导数是函数的局部性质。不是所有的函数都有导数，一个函数也不一定在所有的点上都有导数。若某函数在某一点导数存在，则称其在这一点可导(可微分)，否则称为不可导(不可微分)。如果函数的自变量和取值都是实数的话，那么函数在某一点的导数就是该函数所代表的曲线在这一点上的切线斜率。 导数一般定义如下： 直观上 $f(x)-f(a)$ 代表函数值从$a$到$x$的变化量，那么， $$\\frac{f(x)-f(a)}{x-a}$$ 代表的是从 $a$ 到 $x$ 的平均变化率。若实函数$f$于实数$a$有定义，且以下极限: $$\\lim_{x\\rightarrow a} \\frac{f(x)-f(a)}{x-a}$$ 存在则称 $f$ 在 $a$ 处可导。 来源维基百科导数定义。 一维函数$f(x)$的一阶导数的一个基本定义是差分： $$\\frac{\\partial f}{\\partial x} = f(x+1)-f(x) \\tag{3.48}$$ 同理可将$f(x)$的二阶导数定义为差分: $$\\frac{\\partial ^{2}f}{\\partial x^{2}} = f(x+1)+f(x-1)-2f(x) \\tag{3.49}$$ 3.6.2，使用二阶导数锐化图像-拉普拉斯 使用二阶导数锐化图像的方法：首先定义二阶导数的离散公式，然后在这个公式的基础上构造一个滤波器核。对于两个变量的函数（图像$f(x,y)$）其中拉普拉斯算子（核）的定义如下: $$ \\Delta f= \\frac{\\partial ^{2}f}{\\partial x^{2}}+ \\frac{\\partial ^{2}f}{\\partial y^{2}} \\tag{3.50}$$ 因为任意阶的导数都是线性算子，所以拉普拉斯是线性算子。 由差分和拉普拉斯二阶导数的定义可得如下公式: $$\\Delta f = f(x+1, y) + f(x-1, y) + f(x, y+1) + f(x, y-1)-4f(x,y) \\tag{3.53}$$ 上述公式可以用图3.45(a)中的核进行卷积运算来实现；因此，图像锐化的滤波原理类似于3.5节内容所述的低通滤波，只是这里使用不同的核系数。 使用拉普拉斯算子锐化图像的方法描述如下: 直接的拉普拉斯图像往往是黑色和无特征的。 拉普拉斯应用示例如下: 3.6.3，钝化掩蔽和高提升滤波 从原图像减去一幅钝化（平滑后）图像，是20世纪30年代以来印刷和出版业一直用来锐化图像的方法，这个过程称为钝化掩蔽，步骤如下： 模糊原图像。 从原图像减去模糊后的图像（产生的差称为模板）。 将模板与原图像相加。 注意较大的 k 值会产生令人无法接受的图像。 钝化掩蔽效果示例： 3.6.4，使用一阶导数锐化图像-梯度 包括罗伯特交叉梯度算子和 Sobel 算子，使用一阶梯度算子锐化图像常用在工业缺陷检测。梯度算子核系数矩阵如下: 3.7，低通、高通、带阻核带通滤波器 3.8，组合使用空间增强方法 使用拉普拉斯来突出细节，使用梯度来增强突出的边缘。 四，频率域滤波 4.1，背景 傅里叶级数: 任何周期函数都可表示为不同频率的正弦函数和/或余弦函数之和，其中每个正线函数和/或余弦函数都乘以不同的系数（我们现在称该和为傅里叶级数）。 傅里叶变换: 任何非周期函数都可表示为正弦函数和/或余弦函数乘以加权函数的积分。 略。 4.2，基本概念 复数 傅里叶级数 冲激函数及其取样（筛选）性质 连续单变量函数的傅里叶变换 卷积 4.3，取样和取样函数的傅里叶变换 略。 4.8，使用低通频率域滤波器平滑图像 图像中的边缘和其他急剧的灰度变化（如噪声）主要影响其傅里叶变换的高频内容，因此，在频率域中是通过衰减高频（即低通滤波）来实现平滑（模糊）的。 五，图像复原与重建 图像增强主要是一种主观处理，而图像复原很大程度上是一种客观处理技术。 六，彩色图像处理 6.1，彩色基础 光的特性是色彩科学的核心，如果光是无色（无颜色）的，那么其属性就只有亮度或数值，具体体现就是20世纪20年度的黑白电影。其中术语灰度（或亮度）级是关于（从黑色变为灰色最终变为白色）灰度的一个测量，是一个标量。 彩色光在电磁波谱中的波长范围是 400～700nm，描述光源质量的 3 个基本量是辐射亮度、发光强度和亮度。人眼的生理结构特性表现为其对红色、绿色和蓝色光更为敏感，该特性使得人眼看到的颜色是三原色 [红 R、绿 G、蓝 B] 的不同组合。三原色相加可以产生光的二次色，如深红色（R+B）、青色（G+B）和黄色（R+G）。 区分不同颜色的特性通常是亮度、色调和饱和度，色调与饱和度一起称为色度。 亮度：亮度体现的是发光强度（灰度级）的消色概念。 色调：色调是混合光波中与主导波长相关的属性，表示被观察者感知的主导色。即，当我们说一个物体颜色说红色、橙色或红色时，说的就是物体的色调。 饱和度：饱和度指的是相对的纯度（纯色被白光稀释的程度），或与一种色调混合的白光量。纯光谱颜色是完全饱和的，非光谱颜色如淡紫色（紫色加白色）是不饱和度，饱和度与所加白光量成反比。 6.2，彩色模型 彩色模型（也称彩色空间或彩色系统）的目的是以某种标准的方式来方便地规定颜色。彩色模型本质上规定：（1）坐标系；（2）坐标系内的字空间，模型内的每种颜色都可由字空间内包含的一个点来表示。数字图像处理中的面向硬件的彩色模型常用有以下几种： 针对彩色显示器和彩色摄像机开发的 RGB（红色、绿色、蓝色）模型。 针对彩色打印机开发的 CMY（青色、深红色、黄色）模型和 CMYK（青色、深红色、黄色、黑色）模型。 针对人们描述和解释颜色的方式开发的 HSI （色调、饱和度、亮度）模型。HSI模型有一个优点：它能够解除图像中颜色和灰度级信息的联系。 6.2.1，RGB 彩色模型 在 RGB 模型中，每种颜色都以其红色、绿色和蓝色光谱成分显示，该模型是根据笛卡尔坐标系建立的。RGB 彩色立方体简图如下所示： 全彩色图像通常是指一幅 24 比特的 RGB 彩色图像（每幅分量图像都是 8 位，值域是 [0, 255]），颜色总数自然是 $(2^8)^3$=16777216。 6.2.2，HSI 彩色模型 RGB 、CMY 和 CMYK 模型适合硬件实现但是并不能很好的描述人类世纪解释的颜色；观察彩色物体时，我们会用色调、饱和度和亮度来描述这个物体，由此提出了 HSI 彩色模型（色调、饱和度、亮度）。 从 RGB 到 HSI 的彩色变换的公式如下： 6.4，全彩色图像处理基础 全彩色图像处理方法主要分为两种。第一种方法是首先分布处理每幅灰度级分量图像，然后将处理后的各幅分量图像合成为一幅彩色图像。第二种方法是直接处理彩色像素。因为全彩色图像至少有 3 个分量，因此彩色像素是向量。全彩色图像的公式定义如下： 6.5，彩色变换 本节中描述的技术是在单个彩色模型中处理彩色图像的各个分量，而不是像6.2节中那样在彩色模型之间进行彩色变换。 6.5.1，公式 使用如下公式对多光谱图像的颜色变换建模: $$s_i = T_i(r_i), i=1,2,...,n$$ 式中，$n$是分量图像的总数，$r_i$是输入分量图像的灰度值，$s_i$是输出分量图像中空间上的对应灰度，$T_i$是对$r_i$操作来产生$s_i$的一组变换或颜色映射函数。 6.5.4，色调和彩色校正 只有校正了图像的色调范围，才能解决图像中颜色的不规则问题，如过饱和颜色和欠饱和颜色问题。图像中的色调范围（也称主特性），是指图像中颜色亮度的一般分布。高主特性图像中的大部分信息集中在高亮度上；低主特性图像中的大部分信息集中在低亮度上；中主特性图像的颜色则介于前两者之间。我们通常希望彩色图像的亮度在高光和阴影之间是等间隔分布的。 彩色平衡。图像的色调特性完成之后，通常要解决彩色不平衡的问题。校正彩色不平衡的方法有很多种，调整一度彩色图像的彩色分量时，要意识到每个操作都会影响图像的整体彩色平衡。也就是说，对一种颜色的感知会受到周围颜色的影响。 6.5.5，彩色图像的直方图处理 和前一节内容是交互式增强方法（手动调节感知合适与否）不同，3.3 节的灰度直方图处理变换可自动地应用于彩色图像。 直方图均衡化会通过一个变换函数，输出一幅具有均匀灰度值直方图的图像。 HSI 彩色空间更适合均匀地分布颜色亮度，同时保持颜色本身（即色调）不变。 6.6，彩色图像平滑和锐化 彩色变换是变换图像中的每个像素而不考虑像素的邻域，那么下一步自然是根据周围像素（邻域）的特性来修改像素的值。本节内容通过彩色图像的平滑和锐化处理来说明这类邻域处理的基本知识。 6.6.1，彩色图像的平滑 灰度级图像的平滑（空间滤波）概念同样可适用于全彩色图像处理，只是我们处理的不再是标量灰度值，而是式(6.37)给出的分量向量。彩色图像平滑公式描述如下: 6.6.2，彩色图像的锐化 向量分析表明，一个向量的拉普拉斯也是一个向量，其分量等于输入向量的各个标量分量的拉普拉斯。在 RGB 彩色系统中，式(6.37)中向量$c$的拉普拉斯为 $$\\bigtriangledown^2 [c(x,y)] = \\begin{bmatrix} \\bigtriangledown^2 R(x,y)\\ \\bigtriangledown^2 G(x,y)\\ \\bigtriangledown^2 B(x,y) \\end{bmatrix}$$ 上式(6.37)表明分别计算每幅图像的拉普拉斯，即可求出全彩色图像的拉普拉斯。使用拉普拉斯锐化彩色图像示例如下: 八，图像压缩和水印 图像压缩是一种减少表示一幅图像所需数据量的技术与课业。本章内容是介绍性的，适用于图像和视频应用。 8.1，基础 二维灰度矩阵是我们查看和解释图像的首选格式，并且是评判其他表示的标准。二维灰度阵列受如下能被识别和利用的三种主要数据冗余的影响： 编码冗余。 空间和时间冗余。 无关信息。 8.1.7，图像格式、存储器（容器）和压缩标准 在数字图像处理环境中，图像文件格式是组织和存储图像数据的标准方法，它定义了数据的排列方式和所用的压缩类型。一些常用的图像压缩标准、文件格式和容器如下图所示: 8.2，霍夫曼编码 霍夫曼提出了消除编码冗余的一种常用技术。 1，霍夫曼编码过程第1步如下: 2，霍夫曼编码过程第2步如下: 8.12，数字图像水印 简单的可见水印定义: 若令 $f_w$ 表示添加了水印的图像，则可将它表示为未加水印图像$f$和水印$w$的线性组合: $$f_w = (1-\\alpha)f + \\alpha$$ 式中，常数$\\alpha$控制水印和底层图像的相对可见度。一个简单的可见水印示例图如下: 参考资料 《数字图像处理第四版》 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/数字图像处理/1-数字图像处理基础.html":{"url":"6-computer_vision/数字图像处理/1-数字图像处理基础.html","title":"1-数字图像处理基础","keywords":"","body":"数字图像处理基础 相机成像的原理 相机成像的原理：针孔相机( Pinhole Camera )通过投影变换，可以将三维相机（Camera）坐标转换为二维的图像坐标，这个变换矩阵是相机的内在属性，称为相机内参（Camera Intrinsic） K。 yaw 航向角，pitch 俯仰角，roll 翻滾角 均值模糊与高斯模糊 均值模糊（不常用） 所谓\"模糊\"，可以理解成每一个像素都取周边像素的平均值(这种叫均值模糊)，在图形上，就相当于产生\"模糊\"效果，但是在数值上，这是一种\"平滑化\"，\"中间点\"会失去细节。（高斯模糊效果如下图所示：） 注意这里，计算平均值时，取值范围越大，\"模糊效果\"越强烈，也就是模糊半径越大，图像就越模糊。从数值角度看，就是数值越平滑。下图是分别是原图、模糊半径 3 像素、模糊半径 10 像素的效果： 高斯模糊 从数学的角度来看，图像的高斯模糊过程就是图像像素与像素的正态分布做卷积, 同时高斯模糊对图像来说就是一个低通滤波器。简单来说高斯模糊就是一种图像模糊滤波器，它用正态分布计算图像中每个像素的变换。其中N维空间正态分布方程为： 在二维空间定义为： 其中 $r$ 是模糊半径 $(r^2 = u^2 + v^2）$，$σ$ 是正态分布的标准偏差。在二维空间中，这个公式生成的曲面的等高线是从中心开始呈正态分布的同心圆。分布不为零的像素组成的卷积矩阵与原始图像做变换。每个像素的值都是周围相邻像素值的加权平均。原始像素的值有最大的高斯分布值，所以有最大的权重，相邻像素随着距离原始像素越来越远，其权重也越来越小。这样进行模糊处理比其它的均衡模糊滤波器更高地保留了边缘效果，效果如下图所示： 图像处理中平滑和锐化操作 平滑处理（smoothing）也称模糊处理（bluring），主要用于消除图像中的噪声部分，平滑处理常用的用途是用来减少图像上的噪点或失真，平滑主要使用图像滤波。在这里，我个人认为可以把图像平滑和图像滤波联系起来，因为图像平滑常用的方法就是图像滤波器。在OpenCV3中常用的图像滤波器有以下几种： 方框滤波——BoxBlur函数 均值滤波（邻域平均滤波）——Blur函数 高斯滤波——GaussianBlur函数（高斯低通滤波是模糊，高斯高通滤波是锐化） 中值滤波——medianBlur函数 双边滤波——bilateralFilter函数 图像锐化操作是为了突出显示图像的边界和其他细节，而图像锐化实现的方法是通过各种算子和滤波器实现的——Canny 算子、Sobel 算子、Laplacian 算子以及 Scharr 滤波器。 图像锐化方法 锐化主要影响图像中的低频分量，不影响图像中的高频分量像锐化的主要目的有两个： 增强图像边缘，使模糊的图像变得更加清晰，颜色变得鲜明突出，图像的质量有所改善，产生更适合人眼观察和识别的图像； 过锐化处理后，目标物体的边缘鲜明，以便于提取目标的边缘、对图像进行分割、目标区域识别、区域形状提取等，进一步的图像理解与分析奠定基础。 图像锐化一般有两种方法： 微分法 高通滤波法 一般来说，图像的能量主要集中在其低频部分，噪声所在的频段主要在高频段，同时图像边缘信息也主要集中在其高频部分。这将导致原始图像在平滑处理之后，图像边缘和图像轮廓模糊的情况出现。为了减少这类不利效果的影响，就需要利用图像锐化技术，使图像的边缘变得清晰。图像锐化处理的目的是为了使图像的边缘、轮廓线以及图像的细节变得清晰. 经过平滑的图像变得模糊的根本原因是因为图像受到了平均或积分运算，因此可以对其进行逆运算(如微分运算)就可以使图像变得清晰。微分运算是求信号的变化率，由傅立叶变换的微分性质可知，微分运算具有较强高频分量作用。从频率域来考虑，图像模糊的实质是因为其高频分量被衰减，因此可以用高通滤波器来使图像清晰。但要注意能够进行锐化处理的图像必须有较高的性噪比，否则锐化后图像性噪比反而更低，从而使得噪声增加的比信号还要多，因此一般是先去除或减轻噪声后再进行锐化处理。 Reference 图像增强－图像锐化 高斯模糊算法 高斯模糊numpy手撕代码 人脸识别的场景下，输入 $105\\times 12$ 的 feature map，用这个在$1000 \\times 512$ 的特征库当中用欧氏距离去匹配 $105 \\times 12$ 的 feature map，用这个在 $1000\\times 512$ 的特征库当中用欧氏距离去匹配 $10\\times 1000$ 的特征，得到这个 output。 单目3D目标检测笔记 EfficientNet is a backbone. In the original paper, they used Hourglass backbone, which has a structure similar to UNet. CenterNet is only a type of decoder, not the whole network. 8 outputs, not exactly classes. 1 for binary mask 7 for regression variables x, y, z, yaw, pitch_sin, pitch_cos, roll Regression predicts raw 3d coordinates of a car x, y, z. And binary mask predicts the 2d position on the image row, col. 2d map is much more precise but contains incomplete information. Function optimize_xy is used to shift x, y (on the road) so that they correspond to row, col (in image) CenterNet 模型：作者在构建模型时将目标作为一个点——即目标BBox的中心点。检测器采用关键点估计来找到中心点，并回归到其他目标属性，例如尺寸，3D位置，方向，甚至姿态。 I use this idea to predict x, y, z coordinates of the vehicle and also yaw, pitch_cos, pitch_sin, roll angles. For pitch I predict sin and cos, because, as we will see, this angle can be both near 0 and near 3.14. These 7 parameters are my regression target variables instead of shift_x, shift_y, size_x, size_y In this competition the world coordinate (Xw, Yw, Zw) is same with camera coordinate (Xc, Yc, Zc), the camera is origin. Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/数字图像处理/2-OpenCV3图像处理笔记.html":{"url":"6-computer_vision/数字图像处理/2-OpenCV3图像处理笔记.html","title":"2-OpenCV3图像处理笔记","keywords":"","body":" 此笔记针对 Python 版本的 opencv3，c++ 版本的函数和 python 版本的函数参数几乎一样，只是矩阵格式从 ndarray 类型变成适合 c++ 的 mat 模板类型。注意，因为 python 版本的opncv只提供接口没有实现，故函数原型还是来自 c++版本的opencv，但是参数解释中的数据类型还是和 python 保持一致。 图像的载入：imread() 函数 函数原型： Mat imread(const sting& filename, int flags=None) 参数解释： filename：图像的文件路径，sting 字符串类型 flags：载入标识，以何种方式读取图片，int 类型的 flags。常用取值解释如下： flags = 0：始终将图像转成灰度图再返回 flags = 1：始终将图像转换成彩色图再返回，如果读取的是灰度图，则其返回的矩阵 shape 将变为 (height, width, 3) flags = 2：如果载入的图像深度为 16 位或者 32 位，就返回对应深度的图像，否则，就转换为 8 位图像再返回。 总结：读取文件中的图片到 OpenCV 中，返回 Mat 或者 ndarray 类型的矩阵，以彩色模式载入图像时，解码后的图像会默认以 BGR 的通道顺序进行存储。 cv2.imread()函数： python-opencv 库的 imread 函数的 flags 参数取值方式与 C++ 版有所区别。使用函数 cv2.imread() 读入图像，图像要么在此程序的工作路径，要么函数参数指定了完整路径，第二个参数是要告诉函数应该如何读取这幅图片，取值如下： cv2.IMREAD_COLOR : 取值 1，读入一副彩色图像。图像的透明度会被忽略，这是默认参数。 cv2.IMREAD_GRAYSCALE : 取值 0，以灰度模式读入图像。 cv2.IMREAD_UNCHANGED : 取值 -1，读入一幅图像，并且包括图像的 alpha 通道。 Instead of these three flags, you can simply pass integers 1, 0 or -1 respectively. import numpy as np import cv2 # Load an color image in grayscale img = cv2.imread('messi5.jpg',0) opencv-python 库的读取图像函数 cv2.imread() 官方定义如下图。 图像的显示：imshow()函数 函数原型： void imshow(const string &winname, InputArray mat) 参数解释： winname：需要显示的窗口标识名称，string 字符串类型 mat：需要显示的图像矩阵，ndarray numpy 矩阵类型 总结：imshow 函数用于在指定的窗口显示图像，窗口会自动调整为图像大小。 minMaxLoc 函数 函数 cv :: minMaxLoc 查找最小和最大元素值及其位置，返回的位置坐标是先列号，后行号（列号，行号） 。在整个数组中搜索极值，或者如果mask不是空数组，则在指定的数组区域中搜索极值。（只适合单通道矩阵）。函数原型： CV_EXPORTS_W void minMaxLoc(InputArray src, CV_OUT double* minVal, CV_OUT double* maxVal = 0, CV_OUT Point* minLoc = 0, CV_OUT Point* maxLoc = 0, InputArray mask = noArray()); 函数参数解释： src：input single-channel array. minVal：pointer to the returned minimum value; NULL is used if not required. maxVal：pointer to the returned maximum value; NULL is used if not required. minLoc：pointer to the returned minimum location (in 2D case); NULL is used if not required. maxLoc：pointer to the returned maximum location (in 2D case); NULL is used if not required. 位深度的概念 灰度图的位深度是 16，则其矩阵的元素类型为 uint16 ，彩色图其位深度一般是 24 ，红色占 8 个位、蓝色占 8 个位、绿色占 8 个位，其矩阵的元素类型为 uint8。 位分辨率（ Bit Resolution ）又称色彩深度、色深或位深度，在位图图像或视频视频缓冲区，指一个像素中，每个颜色分量（Red、Green、Blue、Alpha 通道）的比特数。 matplotlib.image.imsave 将灰度图的矩阵保存为图像格式时，其默认保存的图像通道数为 4：RGBA，其中 RGB 三个通道对应的二维矩阵数值完全一样。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/数字图像处理/3-视频编解码基础.html":{"url":"6-computer_vision/数字图像处理/3-视频编解码基础.html","title":"3-视频编解码基础","keywords":"","body":" 一，基本术语 1.1，颜色亮度和我们的眼睛 二，视频编码的实现原理 2.1，视频编码技术概述 2.2，帧类型 2.3，帧内编码（帧内预测） 2.4，帧间编码（帧间预测） 三，实际的视频编码器如何工作 3.1，视频容器（视频数据封装） 3.2，编码器发展历史 3.3，通用编码器工作流程 3.3.1，第一步-图片分区 3.3.2，第二步-预测 3.3.3，第三步-转换 3.3.4，第四步-量化 3.3.5，第五步-熵编码 3.3.6，第六步-比特流格式 参考资料 视频编解码算法分为传统算法和基于深度学习的方法，本文主要介绍基于传统算法的视频编解码技术的原理，部分内容和图片参考网上技术博客（链接已放在文章末尾）。 一，基本术语 数字图像的定义及理解可以参考这篇文章：数字图像处理笔记｜一文搞懂数字图像基础。 颜色深度: 存储每个像素颜色的强度，需要占用一定大小的数据空间，这个空间大小即为颜色深度，对于 RGB 色彩模型，颜色深度是 24 （8*3）bit。 图片分辨率: 图像的像素的数量，通常表示为宽*高。 图像/视频宽高比: 单地描述了图像或像素的宽度和高度之间的比例关系。 比特率: 播放一段视频每秒所需的数据量，比特率 = 宽 * 高 * 颜色深度 * 帧每秒。例如，一段每秒 30 帧，每像素 24 bits，分辨率是 480x240 的视频，如果我们不做任何压缩，它将需要 82,944,000 比特每秒或 82.944 Mbps (30x480x240x24)。当比特率几乎恒定时称为恒定比特率（CBR）；但它也可以变化，称为可变比特率（VBR）。 下面这个图形显示了一个受限的 VBR，当帧为黑色时不会花费太多的数据量。 1.1，颜色亮度和我们的眼睛 因为人眼睛的视杆细胞（亮度）比视锥细胞多很多，所以一个合理的推断是相比颜色，我们有更好的能力去区分黑暗和光亮。 我们的眼睛对亮度比对颜色更敏感，可以看看下面的图片来测试。 看不出左图的方块 A 和方块 B 的颜色是相同的，那是因为我们的大脑玩了一个小把戏，这让我们更多的去注意光与暗，而不是颜色。右边这里有一个使用同样颜色的连接器，那么我们（的大脑）就能轻易分辨出事实，它们是同样的颜色。 二，视频编码的实现原理 2.1，视频编码技术概述 编码的目的是为了压缩，所谓编码算法，就是寻找规律构建一个高效模型，将视频数据中的冗余信息去除。 常见的视频的冗余信息和对应的压缩方法如下表： 种类 内容 压缩方法 空间冗余 像素间的相关性 变换编码，预测编码 时间冗余 时间方向上的相关性 帧间预测，运动补偿 图像构造冗余 图像本身的构造 轮廓编码，区域分割 知识冗余 收发两端对人物共有认识 基于知识的编码 视觉冗余 人对视觉特性 非线性量化，位分配 其他 不确定性因素   视频帧冗余信息示例如下图所示： 2.2，帧类型 我们知道视频是由不同的帧画面连续播放形成的，视频的帧主要分为三类，分别是（1）I 帧；（2）B 帧；（3）P 帧。 I 帧（关键帧，帧内编码）: 是自带全部信息的独立帧，是最完整的画面（占用的空间最大），无需参考其它图像便可独立进行解码。视频序列中的第一个帧，始终都是I帧。 P 帧（预测）: “帧间预测编码帧”，需要参考前面的I帧和/或P帧的不同部分，才能进行编码。P帧对前面的P和I参考帧有依赖性。但是，P帧压缩率比较高，占用的空间较小。 B 帧（双向预测）: “双向预测编码帧”，以前帧后帧作为参考帧。不仅参考前面，还参考后面的帧，所以，它的压缩率最高，可以达到200:1。不过，因为依赖后面的帧，所以不适合实时传输（例如视频会议）。 对 I 帧的处理，是采用帧内编码（帧间预测）方式，只利用本帧图像内的空间相关性。 对 P 帧的处理，采用帧间编码（前向运动估计），同时利用空间和时间上的相关性。简单来说，采用运动补偿(motion compensation)算法来去掉冗余信息。 2.3，帧内编码（帧内预测） 帧内编码/预测用于解决单帧空间冗余问题。如果我们分析视频的每一帧，会发现许多区域是相互关联的。 举个例子来理解帧内编码，如下图所示的图片，可以看出这个图大部分区域颜色是一样的。假设这是一个 I 帧 ，我们即将编码红色区域，假设帧中的颜色在垂直方向上保持一致，这意味着未知像素的颜色与临近的像素相同。 这样的先验预测虽然会出错，但是我们可以先利用这项技术（帧内预测），然后减去实际值，算出残差，这样得出的残差矩阵比原始数据更容易压缩。 2.4，帧间编码（帧间预测） 视频帧在时间上的重复，解决这类冗余的技术就是帧间编码/预测。 尝试花费较少的数据量去编码在时间上连续的 0 号帧和 1 号帧。比如做个减法，简单地用 0 号帧减去 1 号帧，得到残差，这样我们就只需要对残差进行编码。 做减法的方法比较简单粗暴，效果不是很好，可以有更好的方法来节省数据量。首先，我们将0 号帧 视为一个个分块的集合，然后我们将尝试将 帧 1 和 帧 0 上的块相匹配。我们可以将这看作是运动预测。 运动补偿是一种描述相邻帧（相邻在这里表示在编码关系上相邻，在播放顺序上两帧未必相邻）差别的方法，具体来说是描述前面一帧（相邻在这里表示在编码关系上的前面，在播放顺序上未必在当前帧前面）的每个小块怎样移动到当前帧中的某个位置去。” 如上图所示，我们预计球会从 x=0, y=25 移动到 x=7, y=26，x 和 y 的值就是运动向量。进一步节省数据量的方法是，只编码这两者运动向量的差。所以，最终运动向量就是 x=7 (6-0), y=1 (26-25)。使用运动预测的方法会找不到完美匹配的块，但使用运动预测时，编码的数据量少于使用简单的残差帧技术，对比图如下图所示： 三，实际的视频编码器如何工作 3.1，视频容器（视频数据封装） 首先视频编码器和视频容器是不一样的，我们常见的各种视频文件名后缀：.mp4 、.mkv 、.avi 和.mpeg 等其实都是视频容器。视频容器定义：将已经编码压缩好的视频轨和音频轨按照一定的格式放到一个文件中，这个特定的文件类型即为视频容器。 3.2，编码器发展历史 视频编码器的发展历史见下图： 3.3，通用编码器工作流程 虽然视频编码器的已经经历了几十年的发展历史，但是其还是有一个主要的工作机制的。 3.3.1，第一步-图片分区 第一步是将帧分成几个分区，子分区甚至更多。分区的目的是为了更精确的处理预测，在微小移动的部分使用较小的分区，而在静态背景上使用较大的分区。 通常，编解码器将这些分区组织成切片（或瓦片），宏（或编码树单元）和许多子分区。这些分区的最大大小对于不同的编码器有所不同，比如 HEVC 设置成 64x64，而 AVC 使用 16x16，但子分区可以达到 4x4 的大小。 3.3.2，第二步-预测 有了分区，我们就可以在它们之上做出预测。对于帧间预测，我们需要发送运动向量和残差；至于帧内预测，我们需要发送预测方向和残差。 3.3.3，第三步-转换 在我们得到残差块（预测分区-真实分区）之后，我们可以用一种方式变换它，这样我们就知道哪些像素我们应该丢弃，还依然能保持整体质量。这个确切的行为有几种变换方式，这里只介绍离散余弦变换（DCT），其功能如下： 将像素块转换为相同大小的频率系数块。 压缩能量，更容易消除空间冗余。 可逆的，也意味着你可以还原回像素。 我们知道在一张图像中，大多数能量会集中在低频部分，所以如果我们将图像转换成频率系数，并丢掉高频系数，我们就能减少描述图像所需的数据量，而不会牺牲太多的图像质量。 DCT 可以把原始图像转换为频率（系数块），然后丢掉最不重要的系数。 我们从丢掉不重要系数后的系数块重构图像，并与原始图像做对比，如下图所示。 可以看出它酷似原图像，与原图相比，我们丢弃了67.1875%，而如何智能的选择丢弃系数则是下一步要考虑的问题。 3.3.4，第四步-量化 当我们丢掉一些（频率）系数块时，在最后一步（变换），我们做了一些形式的量化。这一步，我们选择性地剔除信息（有损部分）或者简单来说，我们将量化系数以实现压缩。 如何量化一个系数块？一个简单的方法是均匀量化，我们取一个块并将其除以单个的值（10），并舍入值。 那如何逆转（反量化）这个系数块呢？可以通过乘以我们先前除以的相同的值（10）来做到。 均匀量化并不是一个最好的量化方案，因为其并没有考虑到每个系数的重要性，我们可以使用一个量化矩阵来代替单个值，这个矩阵可以利用 DCT 的属性，多量化右下部，而少（量化）左上部，JPEG 使用了类似的方法，我们可以通过查看源码看看这个矩阵。 3.3.5，第五步-熵编码 在我们量化数据（图像块／切片／帧）之后，我们仍然可以以无损的方式来压缩它。有许多方法（算法）可用来压缩数据： VLC 编码 算术编码 3.3.6，第六步-比特流格式 完成上述步骤，即已经完成视频数据的编码压缩，后续我们需要将压缩过的帧和内容打包进去。需要明确告知解码器编码定义，如颜色深度，颜色空间，分辨率，预测信息（运动向量，帧内预测方向），档次*，级别*，帧率，帧类型，帧号等等更多信息。 参考资料 digital_video_introduction 零基础，史上最通俗视频编码技术入门 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/2D目标检测/0-目标检测模型的基础.html":{"url":"6-computer_vision/2D目标检测/0-目标检测模型的基础.html","title":"0-目标检测模型的基础","keywords":"","body":" 前言 一，anchor box 二，IOU 三，Focal Loss 3.1，Cross Entropy 3.2，Balanced Cross Entropy 3.3，Focal Loss Definition 四，NMS 4.1，NMS 介绍 4.2，NMS 算法过程 五，Soft NMS 算法 六，目标检测的不平衡问题 6.1，介绍 参考资料 前言 边界框：在⽬标检测⾥，我们通常使⽤边界框（bounding box，缩写是 bbox）来描述⽬标位置。边界框是⼀个矩形框，可以由矩形左上⻆的 x 和 y 轴坐标与右下⻆的 x 和 y 轴坐标确定。 检测网络中的一些术语解释： backbone：翻译为主干网络，主要指用来做特征提取作用的网络，早期分类网络 VGG、ResNet 等去掉用于分类的全连接层的部分就是 backbone 网络。 neck: 指放在 backbone 和 head 之间的网络，作用是更好的融合/利用 backbone 提取的 feature，可以理解为特征增强模块，典型的 neck 是如 FPN 结构。 head：检测头，输出想要结果（分类+定位）的网络，放在模型最后。如 YOLO 使用特定维度的 conv 获取目标的类别和 bbox 信息。 一，anchor box ⽬标检测算法通常会在输⼊图像中采样⼤量的区域，然后判断这些区域中是否包含我们感兴趣的⽬标，并调整区域边缘从而更准确地预测⽬标的真实边界框（ground-truth bounding box）。不同的模型使⽤的区域采样⽅法可能不同。两阶段检测模型常用的⼀种⽅法是：以每个像素为中⼼⽣成多个⼤小和宽⾼⽐（aspect ratio）不同的边界框。这些边界框被称为锚框（anchor box）。 在 Faster RCNN 模型中，每个像素都生成 9 个大小和宽高比都不同的 anchors。在代码中，anchors 是一组由 generate_anchors.py 生成的矩形框列表。其中每行的 4 个值 (x1,y1,x2,y2) 表示矩形左上和右下角点坐标。9 个矩形共有 3 种形状，长宽比为大约为 {1:1, 1:2, 2:1} 三种, 实际上通过 anchors 就引入了检测中常用到的多尺度方法。generate_anchors.py 的代码如下： 注意，这里生成的只是 base anchors，其中一个 框的左上角坐标为 (0,0) 坐标（特征图左上角）的 9 个 anchor，后续还需网格化（meshgrid）生成其他 anchor。同一个 scale，但是不同的 anchor ratios 生成的 anchors 面积理论上是要一样的。 import numpy as np import six from six import __init__ # 兼容python2和python3模块 def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]): \"\"\"Generate base anchors by enumerating aspect ratio and scales. Args: base_size (number): The width and the height of the reference window. ratios (list of floats): anchor 的宽高比 anchor_scales (list of numbers): anchor 的尺度 Returns: Base anchors in a single-level feature maps.`(R, 4)`. bounding box is `(x_{min}, y_{min}, x_{max}, y_{max})` \"\"\" import numpy as np py = base_size / 2. px = base_size / 2. anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32) for i in six.moves.range(len(ratios)): for j in six.moves.range(len(anchor_scales)): // 乘以感受野值，得到缩放后的 anchor 大小 h = base_size * anchor_scales[j] * np.sqrt(ratios[i]) w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i]) index = i * len(anchor_scales) + j anchor_base[index, 0] = px - w / 2. anchor_base[index, 1] = py - h / 2. anchor_base[index, 2] = px + h / 2. anchor_base[index, 3] = py + w / 2. return anchor_base # test if __name__ == \"__main__\": bbox_list = generate_anchor_base() print(bbox_list) 程序运行输出如下： [[ -82.50967 -37.254833 53.254833 98.50967 ] [-173.01933 -82.50967 98.50967 189.01933 ] [-354.03867 -173.01933 189.01933 370.03867 ] [ -56. -56. 72. 72. ] [-120. -120. 136. 136. ] [-248. -248. 264. 264. ] [ -37.254833 -82.50967 98.50967 53.254833] [ -82.50967 -173.01933 189.01933 98.50967 ] [-173.01933 -354.03867 370.03867 189.01933 ]] 二，IOU 交并比（Intersection-over-Union，IoU），目标检测中使用的一个概念，是模型产生的候选框（candidate bound）与原标记框（ground truth bound）的交叠率，即它们的交集与并集的比值。最理想情况是完全重叠，即比值为 1。计算公式如下： 代码实现如下： # _*_ coding:utf-8 _*_ # 计算iou \"\"\" bbox的数据结构为(xmin,ymin,xmax,ymax)--(x1,y1,x2,y2), 每个bounding box的左上角和右下角的坐标 输入： bbox1, bbox2: Single numpy bounding box, Shape: [4] 输出： iou值 \"\"\" import numpy as np import cv2 def iou(bbox1, bbox2): \"\"\" 计算两个bbox(两框的交并比)的iou值 :param bbox1: (x1,y1,x2,y2), type: ndarray or list :param bbox2: (x1,y1,x2,y2), type: ndarray or list :return: iou, type float \"\"\" if type(bbox1) or type(bbox2) != 'ndarray': bbox1 = np.array(bbox1) bbox2 = np.array(bbox2) assert bbox1.size == 4 and bbox2.size == 4, \"bounding box coordinate size must be 4\" xx1 = np.max((bbox1[0], bbox2[0])) yy1 = np.min((bbox1[1], bbox2[1])) xx2 = np.max((bbox1[2], bbox2[2])) yy2 = np.min((bbox1[3], bbox2[3])) bwidth = xx2 - xx1 bheight = yy2 - yy1 area = bwidth * bheight # 求两个矩形框的交集 union = (bbox1[2] - bbox1[0])*(bbox1[3] - bbox1[1]) + (bbox2[2] - bbox2[0])*(bbox2[3] - bbox2[1]) - area # 求两个矩形框的并集 iou = area / union return iou if __name__=='__main__': rect1 = (461, 97, 599, 237) # (top, left, bottom, right) rect2 = (522, 127, 702, 257) iou_ret = round(iou(rect1, rect2), 3) # 保留3位小数 print(iou_ret) # Create a black image img=np.zeros((720,720,3), np.uint8) cv2.namedWindow('iou_rectangle') \"\"\" cv2.rectangle 的 pt1 和 pt2 参数分别代表矩形的左上角和右下角两个点, coordinates for the bounding box vertices need to be integers if they are in a tuple, and they need to be in the order of (left, top) and (right, bottom). Or, equivalently, (xmin, ymin) and (xmax, ymax). \"\"\" cv2.rectangle(img,(461, 97),(599, 237),(0,255,0),3) cv2.rectangle(img,(522, 127),(702, 257),(0,255,0),3) font = cv2.FONT_HERSHEY_SIMPLEX cv2.putText(img, 'IoU is ' + str(iou_ret), (341,400), font, 1,(255,255,255),1) cv2.imshow('iou_rectangle', img) cv2.waitKey(0) 代码输出结果如下所示： 三，Focal Loss Focal Loss 是在二分类问题的交叉熵（CE）损失函数的基础上引入的，所以需要先学习下交叉熵损失的定义。 3.1，Cross Entropy 在深度学习中我们常使用交叉熵来作为分类任务中训练数据分布和模型预测结果分布间的代价函数。对于同一个离散型随机变量 $\\textrm{x}$ 有两个单独的概率分布 $P(x)$ 和 $Q(x)$，其交叉熵定义为： P 表示真实分布， Q 表示预测分布。 $$H(P,Q) = \\mathbb{E}{\\textrm{x}\\sim P} log Q(x)= -\\sum{i}P(x_i)logQ(x_i) \\tag{1} $$ 但在实际计算中，我们通常不这样写，因为不直观。在深度学习中，以二分类问题为例，其交叉熵损失（CE）函数如下： $$Loss = L(y, p) = -ylog(p)-(1-y)log(1-p) \\tag{2}$$ 其中 $p$ 表示当预测样本等于 $1$ 的概率，则 $1-p$ 表示样本等于 $0$ 的预测概率。因为是二分类，所以样本标签 $y$ 取值为 ${1,0}$，上式可缩写至如下： $$CE = \\left{\\begin{matrix} -log(p), & if \\quad y=1 \\ -log(1-p), & if\\quad y=0 \\tag{3} \\end{matrix}\\right. $$ 为了方便，用 $p_t$ 代表 $p$，$p_t$ 定义如下： $$p_t = {\\begin{matrix} p, & if \\quad y=1\\ 1-p, & if\\quad y=0 \\end{matrix}$$ 则$(3)$式可写成： $$CE(p, y) = CE(p_t) = -log(p_t) \\tag{4}$$ 前面的交叉熵损失计算都是针对单个样本的，对于所有样本，二分类的交叉熵损失计算如下： $$L = \\frac{1}{N}(\\sum{y_i = 1}^{m}-log(p)-\\sum{y_i = 0}^{n}log(1-p))$$ 其中 $m$ 为正样本个数，$n$ 为负样本个数，$N$ 为样本总数，$m+n=N$。当样本类别不平衡时，损失函数 $L$ 的分布也会发生倾斜，如 $m \\ll n$ 时，负样本的损失会在总损失占主导地位。又因为损失函数的倾斜，模型训练过程中也会倾向于样本多的类别，造成模型对少样本类别的性能较差。 再衍生以下，对于所有样本，多分类的交叉熵损失计算如下： $$L = \\frac{1}{N} \\sumi^N L_i = -\\frac{1}{N}(\\sum_i \\sum{c=1}^M y{ic}log(p{ic})$$ 其中，$M$ 表示类别数量，$y{ic}$ 是符号函数，如果样本 $i$ 的真实类别等于 $c$ 取值 1，否则取值 0; $p{ic}$ 表示样本 $i$ 预测为类别 $c$ 的概率。 对于多分类问题，交叉熵损失一般会结合 softmax 激活一起实现，PyTorch 代码如下，代码出自这里。 import numpy as np # 交叉熵损失 class CrossEntropyLoss(): \"\"\" 对最后一层的神经元输出计算交叉熵损失 \"\"\" def __init__(self): self.X = None self.labels = None def __call__(self, X, labels): \"\"\" 参数： X: 模型最后fc层输出 labels: one hot标注，shape=(batch_size, num_class) \"\"\" self.X = X self.labels = labels return self.forward(self.X) def forward(self, X): \"\"\" 计算交叉熵损失 参数： X：最后一层神经元输出，shape=(batch_size, C) label：数据onr-hot标注，shape=(batch_size, C) return： 交叉熵loss \"\"\" self.softmax_x = self.softmax(X) log_softmax = self.log_softmax(self.softmax_x) cross_entropy_loss = np.sum(-(self.labels * log_softmax), axis=1).mean() return cross_entropy_loss def backward(self): grad_x = (self.softmax_x - self.labels) # 返回的梯度需要除以batch_size return grad_x / self.X.shape[0] def log_softmax(self, softmax_x): \"\"\" 参数: softmax_x, 在经过softmax处理过的X return: log_softmax处理后的结果shape = (m, C) \"\"\" return np.log(softmax_x + 1e-5) def softmax(self, X): \"\"\" 根据输入，返回softmax 代码利用softmax函数的性质: softmax(x) = softmax(x + c) \"\"\" batch_size = X.shape[0] # axis=1 表示在二维数组中沿着横轴进行取最大值的操作 max_value = X.max(axis=1) #每一行减去自己本行最大的数字,防止取指数后出现inf，性质：softmax(x) = softmax(x + c) # 一定要新定义变量，不要用-=，否则会改变输入X。因为在调用计算损失时，多次用到了softmax，input不能改变 tmp = X - max_value.reshape(batch_size, 1) # 对每个数取指数 exp_input = np.exp(tmp) # shape=(m, n) # 求出每一行的和 exp_sum = exp_input.sum(axis=1, keepdims=True) # shape=(m, 1) return exp_input / exp_sum 3.2，Balanced Cross Entropy 对于正负样本不平衡的问题，较为普遍的做法是引入 $\\alpha \\in(0,1)$ 参数来解决，上面公式重写如下： $$CE(p_t) = -\\alpha log(p_t) = \\left{\\begin{matrix} -\\alpha log(p), & if \\quad y=1\\ -(1-\\alpha)log(1-p), & if\\quad y=0 \\end{matrix}\\right.$$ 对于所有样本，二分类的平衡交叉熵损失函数如下： $$L = \\frac{1}{N}(\\sum{y_i = 1}^{m}-\\alpha log(p)-\\sum{y_i = 0}^{n}(1 - \\alpha) log(1-p))$$ 其中 $\\frac{\\alpha}{1-\\alpha} = \\frac{n}{m}$，即 $\\alpha$ 参数的值是根据正负样本分布比例来决定的， 3.3，Focal Loss Definition 虽然 $\\alpha$ 参数平衡了正负样本（positive/negative examples），但是它并不能区分难易样本（easy/hard examples），而实际上，目标检测中大量的候选目标都是易分样本。这些样本的损失很低，但是由于难易样本数量极不平衡，易分样本的数量相对来讲太多，最终主导了总的损失。而本文的作者认为，易分样本（即，置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本（这个假设是有问题的，是 GHM 的主要改进对象） Focal Loss 作者建议在交叉熵损失函数上加上一个调整因子（modulating factor）$(1-p_t)^\\gamma$，把高置信度 $p$（易分样本）样本的损失降低一些。Focal Loss 定义如下： $$FL(p_t) = -(1-p_t)^\\gamma log(p_t) = {\\begin{matrix} -(1-p)^\\gamma log(p), & if \\quad y=1\\ -p^\\gamma log(1-p), & if\\quad y=0 \\end{matrix}$$ Focal Loss 有两个性质： 当样本被错误分类且 $p_t$ 值较小时，调制因子接近于 1，loss 几乎不受影响；当 $p_t$ 接近于 1，调质因子（factor）也接近于 0，容易分类样本的损失被减少了权重，整体而言，相当于增加了分类不准确样本在损失函数中的权重。 $\\gamma$ 参数平滑地调整容易样本的权重下降率，当 $\\gamma = 0$ 时，Focal Loss 等同于 CE Loss。 $\\gamma$ 在增加，调制因子的作用也就增加，实验证明 $\\gamma = 2$ 时，模型效果最好。 直观地说，调制因子减少了简单样本的损失贡献，并扩大了样本获得低损失的范围。例如，当$\\gamma = 2$ 时，与 $CE$ 相比，分类为 $p_t = 0.9$ 的样本的损耗将降低 100 倍，而当 $p_t = 0.968$ 时，其损耗将降低 1000 倍。这反过来又增加了错误分类样本的重要性（对于 $pt≤0.5$ 和 $\\gamma = 2$，其损失最多减少 4 倍）。在训练过程关注对象的排序为正难 > 负难 > 正易 > 负易。 在实践中，我们常采用带 $\\alpha$ 的 Focal Loss： $$FL(p_t) = -\\alpha (1-p_t)^\\gamma log(p_t)$$ 作者在实验中采用这种形式，发现它比非 $\\alpha$ 平衡形式（non-$\\alpha$-balanced）的精确度稍有提高。实验表明 $\\gamma$ 取 2，$\\alpha$ 取 0.25 的时候效果最佳。 网上有各种版本的 Focal Loss 实现代码，大多都是基于某个深度学习框架实现的，如 Pytorch和 TensorFlow，我选取了一个较为清晰的代码作为参考，代码来自 这里。 后续有必要自己实现以下，有时间还要去看看 Caffe 的实现。 # -*- coding: utf-8 -*- # @Author : LG from torch import nn import torch from torch.nn import functional as F class focal_loss(nn.Module): def __init__(self, alpha=0.25, gamma=2, num_classes = 3, size_average=True): \"\"\" focal_loss损失函数, -α(1-yi)**γ *ce_loss(xi,yi) 步骤详细的实现了 focal_loss损失函数. :param alpha: 阿尔法α,类别权重. 当α是列表时,为各类别权重,当α为常数时,类别权重为[α, 1-α, 1-α, ....],常用于 目标检测算法中抑制背景类 , retainnet中设置为0.25 :param gamma: 伽马γ,难易样本调节参数. retainnet中设置为2 :param num_classes: 类别数量 :param size_average: 损失计算方式,默认取均值 \"\"\" super(focal_loss,self).__init__() self.size_average = size_average if isinstance(alpha,list): assert len(alpha)==num_classes # α可以以list方式输入,size:[num_classes] 用于对不同类别精细地赋予权重 print(\" --- Focal_loss alpha = {}, 将对每一类权重进行精细化赋值 --- \".format(alpha)) self.alpha = torch.Tensor(alpha) else: assert alpha mmdetection 框架给出的 focal loss 代码如下（有所删减）： # This method is only for debugging def py_sigmoid_focal_loss(pred, target, weight=None, gamma=2.0, alpha=0.25, reduction='mean', avg_factor=None): \"\"\"PyTorch version of `Focal Loss `_. Args: pred (torch.Tensor): The prediction with shape (N, C), C is the number of classes target (torch.Tensor): The learning label of the prediction. weight (torch.Tensor, optional): Sample-wise loss weight. gamma (float, optional): The gamma for calculating the modulating factor. Defaults to 2.0. alpha (float, optional): A balanced form for Focal Loss. Defaults to 0.25. reduction (str, optional): The method used to reduce the loss into a scalar. Defaults to 'mean'. avg_factor (int, optional): Average factor that is used to average the loss. Defaults to None. \"\"\" pred_sigmoid = pred.sigmoid() target = target.type_as(pred) pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target) focal_weight = (alpha * target + (1 - alpha) * (1 - target)) * pt.pow(gamma) loss = F.binary_cross_entropy_with_logits( pred, target, reduction='none') * focal_weigh return loss 四，NMS 4.1，NMS 介绍 在目标检测中，常会利用非极大值抑制算法(NMS，non maximum suppression)对生成的大量候选框进行后处理，去除冗余的候选框，得到最佳检测框（bbox），以加快目标检测的效，其本质思想搜素局部最大值，抑制非极大值。许多目标检测模型都利用到了 NMS 算法，如 DPM，YOLO，SSD，Faster R-CNN 等。NMS过程如下图所示： 以上图为例，每个选出来的 Bounding Box 检测框（即 BBox）用（x,y,h,w, confidence score，Pdog,Pcat）表示，confidence score 表示 background 和 foreground 的置信度得分，取值范围[0,1]。Pdog, Pcat 分布代表类别是狗和猫的概率。如果是 100 类的目标检测模型，BBox 输出向量为 5+100=105。 4.2，NMS 算法过程 NMS 的目的就是除掉重复的边界框，其主要是通过迭代的形式，不断地以最大得分的框去与其他框做 IoU 操作，并过滤那些 IoU 较大的框。 其实现的思想主要是将各个框的置信度进行排序，然后选择其中置信度最高的框 A，将其作为标准选择其他框，同时设置一个阈值，当其他框 B 与 A 的重合程度超过阈值就将 B 舍弃掉，然后在剩余的框中选择置信度最大的框，重复上述操作。多目标检测的 NMS 算法过程如下： for object in all objects: 1. 将所有 `bboxs` 按照 `confidence` 排序，并标记当前 `confidence` 最大的 `bbox`，即要保留的 bbox； 2. 计算当前最大 confidence 对应的 `bbox` 和剩下所有 `bbox` 的 `IOU`； 3. 去除 `IOU` 大于设定阈值的 `bbox`，得到新的 `bboxs`； 4. 对于新生下来的 `bboxs`，循环执行步骤 `2、3`，直到所有的 `bbox` 都满足要求（即无法再移除 `bbox`）。 nms 的 python 代码如下： import numpy as np def py_nms(bboxs, thresh): \"\"\"Pure Python NMS baseline.注意，这里的计算都是在矩阵层面上计算的 greedily select boxes with high confidence and overlap with current maximum = thresh :param bboxs: [[x1, y1, x2, y2 score],] # ndarray, shape(-1,5) :param thresh: retain overlap 0: # 1, 保留当前 confidence 最大的 bbox加入到返回框列表中 index = order[-1] picked_bboxs.append(bboxs[index]] # 2，计算当前 confidence 最大的 bbox 和剩下 bbox 的 IOU xx1 = np.maximum(x1[-1], x1[order[:-1]]) xx2 = np.maximum(x2[-1], x2[order[:-1]]) yy1 = np.maximum(y1[-1], y1[order[:-1]]) yy1 = np.maximum(y2[-1], y2[order[:-1]]) # 计算相交框的面积,注意矩形框不相交时 w 或 h 算出来会是负数，用0代替 w = np.maximum(0.0, xx2 - xx1 + 1) h = np.maximum(0.0, yy2 - yy1 + 1) overlap_area = w * h IOUs = overlap_area/(areas[index] + areas[order[:-1]] - overlap_area) # 3，只保留 `IOU` 小于设定阈值的 `bbox`，得到新的 `bboxs`，更新剩下来 bbox的索引 remain_index = np.where(IOUs 程序输出如下： [0, 2, 3] [[ 30. 20. 230. 200. 1. ] [210. 30. 420. 5. 0.8] [430. 280. 460. 360. 0.7]] 另一个版本的 nms 的 python 代码如下： from __future__ import print_function import numpy as np import time def intersect(box_a, box_b): max_xy = np.minimum(box_a[:, 2:], box_b[2:]) min_xy = np.maximum(box_a[:, :2], box_b[:2]) inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf) return inter[:, 0] * inter[:, 1] def get_iou(box_a, box_b): \"\"\"Compute the jaccard overlap of two sets of boxes. The jaccard overlap is simply the intersection over union of two boxes. E.g.: A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B) The box should be [x1,y1,x2,y2] Args: box_a: Single numpy bounding box, Shape: [4] or Multiple bounding boxes, Shape: [num_boxes,4] box_b: Single numpy bounding box, Shape: [4] Return: jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]] \"\"\" if box_a.ndim==1: box_a=box_a.reshape([1,-1]) inter = intersect(box_a, box_b) area_a = ((box_a[:, 2]-box_a[:, 0]) * (box_a[:, 3]-box_a[:, 1])) # [A,B] area_b = ((box_b[2]-box_b[0]) * (box_b[3]-box_b[1])) # [A,B] union = area_a + area_b - inter return inter / union # [A,B] def nms(bboxs,scores,thresh): \"\"\" The box should be [x1,y1,x2,y2] :param bboxs: multiple bounding boxes, Shape: [num_boxes,4] :param scores: The score for the corresponding box :return: keep inds \"\"\" if len(bboxs)==0: return [] order=scores.argsort()[::-1] keep=[] while order.size>0: i=order[0] keep.append(i) ious=get_iou(bboxs[order],bboxs[i]) order=order[ious 五，Soft NMS 算法 Soft NMS 算法是对 NMS 算法的改进，是发表在 ICCV2017 的论文 中提出的。NMS 算法存在一个问题是可能会把一些相邻检测框框给过滤掉（即将 IOU 大于阈值的窗口的得分全部置为 0 ），从而导致目标的 recall 指标比较低。而 Soft NMS 算法会为相邻检测框设置一个衰减函数而非彻底将其分数置为零。Soft NMS 算法流程如下图所示： 原来的 NMS 算法可以通过以下分数重置函数来描述： 论文对 NMS 原有的分数重置函数的改进有两种形式，一种是线性加权的。设 $s_i$ 为第 $i$ 个 bbox 的 score, 则在应用 Soft NMS 时各个 bbox score 的计算公式如下： 另一种是高斯加权形式的，其不需要设置 iou 阈值 $N_t$。高斯惩罚系数(与上面的线性截断惩罚不同的是, 高斯惩罚会对其他所有的 bbox 作用)，计算公式图如下： 注意，这两种形式，思想都是 $M$ 为当前得分最高框，$b{i}$ 为待处理框， $b{i}$ 和 $M$ 的 IOU 越大，bbox 的得分 $s{i}$ 就下降的越厉害 ( $N{t}$ 为给定阈值)。Soft NMS 在每轮迭代时，先选择分数最高的预测框作为 $M$，并对 $B$ 中的每一个检测框 $b_i$ 进行 re-score，得到新的 score，当该框的新 score 低于某设定阈值时，则立即将该框删除。 soft nms 的 python 代码如下： def soft_nms(bboxes, Nt=0.3, sigma2=0.5, score_thresh=0.3, method=2): # 在 bboxes 之后添加对应的下标[0, 1, 2...], 最终 bboxes 的 shape 为 [n, 5], 前四个为坐标, 后一个为下标 res_bboxes = deepcopy(bboxes) N = bboxes.shape[0] # 总的 box 的数量 indexes = np.array([np.arange(N)]) # 下标: 0, 1, 2, ..., n-1 bboxes = np.concatenate((bboxes, indexes.T), axis=1) # concatenate 之后, bboxes 的操作不会对外部变量产生影响 # 计算每个 box 的面积 x1 = bboxes[:, 0] y1 = bboxes[:, 1] x2 = bboxes[:, 2] y2 = bboxes[:, 3] scores = bboxes[:, 4] areas = (x2 - x1 + 1) * (y2 - y1 + 1) for i in range(N): # 找出 i 后面的最大 score 及其下标 pos = i + 1 if i != N - 1: maxscore = np.max(scores[pos:], axis=0) maxpos = np.argmax(scores[pos:], axis=0) else: maxscore = scores[-1] maxpos = 0 # 如果当前 i 的得分小于后面的最大 score, 则与之交换, 确保 i 上的 score 最大 if scores[i] Nt] = weight[iou > Nt] - iou[iou > Nt] elif method == 2: # gaussian weight = np.exp(-(iou * iou) / sigma2) else: # original NMS weight = np.ones(iou.shape) weight[iou > Nt] = 0 scores[pos:] = weight * scores[pos:] # select the boxes and keep the corresponding indexes inds = bboxes[:, 5][scores > score_thresh] keep = inds.astype(int) return res_bboxes[keep] 六，目标检测的不平衡问题 论文 Imbalance Problems in Object Detection 给出了详细的综述。这篇论文主要是系统的分析了目标检测中的不平衡问题，并按照问题进行分类，提出了四类不平衡，并对每个问题现有的解决方案批判性的提出了观点，且给出了一个实时跟踪最新的不平衡问题研究的网页。 6.1，介绍 文章指出当有关输入属性的分布影响性能时，就会出现与输入属性相关的不平衡问题。论文将不平衡问题归为四类： Class imbalance: 类别不平衡。不同类别的输入边界框的数量不同，包括前景/背景和前景/前景类别的不平衡，RPN 和 Focal Loss 就是解决这类问题。 Scale imbalance: 尺度不平衡，主要是目标边界框的尺度不平衡引起的，也包括将物体分配至 feature pyramid 时的不平衡。典型如 FPN 就是解决物体多尺度问题的。 Spatial imbalance: 空间不平衡。包括不同样本对回归损失贡献的不平衡，IoU 分布的不平衡，和目标分布位置的不平衡。 Objective imbalance:不同任务（分类、回归）对总损失贡献的不平衡。 参考资料 Focal Loss for Dense Object Detection NMS介绍 Faster RCNN 源码解读(2) -- NMS(非极大抑制) nms Imbalance Problems in Object Detection: A Review 5分钟理解Focal Loss与GHM——解决样本不平衡利器 focal loss 通俗讲解 损失函数｜交叉熵损失函数 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/2D目标检测/1-目标检测模型的评价标准-AP与mAP.html":{"url":"6-computer_vision/2D目标检测/1-目标检测模型的评价标准-AP与mAP.html","title":"1-目标检测模型的评价标准-AP与mAP","keywords":"","body":"目录 [toc] 前言 为了了解模型的泛化能力，即判断模型的好坏，我们需要用某个指标来衡量，有了评价指标，就可以对比不同模型的优劣，并通过这个指标来进一步调参优化模型。对于分类和回归两类监督模型，分别有各自的评判标准。 不同的问题和不同的数据集都会有不同的模型评价指标，比如分类问题，数据集类别平衡的情况下可以使用准确率作为评价指标，但是现实中的数据集几乎都是类别不平衡的，所以一般都是采用 AP 作为分类的评价指标，分别计算每个类别的 AP，再计算mAP。 一，精确率、召回率与F1 1.1，准确率 准确率（精度） – Accuracy，预测正确的结果占总样本的百分比，定义如下： $$ 准确率 = (TP+TN)/(TP+TN+FP+FN)$$ 错误率和精度虽然常用，但是并不能满足所有任务需求。以西瓜问题为例，假设瓜农拉来一车西瓜，我们用训练好的模型对西瓜进行判别，现如精度只能衡量有多少比例的西瓜被我们判断类别正确（两类：好瓜、坏瓜）。但是若我们更加关心的是“挑出的西瓜中有多少比例是好瓜”，或者”所有好瓜中有多少比例被挑出来“，那么精度和错误率这个指标显然是不够用的。 虽然准确率可以判断总的正确率，但是在样本不平衡的情况下，并不能作为很好的指标来衡量结果。举个简单的例子，比如在一个总样本中，正样本占 90%，负样本占 10%，样本是严重不平衡的。对于这种情况，我们只需要将全部样本预测为正样本即可得到 90% 的高准确率，但实际上我们并没有很用心的分类，只是随便无脑一分而已。这就说明了：由于样本不平衡的问题，导致了得到的高准确率结果含有很大的水分。即如果样本不平衡，准确率就会失效。 1.2，精确率、召回率 精确率（查准率）P、召回率（查全率）R 的计算涉及到混淆矩阵的定义，混淆矩阵表格如下： |名称|定义| |---|---| |True Positive(真正例, TP)|将正类预测为正类数| |True Negative(真负例, TN)|将负类预测为负类数| |False Positive(假正例, FP)|将负类预测为正类数 → 误报 (Type I error)| |False Negative(假负例子, FN)|将正类预测为负类数 → 漏报 (Type II error)| 查准率与查全率计算公式： 查准率（精确率）$P = TP/(TP+FP)$ 查全率（召回率）$R = TP/(TP+FN)$ 精准率和准确率看上去有些类似，但是完全不同的两个概念。精准率代表对正样本结果中的预测准确程度，而准确率则代表整体的预测准确程度，既包括正样本，也包括负样本。 精确率描述了模型有多准，即在预测为正例的结果中，有多少是真正例；召回率则描述了模型有多全，即在为真的样本中，有多少被我们的模型预测为正例。精确率和召回率的区别在于分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。 1.3，F1 分数 如果想要找到 $P$ 和 $R$ 二者之间的一个平衡点，我们就需要一个新的指标：$F1$ 分数。$F1$ 分数同时考虑了查准率和查全率，让二者同时达到最高，取一个平衡。$F1$ 计算公式如下： 这里的 $F1$ 计算是针对二分类模型，多分类任务的 $F1$ 的计算请看下面。 $$F1 = \\frac{2\\times P\\times R}{P+R} = \\frac{2\\times TP}{样例总数+TP-TN}$$ $F1$ 度量的一般形式：$F{\\beta}$，能让我们表达出对查准率/查全率的偏见，$F{\\beta}$ 计算公式如下： $$F_{\\beta} = \\frac{1+\\beta^{2}\\times P\\times R}{(\\beta^{2}\\times P)+R}$$ 其中 $\\beta >1$ 对查全率有更大影响，$\\beta 不同的计算机视觉问题，对两类错误有不同的偏好，常常在某一类错误不多于一定阈值的情况下，努力减少另一类错误。在目标检测中，mAP（mean Average Precision）作为一个统一的指标将这两种错误兼顾考虑。 很多时候我们会有多个混淆矩阵，例如进行多次训练/测试，每次都能得到一个混淆矩阵；或者是在多个数据集上进行训练/测试，希望估计算法的”全局“性能；又或者是执行多分类任务，每两两类别的组合都对应一个混淆矩阵；....总而来说，我们希望能在 $n$ 个二分类混淆矩阵上综合考虑查准率和查全率。 一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率，记为 $(P_1,R_1),(P_2,R_2),...,(P_n,R_n)$ 然后取平均，这样得到的是”宏查准率（Macro-P）“、”宏查准率（Macro-R）“及对应的”宏 $F1$（Macro-F1）“： $$Macro\\ P = \\frac{1}{n}\\sum_{i=1}^{n}P_i$$ $$Macro\\ R = \\frac{1}{n}\\sum_{i=1}^{n}R_i$$ $$Macro\\ F1 = \\frac{2 \\times Macro\\ P\\times Macro\\ R}{Macro\\ P + Macro\\ R}$$ 另一种做法是将各混淆矩阵对应元素进行平均，得到 $TP、FP、TN、FN$ 的平均值，再基于这些平均值计算出”微查准率“（Micro-P）、”微查全率“（Micro-R）和”微 $F1$“（Mairo-F1） $$Micro\\ P = \\frac{\\overline{TP}}{\\overline{TP}+\\overline{FP}}$$ $$Micro\\ R = \\frac{\\overline{TP}}{\\overline{TP}+\\overline{FN}}$$ $$Micro\\ F1 = \\frac{2 \\times Micro\\ P\\times Micro\\ R}{MacroP+Micro\\ R}$$ 1.4，PR 曲线 精准率和召回率的关系可以用一个 P-R 图来展示，以查准率 P 为纵轴、查全率 R 为横轴作图，就得到了查准率－查全率曲线，简称 P-R 曲线，PR 曲线下的面积定义为 AP: 1.4.1，如何理解 P-R 曲线 可以从排序型模型或者分类模型理解。以逻辑回归举例，逻辑回归的输出是一个 0 到 1 之间的概率数字，因此，如果我们想要根据这个概率判断用户好坏的话，我们就必须定义一个阈值 。通常来讲，逻辑回归的概率越大说明越接近 1，也就可以说他是坏用户的可能性更大。比如，我们定义了阈值为 0.5，即概率小于 0.5 的我们都认为是好用户，而大于 0.5 都认为是坏用户。因此，对于阈值为 0.5 的情况下，我们可以得到相应的一对查准率和查全率。 但问题是：这个阈值是我们随便定义的，我们并不知道这个阈值是否符合我们的要求。 因此，为了找到一个最合适的阈值满足我们的要求，我们就必须遍历 0 到 1 之间所有的阈值，而每个阈值下都对应着一对查准率和查全率，从而我们就得到了 PR 曲线。 最后如何找到最好的阈值点呢？ 首先，需要说明的是我们对于这两个指标的要求：我们希望查准率和查全率同时都非常高。 但实际上这两个指标是一对矛盾体，无法做到双高。图中明显看到，如果其中一个非常高，另一个肯定会非常低。选取合适的阈值点要根据实际需求，比如我们想要高的查全率，那么我们就会牺牲一些查准率，在保证查全率最高的情况下，查准率也不那么低。。 1.5，ROC 曲线与 AUC 面积 PR 曲线是以 Recall 为横轴，Precision 为纵轴；而 ROC 曲线则是以 FPR 为横轴，TPR 为纵轴。P-R 曲线越靠近右上角性能越好**。PR 曲线的两个指标都聚焦于正例 PR 曲线展示的是 Precision vs Recall 的曲线，ROC 曲线展示的是 FPR（x 轴：False positive rate） vs TPR（True positive rate, TPR）曲线。 [ ] ROC 曲线 [ ] AUC 面积 二，AP 与 mAP 2.1，AP 与 mAP 指标理解 AP 衡量的是训练好的模型在每个类别上的好坏，mAP 衡量的是模型在所有类别上的好坏，得到 AP 后 mAP 的计算就变得很简单了，就是取所有 AP 的平均值。AP 的计算公式比较复杂（所以单独作一章节内容），详细内容参考下文。 mAP 这个术语有不同的定义。此度量指标通常用于信息检索、图像分类和目标检测领域。然而这两个领域计算 mAP 的方式却不相同。这里我们只谈论目标检测中的 mAP 计算方法。 mAP 常作为目标检测算法的评价指标，具体来说就是，对于每张图片检测模型会输出多个预测框（远超真实框的个数），我们使用 IoU (Intersection Over Union，交并比)来标记预测框是否预测准确。标记完成后，随着预测框的增多，查全率 R 总会上升，在不同查全率 R 水平下对准确率 P 做平均，即得到 AP，最后再对所有类别按其所占比例做平均，即得到 mAP 指标。 2.2，近似计算AP 知道了AP 的定义，下一步就是理解AP计算的实现，理论上可以通过积分来计算AP，公式如下： $$AP=\\int_0^1 P(r) dr$$ 但通常情况下都是使用近似或者插值的方法来计算 $AP$。 $$AP = \\sum_{k=1}^{N}P(k)\\Delta r(k)$$ 近似计算 $AP$ (approximated average precision)，这种计算方式是 approximated 形式的； 很显然位于一条竖直线上的点对计算 $AP$ 没有贡献； 这里 $N$ 为数据总量，$k$ 为每个样本点的索引， $Δr(k)=r(k)−r(k−1)$。 近似计算 AP 和绘制 PR 曲线代码如下： import numpy as np import matplotlib.pyplot as plt class_names = [\"car\", \"pedestrians\", \"bicycle\"] def draw_PR_curve(predict_scores, eval_labels, name, cls_idx=1): \"\"\"calculate AP and draw PR curve, there are 3 types Parameters: @all_scores: single test dataset predict scores array, (-1, 3) @all_labels: single test dataset predict label array, (-1, 3) @cls_idx: the serial number of the AP to be calculated, example: 0,1,2,3... \"\"\" # print('sklearn Macro-F1-Score:', f1_score(predict_scores, eval_labels, average='macro')) global class_names fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 10)) # Rank the predicted scores from large to small, extract their corresponding index(index number), and generate an array idx = predict_scores[:, cls_idx].argsort()[::-1] eval_labels_descend = eval_labels[idx] pos_gt_num = np.sum(eval_labels == cls_idx) # number of all gt predict_results = np.ones_like(eval_labels) tp_arr = np.logical_and(predict_results == cls_idx, eval_labels_descend == cls_idx) # ndarray fp_arr = np.logical_and(predict_results == cls_idx, eval_labels_descend != cls_idx) tp_cum = np.cumsum(tp_arr).astype(float) # ndarray, Cumulative sum of array elements. fp_cum = np.cumsum(fp_arr).astype(float) precision_arr = tp_cum / (tp_cum + fp_cum) # ndarray recall_arr = tp_cum / pos_gt_num ap = 0.0 prev_recall = 0 for p, r in zip(precision_arr, recall_arr): ap += p * (r - prev_recall) # pdb.set_trace() prev_recall = r print(\"------%s, ap: %f-----\" % (name, ap)) fig_label = '[%s, %s] ap=%f' % (name, class_names[cls_idx], ap) ax.plot(recall_arr, precision_arr, label=fig_label) ax.legend(loc=\"lower left\") ax.set_title(\"PR curve about class: %s\" % (class_names[cls_idx])) ax.set(xticks=np.arange(0., 1, 0.05), yticks=np.arange(0., 1, 0.05)) ax.set(xlabel=\"recall\", ylabel=\"precision\", xlim=[0, 1], ylim=[0, 1]) fig.savefig(\"./pr-curve-%s.png\" % class_names[cls_idx]) plt.close(fig) 2.3，插值计算 AP 插值计算(Interpolated average precision) $AP$ 的公式的演变过程这里不做讨论，详情可以参考这篇文章，我这里的公式和图也是参考此文章的。11 点插值计算方式计算 $AP$ 公式如下： 这是通常意义上的 11 points_Interpolated 形式的 AP，选取固定的 ${0,0.1,0.2,…,1.0}$ 11 个阈值，这个在 PASCAL2007 中使用 这里因为参与计算的只有 11 个点，所以 $K=11$，称为 11 points_Interpolated，$k$ 为阈值索引 $P_{interp}(k)$ 取第 $k$ 个阈值所对应的样本点之后的样本中的最大值，只不过这里的阈值被限定在了 ${0,0.1,0.2,…,1.0}$ 范围内。 从曲线上看，真实 AP，11-points Interpolated AP 可能大也可能小，当数据量很多的时候会接近于 Interpolated AP，与 Interpolated AP 不同，前面的公式中计算 AP 时都是对 PR 曲线的面积估计，PASCAL 的论文里给出的公式就更加简单粗暴了，直接计算11 个阈值处的 precision 的平均值。PASCAL 论文给出的 11 点计算 AP 的公式如下。 1, 在给定 recal 和 precision 的条件下计算 AP： def voc_ap(rec, prec, use_07_metric=False): \"\"\" ap = voc_ap(rec, prec, [use_07_metric]) Compute VOC AP given precision and recall. If use_07_metric is true, uses the VOC 07 11 point method (default:False). \"\"\" if use_07_metric: # 11 point metric ap = 0. for t in np.arange(0., 1.1, 0.1): if np.sum(rec >= t) == 0: p = 0 else: p = np.max(prec[rec >= t]) ap = ap + p / 11. else: # correct AP calculation # first append sentinel values at the end mrec = np.concatenate(([0.], rec, [1.])) mpre = np.concatenate(([0.], prec, [0.])) # compute the precision envelope for i in range(mpre.size - 1, 0, -1): mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i]) # to calculate area under PR curve, look for points # where X axis (recall) changes value i = np.where(mrec[1:] != mrec[:-1])[0] # and sum (\\Delta recall) * prec ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1]) return ap 2，给定目标检测结果文件和测试集标签文件 xml 等计算 AP： def parse_rec(filename): \"\"\" Parse a PASCAL VOC xml file Return : list, element is dict. \"\"\" tree = ET.parse(filename) objects = [] for obj in tree.findall('object'): obj_struct = {} obj_struct['name'] = obj.find('name').text obj_struct['pose'] = obj.find('pose').text obj_struct['truncated'] = int(obj.find('truncated').text) obj_struct['difficult'] = int(obj.find('difficult').text) bbox = obj.find('bndbox') obj_struct['bbox'] = [int(bbox.find('xmin').text), int(bbox.find('ymin').text), int(bbox.find('xmax').text), int(bbox.find('ymax').text)] objects.append(obj_struct) return objects def voc_eval(detpath, annopath, imagesetfile, classname, cachedir, ovthresh=0.5, use_07_metric=False): \"\"\"rec, prec, ap = voc_eval(detpath, annopath, imagesetfile, classname, [ovthresh], [use_07_metric]) Top level function that does the PASCAL VOC evaluation. detpath: Path to detections result file detpath.format(classname) should produce the detection results file. annopath: Path to annotations file annopath.format(imagename) should be the xml annotations file. imagesetfile: Text file containing the list of images, one image per line. classname: Category name (duh) cachedir: Directory for caching the annotations [ovthresh]: Overlap threshold (default = 0.5) [use_07_metric]: Whether to use VOC07's 11 point AP computation (default False) \"\"\" # assumes detections are in detpath.format(classname) # assumes annotations are in annopath.format(imagename) # assumes imagesetfile is a text file with each line an image name # cachedir caches the annotations in a pickle file # first load gt if not os.path.isdir(cachedir): os.mkdir(cachedir) cachefile = os.path.join(cachedir, '%s_annots.pkl' % imagesetfile) # read list of images with open(imagesetfile, 'r') as f: lines = f.readlines() imagenames = [x.strip() for x in lines] if not os.path.isfile(cachefile): # load annotations recs = {} for i, imagename in enumerate(imagenames): recs[imagename] = parse_rec(annopath.format(imagename)) if i % 100 == 0: print('Reading annotation for {:d}/{:d}'.format( i + 1, len(imagenames))) # save print('Saving cached annotations to {:s}'.format(cachefile)) with open(cachefile, 'wb') as f: pickle.dump(recs, f) else: # load with open(cachefile, 'rb') as f: try: recs = pickle.load(f) except: recs = pickle.load(f, encoding='bytes') # extract gt objects for this class class_recs = {} npos = 0 for imagename in imagenames: R = [obj for obj in recs[imagename] if obj['name'] == classname] bbox = np.array([x['bbox'] for x in R]) difficult = np.array([x['difficult'] for x in R]).astype(np.bool) det = [False] * len(R) npos = npos + sum(~difficult) class_recs[imagename] = {'bbox': bbox, 'difficult': difficult, 'det': det} # read dets detfile = detpath.format(classname) with open(detfile, 'r') as f: lines = f.readlines() splitlines = [x.strip().split(' ') for x in lines] image_ids = [x[0] for x in splitlines] confidence = np.array([float(x[1]) for x in splitlines]) BB = np.array([[float(z) for z in x[2:]] for x in splitlines]) nd = len(image_ids) tp = np.zeros(nd) fp = np.zeros(nd) if BB.shape[0] > 0: # sort by confidence sorted_ind = np.argsort(-confidence) sorted_scores = np.sort(-confidence) BB = BB[sorted_ind, :] image_ids = [image_ids[x] for x in sorted_ind] # go down dets and mark TPs and FPs for d in range(nd): R = class_recs[image_ids[d]] bb = BB[d, :].astype(float) ovmax = -np.inf BBGT = R['bbox'].astype(float) if BBGT.size > 0: # compute overlaps # intersection ixmin = np.maximum(BBGT[:, 0], bb[0]) iymin = np.maximum(BBGT[:, 1], bb[1]) ixmax = np.minimum(BBGT[:, 2], bb[2]) iymax = np.minimum(BBGT[:, 3], bb[3]) iw = np.maximum(ixmax - ixmin + 1., 0.) ih = np.maximum(iymax - iymin + 1., 0.) inters = iw * ih # union uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) + (BBGT[:, 2] - BBGT[:, 0] + 1.) * (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters) overlaps = inters / uni ovmax = np.max(overlaps) jmax = np.argmax(overlaps) if ovmax > ovthresh: if not R['difficult'][jmax]: if not R['det'][jmax]: tp[d] = 1. R['det'][jmax] = 1 else: fp[d] = 1. else: fp[d] = 1. # compute precision recall fp = np.cumsum(fp) tp = np.cumsum(tp) rec = tp / float(npos) # avoid divide by zero in case the first detection matches a difficult # ground truth prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps) ap = voc_ap(rec, prec, use_07_metric) return rec, prec, ap 2.4，mAP 计算方法 因为 $mAP$ 值的计算是对数据集中所有类别的 $AP$ 值求平均，所以我们要计算 $mAP$，首先得知道某一类别的 $AP$ 值怎么求。不同数据集的某类别的 $AP$ 计算方法大同小异，主要分为三种： （1）在 VOC2007，只需要选取当 $Recall >= 0, 0.1, 0.2, ..., 1$ 共 11 个点时的 Precision 最大值，然后 $AP$ 就是这 11 个 Precision 的平均值，$mAP$ 就是所有类别 $AP$ 值的平均。VOC 数据集中计算 $AP$ 的代码（用的是插值计算方法，代码出自py-faster-rcnn仓库） （2）在 VOC2010 及以后，需要针对每一个不同的 Recall 值（包括 0 和 1），选取其大于等于这些 Recall 值时的 Precision 最大值，然后计算 PR 曲线下面积作为 $AP$ 值，$mAP$ 就是所有类别 $AP$ 值的平均。 （3）COCO 数据集，设定多个 IOU 阈值（0.5-0.95, 0.05 为步长），在每一个 IOU 阈值下都有某一类别的 AP 值，然后求不同 IOU 阈值下的 AP 平均，就是所求的最终的某类别的 AP 值。 三，目标检测度量标准汇总 评价指标 定义及理解 mAP mean Average Precision, 即各类别 AP 的平均值 AP PR 曲线下面积，后文会详细讲解 PR 曲线 Precision-Recall 曲线 Precision $TP / (TP + FP)$ Recall $TP / (TP + FN)$ TP IoU>0.5 的检测框数量（同一 Ground Truth 只计算一次，阈值取 0.5） FP IoU 的检测框，或者是检测到同一个 GT 的多余检测框的数量 FN 没有检测到的 GT 的数量 四，参考资料 目标检测评价标准-AP mAP 目标检测的性能评价指标 Soft-NMS Recent Advances in Deep Learning for Object Detection A Simple and Fast Implementation of Faster R-CNN 分类模型评估指标——准确率、精准率、召回率、F1、ROC曲线、AUC曲线 一文让你彻底理解准确率，精准率，召回率，真正率，假正率，ROC/AUC Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/2D目标检测/2-Faster-RCNN网络详解.html":{"url":"6-computer_vision/2D目标检测/2-Faster-RCNN网络详解.html","title":"2-Faster-RCNN网络详解","keywords":"","body":" Faster RCNN 网络概述 Conv layers RPN 网络 Anchors 生成 RPN 网络训练集 positive/negative 二分类 RPN 生成 RoIs(Proposal Layer) RPN 网络总结 ROIHead/Fast R-CNN Roi pooling ROI Head 训练 ROI Head 测试 概念理解 四类损失 三个 creator 参考资料 本文为学习笔记，部分内容参考网上资料和论文而写的，内容涉及 Faster RCNN 网络结构理解和代码实现原理。 Faster RCNN 网络概述 backbone 为 vgg16 的 faster rcnn 网络结构如下图所示，可以清晰的看到该网络对于一副任意大小 PxQ 的图像，首先缩放至固定大小 MxN，然后将 MxN 图像送入网络；而 Conv layers 中包含了 13 个 conv 层 + 13 个 relu 层 + 4 个 pooling 层；RPN 网络首先经过 3x3 卷积，再分别生成 positive anchors 和对应 bounding box regression 偏移量，然后计算出 proposals；而 Roi Pooling 层则利用 proposals 从 feature maps 中提取 proposal feature 送入后续全连接和 softmax 网络作 classification（即分类： proposal 是哪种 object）。 Conv layers 论文中 Faster RCNN 虽然支持任意图片输入，但是进入 Conv layers 网络之前会对图片进行规整化尺度操作，如可设定图像短边不超过 600，图像长边不超过 1000，我们可以假定 $M\\times N=1000\\times 600$（如果图片少于该尺寸，可以边缘补 0，即图像会有黑色边缘）。 13 个 conv 层：kernel_size=3, pad=1, stride=1，卷积公式：N = (W − F + 2P )/S+1，所以可知 conv 层不会改变图片大小 13 个 relu 层: 激活函数，增加非线性，不改变图片大小 4 个 pooling 层：kernel_size=2,stride=2，pooling 层会让输出图片变成输入图片的 1/2。 所以经过 Conv layers，图片大小变成 $(M/16) \\ast (N/16)$，即：$60\\ast 40(1000/16≈60,600/16≈40)$；则 Feature Map 尺寸为 $60\\ast 40\\ast 512$-d (注：VGG16 是512-d, ZF 是 256-d，d 是指特征图通道数，也叫特征图数量)，表示特征图的大小为 $60\\ast 40$，数量为 512。 RPN 网络 RPN 在 Extractor（特征提取 backbone ）输出的 feature maps 的基础之上，先增加了一个 3*3 卷积（用来语义空间转换？），然后利用两个 1x1 的卷积分别进行二分类（是否为正样本）和位置回归。RPN 网络在分类和回归的时候，分别将每一层的每一个 anchor 分为背景和前景两类，以及回归四个位移量，进行分类的卷积核通道数为9×2（9 个 anchor，每个 anchor 二分类，使用交叉熵损失），进行回归的卷积核通道数为 9×4（9个anchor，每个 anchor 有 4 个位置参数）。RPN是一个全卷积网络（fully convolutional network），这样对输入图片的尺寸就没有要求了。 RPN 完成 positive/negative 分类 + bounding box regression 坐标回归两个任务。 Anchors 在RPN中，作者提出了anchors，在代码中，anchors 是一组由 generate_anchors.py 生成的矩形框列表。运行官方代码的 generate_anchors.py 可以得到以下示例输出.这里生成的坐标是在原图尺寸上的坐标，在特征图上的一个像素点，可以对应到原图上一个 $16\\times 16$大小的区域。 [[ -84. -40. 99. 55.] [-176. -88. 191. 103.] [-360. -184. 375. 199.] [ -56. -56. 71. 71.] [-120. -120. 135. 135.] [-248. -248. 263. 263.] [ -36. -80. 51. 95.] [ -80. -168. 95. 183.] [-168. -344. 183. 359.]] 其中每行的 4 个值 $(x{1}, y{1}, x{2}, y{2})$ 表矩形左上和右下角点坐标。9 个矩形共有 3 种形状，长宽比为大约为 $width:height \\epsilon {1:1, 1:2, 2:1}$ 三种，如下图。实际上通过 anchors 就引入了检测中常用到的多尺度方法。 注意，generate_anchors.py 生成的只是 base anchors，其中一个 框的左上角坐标为 (0,0) 坐标（特征图左上角）的 9 个 anchor，后续还需网格化（meshgrid）生成其他 anchor。同一个 scale，但是不同的 anchor ratios 生成的 anchors 面积理论上是要一样的。 然后利用这 9 种 anchor 在特征图左右上下移动（遍历），每一个特征图上的任意一个点都有 9 个 anchor，假设原图大小为 MxN，经过 Conv layers 下采样 16 倍，则每个 feature map 生成 (M/16)*(N/16)*9个 anchor。例如，对于一个尺寸为 62×37 的 feature map，有 62×37×9 ≈ 20000 个 anchor，并输出特征图上面每个点对应的原图 anchor 坐标。这种做法很像是暴力穷举，20000 多个 anchor，哪怕是蒙也能够把绝大多数的 ground truth bounding boxes 蒙中。 因此可知，anchor 的数量和 feature map 大小相关，不同的 feature map 对应的 anchor 数量也不一样。 生成 RPN 网络训练集 在这个任务中，RPN 做的事情就是利用（AnchorTargetCreator）将 20000 多个候选的 anchor 选出 256 个 anchor 进行分类和回归位置。选择过程如下： 对于每一个 ground truth bounding box (gt_bbox)，选择和它重叠度（IoU）最高的一个 anchor 作为正样本; 对于剩下的 anchor，从中选择和任意一个 gt_bbox 重叠度超过 0.7 的 anchor ，同样作为正样本;特殊情况下，如果正样本不足 128(256 的 1/2)，则用负样本凑。 随机选择和 gt_bbox 重叠度小于 0.3 的 anchor 作为负样本。 本和正样本的总数为256 ，正负样本比例 1:1。 positive/negative 二分类 由 $1\\times 1$ 卷积实现，卷积通道数为 $9\\times 2$（每个点有 9 个 anchor，每个 anchor 二分类，使用交叉熵损失），后面接 softmax 分类获得 positive anchors，也就相当于初步提取了检测目标候选区域 box（一般认为目标在 positive anchors 中）。所以可知，RPN 的一个任务就是在原图尺度上，设置了大量的候选 anchor，并通过 AnchorTargetCreator 类去挑选正负样本比为 1:1 的 256 个 anchor，然后再用 CNN ($1\\times 1$ 卷积，卷积通道数 $9\\times 2$) 去判断挑选出来的 256 个 anchor 哪些有目标的 positive anchor，哪些是没目标的 negative anchor。 在挑选 1:1 正负样本比例的 anchor 用作 RPN 训练集后，还需要计算训练集数据对应的标签。对于每个 anchor, 对应的标签是 gt_label 和 gt_loc。gt_label 要么为 1（前景），要么为 0（背景），而 gt_loc 则是由 4 个位置参数 $(tx,t_y,t_w,t_h)$ 组成，它们是 anchor box 与 ground truth bbox 之间的偏移量，因为回归偏移量比直接回归座标更好。在 Faster RCNN原文，positive anchor 与 ground truth 之间的偏移量 $(t{x}, t{y})$ 与尺度因子 $(t{w}, t_{h})$ 计算公式如下: $$t{x} = (x-x{a})/w{a}, t{y}=(y-y{a})/h{a} \\\\ t{w} = log(w/w{a}), t{h}=log(h/h{a}) \\\\ t{x}^{\\ast } = (x^{\\ast }-x{a})/w{a}, t{y}^{\\ast}=(y^{\\ast}-y{a})/h{a} \\\\ t{w}^{\\ast } = log(w^{\\ast }/w{a}), t{h}^{\\ast }=log(h^{\\ast }/h{a}) $$ 参数解释：where $x, y, w,$ and $h$ denote the box’s center coordinates and its width and height. Variables $x, x_{a}$，and $x^{*}$ are for the predicted box, anchor box, and groundtruth box respectively (likewise for $y, w, h$). 计算分类损失用的是交叉熵损失，计算回归损失用的是 Smooth_L1_loss。在计算回归损失的时候，只计算正样本（前景）的损失，不计算负样本的位置损失。RPN 网络的 Loss 计算公式如下： 损失函数：L1 loss, L2 loss, smooth L1 loss 公式解释：Here, $i$ is the index of an anchor in a mini-batch and $p{i}$ is the predicted probability of anchor i being an object. The ground-truth label $p_i^{\\ast}$ is 1 if the anchor is positive, and is 0 if the anchor is negative. $t{i}$ is a vector representing the 4 parameterized coordinates of the predicted bounding box, and $t_i^*$ is that of theground-truth box associated with a positive anchor. RPN 生成 RoIs(Proposal Layer) RPN 网络在自身训练的同时，还会由 Proposal Layer 层产生 RoIs（region of interests）给 Fast RCNN（RoIHead）作为训练样本。RPN 生成 RoIs 的过程( ProposalCreator )如下： 对于每张图片，利用它的 feature map， 计算 (H/16)× (W/16)×9（大概 20000）个 anchor 属于前景的概率，以及对应的位置参数，并选取概率较大的 12000 个 anchor； 利用回归的位置参数，修正这 12000 个 anchor 的位置，得到RoIs； 利用非极大值（(Non-maximum suppression, NMS）抑制，选出概率最大的 2000 个 RoIs。 在 RPN 中，从上万个 anchor 中，选一定数目(2000 或 300)，调整大小和位置生成 RoIs，用于 ROI Head/Fast RCNN 训练或测试，然后 ProposalTargetCreator 再从 RoIs 中会中选择 128 个 RoIs 用以 ROIHead 的训练）。 注意：RoIs 对应的尺寸是原图大小，同时在 inference 的时候，为了提高处理速度，12000 和 2000 分别变为 6000 和 300。Proposal Layer 层，这部分的操作不需要进行反向传播，因此可以利用 numpy/tensor 实现。 RPN 网络总结 RPN 网络结构：生成 anchors -> softmax 分类器提取 positvie anchors -> bbox reg 回归 positive anchors -> Proposal Layer 生成 proposals RPN 的输出：RoIs(region of interests)（形如 2000×4 或者 300×4 的 tensor） ROIHead/Fast R-CNN RPN 只是给出了 2000 个 候选框，RoI Head 在给出的 2000 候选框之上继续进行分类和位置参数的回归。ROIHead 网络包括 RoI pooling + Classification(全连接分类)两部分，网络结构如下： 由于 RoIs 给出的 2000 个 候选框，分别对应 feature map 不同大小的区域。首先利用 ProposalTargetCreator 挑选出 128 个 sample_rois, 然后使用了 RoI Pooling 将这些不同尺寸的区域全部 pooling 到同一个尺度 $7\\times 7$ 上，并输出 $7\\times 7$ 大小的 feature map 送入后续的两个全连接层。两个全连接层分别完成类别分类和 bbox 回归的作用： FC 21 用来分类，预测 RoIs 属于哪个类别（20个类+背景） FC 84 用来回归位置（21个类，每个类都有4个位置参数） 论文中之所以设定为 pooling 成 7×7 的尺度，其实是为了网络输出是固定大小的vector or matrix，从而能够共享 VGG 后面两个全连接层的权重。当所有的 RoIs 都被pooling 成（512×7×7）的 feature map 后，将它 reshape 成一个一维的向量，就可以利用 VGG16 预训练的权重来初始化前两层全连接（FC 4096）。 Roi pooling RoI pooling 负责将 128 个 RoI 区域对应的 feature map 进行截取，而后利用 RoI pooling 层输出 $7\\times 7$ 大小的 feature map，送入后续的全连接网络。从论文给出的 Faster R-CNN 网络结构图中，可以看到 Rol pooling 层有 2 个输入： 原始的 feature maps RPN 输出的 RoIs (proposal boxes, 大小各不相同） RoI Pooling 的两次量化过程： (1) 因为 proposals是对应 $M\\times N$ 的原图尺寸，所以在原图上生成的 region proposal 需要映射到 feature map 上（坐标值缩小 16 倍），需要除以 $16/32$（下采样倍数），这时候边界会出现小数，自然就需要量化。 (2) 将 proposals 对应的 feature map 区域水平划分成 $k\\times k$ ($7\\times 7$) 的 bins，并对每个 bin 中均匀选取多少个采样点，然后进行 max pooling，也会出现小数，自然就产生了第二次量化。 Mask RCNN 算法中的 RoI Align 如何改进: ROI Align 并不需要对两步量化中产生的浮点数坐标的像素值都进行计算，而是设计了一套优雅的流程。如下图，其中虚线代表的是一个 feature map，实线代表的是一个 roi (在这个例子中，一个 roi 是分成了 $2\\times 2$ 个 bins)，实心点代表的是采样点，每个 bin 中有 4 个采样点。我们通过双线性插值的方法根据采样点周围的四个点计算每一个采样点的值，然后对着四个采样点执行最大池化操作得到当前 bin 的像素值。 RoI Align 具体做法：假定采样点数为 4，即表示，对于每个 $2.97\\times 2.97$ 的 bin，平分四份小矩形，每一份取其中心点位置，而中心点位置的像素，采用双线性插值法进行计算，这样就会得到四个小数坐标点的像素值。 更多细节内容可以参考 RoIPooling、RoIAlign笔记。 ROI Head 训练 RPN 会产生大约 2000 个 RoIs ，ROI Head 在给出的 2000 个 RoIs 候选框基础上继续分类(目标分类)和位置参数回归。注意，这 2000 个 RoIs 不是都拿去训练，而是利用 ProposalTargetCreator（官方源码可以查看类定义） 选择 128 个 RoIs 用以训练。选择的规则如下： 在和 gt_bboxes 的 IoU 大于 0.5 的 RoIs 内，选择一些（比如 32 个）作为正样本 在和 gt_bboxes 的 IoU 小于等于 0（或者 0.1 ）RoIs 内，的选择一些（比如 $128 - 32 = 96$ 个）作为负样本 选择出的 128 个 RoIs，其正负样本比例为 3:1，在源码中为了便于训练，还对他们的 gt_roi_loc 进行标准化处理（减去均值除以标准差）。 对于分类问题, 和 RPN 一样，直接利用交叉熵损失。 对于位置的回归损失，也采用 Smooth_L1 Loss, 只不过只对正样本计算损失，而且是只对正样本中的对应类别的 $4$ 个参数计算损失。举例来说： 一个 RoI 在经过 FC84 后会输出一个 84 维的 loc 向量。 如果这个 RoI 是负样本, 则这 84 维向量不参与计算 L1_Loss。 如果这个 RoI 是正样本，且属于 类别 $k$, 那么向量的第 $(k×4，k×4+1 ，k×4+2， k×4+3)$ 这 $4$ 位置的值参与计算损失，其余的不参与计算损失。 ROI Head 测试 ROI Head 测试的时候对所有的 RoIs（大概 300 个左右) 计算概率，并利用位置参数调整预测候选框的位置，然后再用一遍极大值抑制（之前在 RPN 的ProposalCreator 也用过）。这里注意： 在 RPN 的时候，已经对 anchor 做了一遍 NMS，在 Fast RCNN 测试的时候，还要再做一遍，所以在Faster RCNN框架中，NMS操作总共有 2 次。 在 RPN 的时候，已经对 anchor 的位置做了回归调整，在 Fast RCNN 阶段还要对 RoI 再做一遍。 在 RPN 阶段分类是二分类，而 Fast RCNN/ROI Head 阶段是 21 分类。 概念理解 在阅读 Faster RCNN 论文和源码中，我们经常会涉及到一些概念的理解。 四类损失 在训练 Faster RCNN 的时候有四个损失： RPN 分类损失：anchor 是否为前景（二分类） RPN 位置回归损失：anchor 位置微调 RoI 分类损失：RoI 所属类别（21 分类，多了一个类作为背景） RoI 位置回归损失：继续对 RoI 位置微调 四个损失相加作为最后的损失，反向传播，更新参数。 三个 creator Faster RCNN 官方源码中有三个 creator 类分别实现不同的功能，不能弄混，各自功能如下： AnchorTargetCreator ： 负责在训练 RPN 的时候，从上万个 anchors 中选择一些(比如 256 )进行训练，并使得正负样本比例大概是 1:1。同时给出训练的位置参数目标，即返回 gt_rpn_loc 和 gt_rpn_label。 ProposalTargetCreator： 负责在训练 RoIHead/Fast R-CNN 的时候，从 RoIs 选择一部分(比如 128 个，正负样本比例 1:3)用以训练。同时给定训练目标, 返回（sample_RoI, gt_RoI_loc, gt_RoI_label）。 ProposalCreator： 在 RPN 中，从上万个 anchor 中，选择一定数目（ 2000 或者 300 ），调整大小和位置，生成 RoIs ，用以 Fast R-CNN 训练或者测试。 其中 AnchorTargetCreator 和 ProposalTargetCreator 类是为了生成训练的目标，只在训练阶段用到，ProposalCreator 是 RPN 为 Fast R-CNN 生成 RoIs ，在训练和测试阶段都会用到。三个 creator 的共同点在于他们都不需要考虑反向传播（因此不同框架间可以共享 numpy 实现）。 参考资料 一文读懂Faster RCNN 从编程实现角度学习Faster RCNN 你真的学会了RoI Pooling了吗 Faster RCNN 学习笔记 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/2D目标检测/3-FPN网络详解.html":{"url":"6-computer_vision/2D目标检测/3-FPN网络详解.html","title":"3-FPN网络详解","keywords":"","body":" 论文背景 引言（Introduction） 特征金字塔网络 FPN FPN网络建立 Anchor锚框生成规则 实验 代码解读 参考资料 本篇文章是论文阅读笔记和网络理解心得总结而来，部分资料和图参考论文和网络资料 论文背景 FPN(feature pyramid networks) 是何凯明等作者提出的适用于多尺度目标检测算法。原来多数的 object detection 算法（比如 faster rcnn）都是只采用顶层特征做预测，但我们知道低层的特征语义信息比较少，但是目标位置准确；高层的特征语义信息比较丰富，但是目标位置比较粗略。另外虽然也有些算法采用多尺度特征融合的方式，但是一般是采用融合后的特征做预测，而本文不一样的地方在于预测是在不同特征层独立进行的。 引言（Introduction） 从上图可以看出，（a）使用图像金字塔构建特征金字塔。每个图像尺度上的特征都是独立计算的，速度很慢。（b）最近的检测系统选择（比如 Faster RCNN）只使用单一尺度特征进行更快的检测。（c）另一种方法是重用 ConvNet（卷积层）计算的金字塔特征层次结构（比如 SSD），就好像它是一个特征化的图像金字塔。（d）我们提出的特征金字塔网络（FPN）与（b）和（c）类似，但更准确。在该图中，特征映射用蓝色轮廓表示，较粗的轮廓表示语义上较强的特征。 特征金字塔网络 FPN 作者提出的 FPN 结构如下图：这个金字塔结构包括一个自底向上的线路，一个自顶向下的线路和横向连接（lateral connections）。 自底向上其实就是卷积网络的前向过程。在前向过程中，feature map 的大小在经过某些层后会改变，而在经过其他一些层的时候不会改变，作者将不改变 feature map 大小的层归为一个 stage，因此这里金字塔结构中每次抽取的特征都是每个 stage 的最后一个层的输出。在代码中我们可以看到共有C1、C2、C3、C4、C5五个特征图，C1 和 C2 的特征图大小是一样的，所以，FPN 的建立也是基于从 C2 到 C5 这四个特征层上。 自顶向下的过程采用上采样（upsampling）进行，而横向连接则是将上采样的结果和自底向上生成的相同大小的 feature map 进行融合（merge）。在融合之后还会再采用 3*3 的卷积核对每个融合结果进行卷积，目的是消除上采样的混叠效应（aliasing effect）。并假设生成的 feature map 结果是 P2，P3，P4，P5，和原来自底向上的卷积结果 C2，C3，C4，C5一一对应。 这里贴一个 ResNet 的结构图：论文中作者采用 conv2_x，conv3_x，conv4_x 和 conv5_x 的输出，对应 C1，C2，C3，C4，C5，因此类似 Conv2就可以看做一个stage。 FPN网络建立 这里自己没有总结，因为已经有篇博文总结得很不错了，在这。 通过 ResNet50 网络，得到图片不同阶段的特征图，最后利用 C2，C3，C4，C5 建立特征图金字塔结构： 将 C5 经过 256 个 1*1 的卷积核操作得到：32*32*256，记为 P5； 将 P5 进行步长为 2 的上采样得到 64*64*256，再与 C4 经过的 256 个 1*1 卷积核操作得到的结果相加，得到 64*64*256，记为 P4； 将 P4 进行步长为 2 的上采样得到 128*128*256，再与 C3 经过的 256 个 1*1 卷积核操作得到的结果相加，得到 128*128*256，记为 P3； 将 P3 进行步长为 2 的上采样得到 256*256*256，再与 C2 经过的 256 个 1*1 卷积核操作得到的结果相加，得到 256*256*256，记为 P2； 将 P5 进行步长为 2 的最大池化操作得到：16*16*256，记为 P6； 结合从 P2 到 P6 特征图的大小，如果原图大小 1024*1024, 那各个特征图对应到原图的步长依次为 [P2,P3,P4,P5,P6]=>[4,8,16,32,64]。 Anchor锚框生成规则 当 Faster RCNN 采用 FPN 的网络作 backbone 后，锚框的生成规则也会有所改变。基于上一步得到的特征图 [P2,P3,P4,P5,P6],再介绍下采用 FPN 的 Faster RCNN（或者 Mask RCNN）网络中 Anchor 锚框的生成，根据源码中介绍的规则，与之前 Faster-RCNN 中的生成规则有一点差别。 遍历 P2 到 P6 这五个特征层，以每个特征图上的每个像素点都生成 Anchor 锚框； 以 P2 层为例，P2 层的特征图大小为 256*256，相对于原图的步长为4，这样 P2上的每个像素点都可以生成一个基于坐标数组 [0,0,3,3] 即 4*4 面积为 16 大小的Anchor锚框，当然，可以设置一个比例 SCALE，将这个基础的锚框放大或者缩小，比如，这里设置 P2 层对应的缩放比例为 16，那边生成的锚框大小就是长和宽都扩大16倍，从 4*4 变成 64*64，面积从 16 变成 4096，当然在保证面积不变的前提下，长宽比可以变换为 32*128、64*64 或 128*32，这样以长、宽比率 RATIO = [0.5,1,2] 完成了三种变换，这样一个像素点都可以生成3个Anchor锚框。在 Faster-RCNN 中可以将 Anchor scale 也可以设置为多个值，而在MasK RCNN 中则是每一特征层只对应着一个 Anchor scale即对应着上述所设置的 16； 以 P2 层每个像素点位中心，对应到原图上，则可生成 256*256*3(长宽三种变换) = 196608 个锚框； 以 P3 层每个像素点为中心，对应到原图上，则可生成 128*128*3 = 49152 个锚框； 以 P4 层每个像素点为中心，对应到原图上，则可生成 64*64*3 = 12288 个锚框； 以 P5 层每个像素点为中心，对应到原图上，则生成 32*32*3 = 3072 个锚框； 以 P6 层每个像素点为中心，对应到原图上，则生成 16*16*3 = 768 个锚框。 从 P2 到 P6 层一共可以在原图上生成 $196608 + 49152 + 12288 + 3072 + 768 = 261888$ 个 Anchor 锚框。 实验 看看加入FPN 的 RPN 网络的有效性，如下表 Table1。网络这些结果都是基于 ResNet-50。评价标准采用 AR，AR 表示 Average Recall，AR 右上角的 100 表示每张图像有 100 个 anchor，AR 的右下角 s，m，l 表示 COCO 数据集中 object 的大小分别是小，中，大。feature 列的大括号 {} 表示每层独立预测。 从（a）（b）（c）的对比可以看出 FPN 的作用确实很明显。另外（a）和（b）的对比可以看出高层特征并非比低一层的特征有效。 （d）表示只有横向连接，而没有自顶向下的过程，也就是仅仅对自底向上（bottom-up）的每一层结果做一个 1*1 的横向连接和 3*3 的卷积得到最终的结果，有点像 Fig1 的（b）。从 feature 列可以看出预测还是分层独立的。作者推测（d）的结果并不好的原因在于在自底向上的不同层之间的 semantic gaps 比较大。 （e）表示有自顶向下的过程，但是没有横向连接，即向下过程没有融合原来的特征。这样效果也不好的原因在于目标的 location 特征在经过多次降采样和上采样过程后变得更加不准确。 （f）采用 finest level 层做预测（参考 Fig2 的上面那个结构），即经过多次特征上采样和融合到最后一步生成的特征用于预测，主要是证明金字塔分层独立预测的表达能力。显然 finest level 的效果不如 FPN 好，原因在于 PRN 网络是一个窗口大小固定的滑动窗口检测器，因此在金字塔的不同层滑动可以增加其对尺度变化的鲁棒性。另外（f）有更多的 anchor，说明增加 anchor 的数量并不能有效提高准确率。 代码解读 这里给出一个基于 Pytorch 的 FPN 网络的代码，来自这里。 ## ResNet的block class Bottleneck(nn.Module): expansion = 4 def __init__(self, in_planes, planes, stride=1): super(Bottleneck, self).__init__() self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False) self.bn3 = nn.BatchNorm2d(self.expansion*planes) self.shortcut = nn.Sequential() if stride != 1 or in_planes != self.expansion*planes: self.shortcut = nn.Sequential( nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion*planes) ) def forward(self, x): out = F.relu(self.bn1(self.conv1(x))) out = F.relu(self.bn2(self.conv2(out))) out = self.bn3(self.conv3(out)) out += self.shortcut(x) out = F.relu(out) return out class FPN(nn.Module): def __init__(self, block, num_blocks): super(FPN, self).__init__() self.in_planes = 64 self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(64) # Bottom-up layers, backbone of the network self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1) self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2) self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2) self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2) # Top layer # 我们需要在C5后面接一个1x1, 256 conv，得到金字塔最顶端的feature self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0) # Reduce channels # Smooth layers # 这个是上面引文中提到的抗aliasing的3x3卷积 self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) # Lateral layers # 为了匹配channel dimension引入的1x1卷积 # 注意这些backbone之外的extra conv，输出都是256 channel self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0) self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0) self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0) def _make_layer(self, block, planes, num_blocks, stride): strides = [stride] + [1]*(num_blocks-1) layers = [] for stride in strides: layers.append(block(self.in_planes, planes, stride)) self.in_planes = planes * block.expansion return nn.Sequential(*layers) ## FPN的lateral connection部分: upsample以后，element-wise相加 def _upsample_add(self, x, y): '''Upsample and add two feature maps. Args: x: (Variable) top feature map to be upsampled. y: (Variable) lateral feature map. Returns: (Variable) added feature map. Note in PyTorch, when input size is odd, the upsampled feature map with `F.upsample(..., scale_factor=2, mode='nearest')` maybe not equal to the lateral feature map size. e.g. original input size: [N,_,15,15] -> conv2d feature map size: [N,_,8,8] -> upsampled feature map size: [N,_,16,16] So we choose bilinear upsample which supports arbitrary output sizes. ''' _,_,H,W = y.size() return F.upsample(x, size=(H,W), mode='bilinear') + y def forward(self, x): # Bottom-up c1 = F.relu(self.bn1(self.conv1(x))) c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1) c2 = self.layer1(c1) c3 = self.layer2(c2) c4 = self.layer3(c3) c5 = self.layer4(c4) # Top-down # P5: 金字塔最顶上的feature p5 = self.toplayer(c5) # P4: 上一层 p5 + 侧边来的 c4 # 其余同理 p4 = self._upsample_add(p5, self.latlayer1(c4)) p3 = self._upsample_add(p4, self.latlayer2(c3)) p2 = self._upsample_add(p3, self.latlayer3(c2)) # Smooth # 输出做一下smooth p4 = self.smooth1(p4) p3 = self.smooth2(p3) p2 = self.smooth3(p2) return p2, p3, p4, p5 参考资料 FPN（feature pyramid networks）算法讲解 Mask RCNN 源代码解析 (1) - 整体思路 Mask RCNN 学习笔记 论文 - Feature Pyramid Networks for Object Detection (FPN) Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/2D目标检测/4-Mask-RCNN详解.html":{"url":"6-computer_vision/2D目标检测/4-Mask-RCNN详解.html","title":"4-Mask-RCNN详解","keywords":"","body":" ROI Pooling 和 ROI Align 的区别 Mask R-CNN 网络结构 骨干网络 FPN anchor 锚框生成规则 实验 参考资料 Mask RCNN 是作者 Kaiming He 于 2018 年发表的论文 ROI Pooling 和 ROI Align 的区别 Understanding Region of Interest — (RoI Align and RoI Warp) Mask R-CNN 网络结构 Mask RCNN 继承自 Faster RCNN 主要有三个改进： feature map 的提取采用了 FPN 的多尺度特征网络 ROI Pooling 改进为 ROI Align 在 RPN 后面，增加了采用 FCN 结构的 mask 分割分支 网络结构如下图所示： 可以看出，Mask RCNN 是一种先检测物体，再分割的思路，简单直接，在建模上也更有利于网络的学习。 骨干网络 FPN 卷积网络的一个重要特征：深层网络容易响应语义特征，浅层网络容易响应图像特征。Mask RCNN 的使用了 ResNet 和 FPN 结合的网络作为特征提取器。 FPN 的代码出现在 ./mrcnn/model.py中，核心代码如下： if callable(config.BACKBONE): _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True, train_bn=config.TRAIN_BN) else: _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE, stage5=True, train_bn=config.TRAIN_BN) # Top-down Layers # TODO: add assert to varify feature map sizes match what's in config P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5) P4 = KL.Add(name=\"fpn_p4add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)]) P3 = KL.Add(name=\"fpn_p3add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)]) P2 = KL.Add(name=\"fpn_p2add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)]) # Attach 3x3 conv to all P layers to get the final feature maps. P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2) P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3) P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4) P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5) # P6 is used for the 5th anchor scale in RPN. Generated by # subsampling from P5 with stride of 2. P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5) # Note that P6 is used in RPN, but not in the classifier heads. rpn_feature_maps = [P2, P3, P4, P5, P6] mrcnn_feature_maps = [P2, P3, P4, P5] 其中 resnet_graph 函数定义如下： def resnet_graph(input_image, architecture, stage5=False, train_bn=True): \"\"\"Build a ResNet graph. architecture: Can be resnet50 or resnet101 stage5: Boolean. If False, stage5 of the network is not created train_bn: Boolean. Train or freeze Batch Norm layers \"\"\" assert architecture in [\"resnet50\", \"resnet101\"] # Stage 1 x = KL.ZeroPadding2D((3, 3))(input_image) x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x) x = BatchNorm(name='bn_conv1')(x, training=train_bn) x = KL.Activation('relu')(x) C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x) # Stage 2 x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn) x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_bn=train_bn) C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_bn=train_bn) # Stage 3 x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn) x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_bn=train_bn) x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_bn=train_bn) C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_bn=train_bn) # Stage 4 x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn) block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture] for i in range(block_count): x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_bn=train_bn) C4 = x # Stage 5 if stage5: x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn) x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_bn=train_bn) C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_bn=train_bn) else: C5 = None return [C1, C2, C3, C4, C5] anchor 锚框生成规则 在 Faster-RCNN 中可以将 SCALE 也可以设置为多个值，而在 Mask RCNN 中则是每一特征层只对应着一个SCALE 即对应着上述所设置的 16。 实验 何凯明在论文中做了很多对比单个模块试验，并放出了对比结果表格。 从上图表格可以看出： sigmoid 和 softmax 对比，sigmoid 有不小提升； 特征网络选择：可以看出更深的网络和采用 FPN 的实验效果更好，可能因为 FPN 综合考虑了不同尺寸的 feature map 的信息，因此能够把握一些更精细的细节。 RoI Align 和 RoI Pooling 对比：在 instance segmentation 和 object detection 上都有不小的提升。这样看来，RoIAlign 其实就是一个更加精准的 RoIPooling，把前者放到 Faster RCNN 中，对结果的提升应该也会有帮助。 参考资料 Mask R-CNN 论文 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/2D目标检测/5-Cascade-RCNN论文解读.html":{"url":"6-computer_vision/2D目标检测/5-Cascade-RCNN论文解读.html","title":"5-Cascade-RCNN论文解读","keywords":"","body":" 摘要 1，介绍 1.1，Faster RCNN 回顾 1.2，mismatch 问题 2，实验分析 2.1，改变IoU阈值对Detector性能的影响 2.2，提高IoU阈值的影响 2.3，和Iterative BBox比较 3，网络结构 参考资料 摘要 虽然低 IoU 阈值，如 0.5，会产生噪声检测（noisy detections），但是，随着 IoU 阈值的增加，检测性能往往会下降。造成这种情况的主要因素有两个：1）由于在训练过程中正样本呈指数下降，过少的正样本导致网络训练期间过拟合。2)dismatch：检测器在最优的 IoU 与输入预测的 IoU 之间会产生mismatch。由此，我们提出了多阶段的目标检测器结构：Cascade R-CNN 来解决 IoU 选择的问题。它由一系列不断增加 IoU 阈值的检测器组成，可以逐步的更接近目标的预测。。检测器是逐步训练的，前一个检测器输出一个良好的数据分布并作为输入，用于训练下一个更高质量的检测器。逐步改进的重采样保证了所有检测器都有一组相同大小的正样本，从而减少了过拟合问题。在 inference 阶段使用级联的检测器结构可以合理的提高了 IOU 的阈值而不会出现 mismatch 问题。 1，介绍 Cascade RCNN 是作者 Zhaowei Cai 于 2018 年发表的论文 Cascade R-CNN: Delving into High Quality Object Detection. 目标检测是一个复杂的问题，需要解决两个主要任务。首先，检测器必须解决识别问题，区分前景目标和背景目标，并为其分配匹配的类别标签。其次，探测器必须解决定位问题，为不同的目标分配精确的 bounding box。许多目标探测器都是基于两阶段网络框架 Faster R-CNN 的。双阶段检测网络是一个多任务学习问题，包括目标的分类和边界回归。与物体识别不同的是，定义正/负样本需要一个 IoU 阈值。通常使用的 IOU 阈值 u=0.5，0.5 对 IOU 的设置是相当低的。检测的目标经常包含很多噪声，如图 (a)所示。IOU 阈值取0.5，会有很多假的预测信息也都包含在内，从而会产生很多错误的预测信息。 1.1，Faster RCNN 回顾 先回顾下 Faster RCNN 的结构，下图是 Faster RCNN 的结构图。 training 阶段和 inference 阶段的不同在于，inference 阶段不能对 proposala 进行采样（因为不知道 gt，自然无法计算 IoU），所以 RPN 网络输出的 300 RoIs(Proposals)会直接输入到 RoI pooling 中，之后通过两个全连接层分别进行类别分类和 bbox 回归。 值得注意的是，Faster RCNN 网络在 RPN 和 Fast RCNN 阶段都需要计算 IoU，用于判定 positive 和 negative。前者是生成 256 个 Proposal 用于 RPN 网络训练，后者是生成 128 个 RoIs(可以理解为 RPN 网络优化后的 Proposals)用于 Fast RCNN 训练。 1.2，mismatch 问题 training 阶段和 inference 阶段，bbox 回归器的输入 proposals 分布是不一样的，training 阶段的输入proposals 质量更高(被采样过，IoU > threshold)，inference 阶段的输入 proposals 质量相对较差（没有被采样过，可能包括很多 IoU mismatch 问题，这个问题是固有存在的，但通常 threshold 取 0.5 时，mismatch 问题还不会很严重。 2，实验分析 2.1，改变IoU阈值对Detector性能的影响 从上图可以看出： 同一个 detector 通常只会在一个小范围的 IoU 阈值 内性能最好，比如 IoU 阈值为 0.5 的 detector，在输入 proposal 和 gt 的阈值为 0.55-0.6 范围内，其性能最好。阈值为 0.6 的 detector 则在 0.6~0.75 阈值范围内性能最佳。 几乎所有的检测器输出框的 IoU 都好于输入 proposal 的 IoU（红绿蓝三条曲线都在灰色对角线上方）。 2.2，提高IoU阈值的影响 主要是分析对提高 IoU 阈值对 RPN 输出 Proposal 数量的影响，实验结果如下图所示。 上图纵坐标表示 RPN 输出 proposal 在各个 IoU 范围内的数量。 第一张图表示级联结构的第一级，可以等同为没有级联结构的 RCNN 网络。从图中可以看出，随着 IoU 的增加，IoU 在 0.6,0.7 及以上范围内的 proposal 数量越来越少。虽然这样产生更高精度的 proposal，但是也带来了两个问题： 过拟合。 更严重的 mismatch 问题。RCNN 结构本身就存在这个问题，IoU 阈值的提高又加剧了这个问题。 第二、三图表示有级联结构的 RCNN，从图中可以看出，随着 stage 的加深，相应区域的依然拥有大量的 proposal，因此不会出现严重的过拟合的现象。 2.3，和Iterative BBox比较 Iterative BBox 的 H 位置都是共享的，而且 3 个分支的 IoU 阈值都取 0.5。Iterative BBox 存在两个问题： 单一阈值 0.5 是无法对所有 proposal 取得良好效果。 此外，detector 会改变样本的分布，使用同一个共享的 H 对检测是有影响的。作者做了下面的实验证明样本分布在各个stage 的变化。 红色表示离群点。 从上图可以看出，没经过一次回归，样本都会更靠近 gt，即输出的样本分布会逐渐变化，使用同一个阈值 0.5 的条件下，后面两个 stage 就会有较多的离群点，使用共享的 Head 网络权重是无法满足输入的变化的。 从上图还可以看出，每个阶段设置不同的 IoU 阈值，可以更好的去除离群点，从而适应不同的输入 proposal 分布。 3，网络结构 网络结构如下图(d) 上图中 (d) 和 (c) 很像，iterative bbox at inference 是在推断时候对回归框进行后处理，即模型输出预测结果后再多次处理，而 Cascade R-CNN 在训练的时候就进行重新采样，不同的 stage 的输入数据分布已经是不同的了。 简单来说 cascade R-CNN 是由一系列的检测模型组成，每个检测模型都基于不同 IOU 阈值的正负样本训练得到，前一个检测模型的输出作为后一个检测模型的输入，因此是 stage by stage 的训练方式，而且越往后的检测模型，其界定正负样本的 IOU 阈值是不断上升的。 Cascade R-CNN 的几个检测网络（Head 网络）是基于不同的 IOU 阈值确定的正负样本上训练得到的。 作者在 COCO 数据集上做了对比实验，达到了 state-of-the-art 精度。其中 backbone 为RsNet-101 的 Cascade RCNN 的 AP 达到了 42.8。 参考资料 Cascade R-CNN 详细解读 Cascade R-CNN解析 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/2D目标检测/6-RetinaNet网络详解.html":{"url":"6-computer_vision/2D目标检测/6-RetinaNet网络详解.html","title":"6-RetinaNet网络详解","keywords":"","body":"目录 [toc] 摘要 Retinanet 是作者 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection. 作者深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。 1，引言 作者认为一阶段检测器的精度不能和两阶段检测相比的原因主要在于，训练过程中的类别不平衡，由此提出了一种新的损失函数-Focal Loss。 R-CNN(Fast RCNN) 类似的检测器之所以能解决类别不平衡问题，是因为两阶段级联结构和启发式采样。提取 proposal 阶段（例如，选择性搜索、EdgeBoxes、DeepMask、RPN）很快的将候选对象位置的数量缩小到一个小数目（例如，1-2k），过滤掉大多数背景样本（其实就是筛选 anchor 数量）。在第二个分类阶段，执行启发式采样（sampling heuristics），例如固定的前景背景比（1:3），或在线难样本挖掘（OHEM），以保持前景和背景之间的平衡。 相比之下，单级检测器必须处理在图像中定期采样的一组更大的候选对象位置。实际上，这通常相当于枚举 ∼100k 个位置，这些位置密集地覆盖空间位置、尺度和纵横。虽然也可以应用类似的启发式采样方法，但效率低下，因为训练过程仍然由易于分类的背景样本主导。 2，相关工作 Two-stage Detectors: 与之前使用两阶段的分类器生成 proposal 不同，Faster RCNN 模型的 RPN 使用单个卷积就可以生成 proposal。 One-stage Detectors：最近的一些研究表明，只需要降低输入图像分辨率和 proposal 数量，两阶段检测器速度就可以变得更快。但是，对于一阶段检测器，即使提高模型计算量，其最后的精度也落后于两阶段方法[17]。同时，作者强调，Reinanet 达到很好的结果的原因不在于网络结构的创新，而在于损失函数的创新。 论文 [17] Speed/accuracy trade-offs for modern convolutional object detectors（注重实验）. 但是，从这几年看，一阶段检测器也可以达到很高的精度，甚至超过两阶段检测器，这几年的一阶段检测和两阶段检测器有相互融合的趋势了。 Class Imbalance: 早期的目标检测器 SSD 等在训练过程中会面临严重的类别不平衡（class imbalance）的问题，即正样本太少，负样本太多，这会导致两个问题： 训练效率低下：大多数候选区域都是容易分类的负样本，并没有提供多少有用的学习信号。 模型退化：易分类的负样本太多会压倒训练，导致模型退化。 通常的解决方案是执行某种形式的难负样本挖掘，如在训练时进行难负样本采样或更复杂的采样/重新称重方案。相比之下，Focla Loss 自然地处理了单级检测器所面临的类别不平衡，并且允许在所有示例上有效地训练，而不需要采样，也不需要容易的负样本来压倒损失和计算的梯度。 Robust Estimation: 人们对设计稳健的损失函数（例如 Huber loss）很感兴趣，该函数通过降低具有大错误的示例（硬示例）的损失来减少对总损失的贡献。相反， Focal Loss 对容易样本(inliers)减少权重来解决（address）类别不平衡问题（class imbalance），这意味着即使容易样本数量大，但是其对总的损失函数贡献也很小。换句话说，Focal Loss 与鲁棒损失相反，它侧重于训练稀疏的难样本。 3，网络架构 retinanet 的网络架构图如下所示。 3.1，Backbone Retinanet 的 Backbone 为 ResNet 网络，ResNet 一般从 18 层到 152 层（甚至更多）不等，主要区别在于采用的残差单元/模块不同或者堆叠残差单元/模块的数量和比例不同，论文主要使用 ResNet50。 两种残差块结构如下图所示，ResNet50 及更深的 ResNet 网络使用的是 bottleneck 残差块。 3.2，Neck Neck 模块即为 FPN 网络结构。FPN 模块接收 c3, c4, c5 三个特征图，输出 P2-P7 五个特征图，通道数都是 256, stride 为 (8,16,32,64,128)，其中大 stride (特征图小)用于检测大物体，小 stride (特征图大)用于检测小物体。P6 和 P7 目的是提供一个大感受野强语义的特征图，有利于大物体和超大物体检测。注意：在 RetinaNet 的 FPN 模块中只包括卷积，不包括 BN 和 ReLU。 3.3，Head Head 即预测头网络。 YOLOv3 的 neck 输出 3 个分支，即输出 3 个特征图， head 模块只有一个分支，由卷积层组成，该卷积层完成目标分类和位置回归的功能。总的来说，YOLOv3 网络的 3 个特征图有 3 个预测分支，分别预测 3 个框，也就是分别预测大、中、小目标。 Retinanet 的 neck 输出 5 个分支，即输出 5 个特征图。head 模块包括分类和位置检测两个分支，每个分支都包括 4 个卷积层，但是 head 模块的这两个分支之间参数不共享，分类 Head 输出通道是 A*K，A 是类别数；检测 head 输出通道是 4*K, K 是 anchor 个数, 虽然每个 Head 的分类和回归分支权重不共享，但是 5 个输出特征图的 Head 模块权重是共享的。 4，Focal Loss Focal Loss 是在二分类问题的交叉熵（CE）损失函数的基础上引入的，所以需要先学习下交叉熵损失的定义。 4.1，Cross Entropy 可额外阅读文章 理解交叉熵损失函数。 在深度学习中我们常使用交叉熵来作为分类任务中训练数据分布和模型预测结果分布间的代价函数。对于同一个离散型随机变量 $\\textrm{x}$ 有两个单独的概率分布 $P(x)$ 和 $Q(x)$，其交叉熵定义为： P 表示真实分布， Q 表示预测分布。 $$H(P,Q) = \\mathbb{E}{\\textrm{x}\\sim P} log Q(x)= -\\sum{i}P(x_i)logQ(x_i) \\tag{1} $$ 但在实际计算中，我们通常不这样写，因为不直观。在深度学习中，以二分类问题为例，其交叉熵损失（CE）函数如下： $$Loss = L(y, p) = -ylog(p)-(1-y)log(1-p) \\tag{2}$$ 其中 $p$ 表示当预测样本等于 $1$ 的概率，则 $1-p$ 表示样本等于 $0$ 的预测概率。因为是二分类，所以样本标签 $y$ 取值为 ${1,0}$，上式可缩写至如下： $$CE = \\left{\\begin{matrix} -log(p), & if \\quad y=1 \\ -log(1-p), & if\\quad y=0 \\tag{3} \\end{matrix}\\right.$$ 为了方便，用 $p_t$ 代表 $p$，$p_t$ 定义如下： $$p_t = \\left{\\begin{matrix} p, & if \\quad y=1 \\ 1-p, & if\\quad y=0 \\end{matrix}\\right.$$ 则$(3)$式可写成： $$CE(p, y) = CE(p_t) = -log(p_t) \\tag{4}$$ 前面的交叉熵损失计算都是针对单个样本的，对于所有样本，二分类的交叉熵损失计算如下： $$L = \\frac{1}{N}(\\sum{y_i = 1}^{m}-log(p)-\\sum{y_i = 0}^{n}log(1-p))$$ 其中 $m$ 为正样本个数，$n$ 为负样本个数，$N$ 为样本总数，$m+n=N$。当样本类别不平衡时，损失函数 $L$ 的分布也会发生倾斜，如 $m \\ll n$ 时，负样本的损失会在总损失占主导地位。又因为损失函数的倾斜，模型训练过程中也会倾向于样本多的类别，造成模型对少样本类别的性能较差。 再衍生以下，对于所有样本，多分类的交叉熵损失计算如下： $$L = \\frac{1}{N} \\sumi^N L_i = -\\frac{1}{N}(\\sum_i \\sum{c=1}^M y{ic}log(p{ic})$$ 其中，$M$ 表示类别数量，$y{ic}$ 是符号函数，如果样本 $i$ 的真实类别等于 $c$ 取值 1，否则取值 0; $p{ic}$ 表示样本 $i$ 预测为类别 $c$ 的概率。 对于多分类问题，交叉熵损失一般会结合 softmax 激活一起实现，PyTorch 代码如下，代码出自这里。 import numpy as np # 交叉熵损失 class CrossEntropyLoss(): \"\"\" 对最后一层的神经元输出计算交叉熵损失 \"\"\" def __init__(self): self.X = None self.labels = None def __call__(self, X, labels): \"\"\" 参数： X: 模型最后fc层输出 labels: one hot标注，shape=(batch_size, num_class) \"\"\" self.X = X self.labels = labels return self.forward(self.X) def forward(self, X): \"\"\" 计算交叉熵损失 参数： X：最后一层神经元输出，shape=(batch_size, C) label：数据onr-hot标注，shape=(batch_size, C) return： 交叉熵loss \"\"\" self.softmax_x = self.softmax(X) log_softmax = self.log_softmax(self.softmax_x) cross_entropy_loss = np.sum(-(self.labels * log_softmax), axis=1).mean() return cross_entropy_loss def backward(self): grad_x = (self.softmax_x - self.labels) # 返回的梯度需要除以batch_size return grad_x / self.X.shape[0] def log_softmax(self, softmax_x): \"\"\" 参数: softmax_x, 在经过softmax处理过的X return: log_softmax处理后的结果shape = (m, C) \"\"\" return np.log(softmax_x + 1e-5) def softmax(self, X): \"\"\" 根据输入，返回softmax 代码利用softmax函数的性质: softmax(x) = softmax(x + c) \"\"\" batch_size = X.shape[0] # axis=1 表示在二维数组中沿着横轴进行取最大值的操作 max_value = X.max(axis=1) #每一行减去自己本行最大的数字,防止取指数后出现inf，性质：softmax(x) = softmax(x + c) # 一定要新定义变量，不要用-=，否则会改变输入X。因为在调用计算损失时，多次用到了softmax，input不能改变 tmp = X - max_value.reshape(batch_size, 1) # 对每个数取指数 exp_input = np.exp(tmp) # shape=(m, n) # 求出每一行的和 exp_sum = exp_input.sum(axis=1, keepdims=True) # shape=(m, 1) return exp_input / exp_sum 4.2，Balanced Cross Entropy 对于正负样本不平衡的问题，较为普遍的做法是引入 $\\alpha \\in(0,1)$ 参数来解决，上面公式重写如下： $$CE(p_t) = -\\alpha log(p_t) = \\left{\\begin{matrix} -\\alpha log(p), & if \\quad y=1\\ -(1-\\alpha)log(1-p), & if\\quad y=0 \\end{matrix}\\right.$$ 对于所有样本，二分类的平衡交叉熵损失函数如下： $$L = \\frac{1}{N}(\\sum{y_i = 1}^{m}-\\alpha log(p)-\\sum{y_i = 0}^{n}(1 - \\alpha) log(1-p))$$ 其中 $\\frac{\\alpha}{1-\\alpha} = \\frac{n}{m}$，即 $\\alpha$ 参数的值是根据正负样本分布比例来决定的， 4.3，Focal Loss Definition 虽然 $\\alpha$ 参数平衡了正负样本（positive/negative examples），但是它并不能区分难易样本（easy/hard examples），而实际上，目标检测中大量的候选目标都是易分样本。这些样本的损失很低，但是由于难易样本数量极不平衡，易分样本的数量相对来讲太多，最终主导了总的损失。而本文的作者认为，易分样本（即，置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本（这个假设是有问题的，是 GHM 的主要改进对象） Focal Loss 作者建议在交叉熵损失函数上加上一个调整因子（modulating factor）$(1-p_t)^\\gamma$，把高置信度 $p$（易分样本）样本的损失降低一些。Focal Loss 定义如下： $$FL(p_t) = -(1-p_t)^\\gamma log(p_t) = \\left{\\begin{matrix} -(1-p)^\\gamma log(p), & if \\quad y=1 \\ -p^\\gamma log(1-p), & if\\quad y=0 \\end{matrix}\\right.$$ Focal Loss 有两个性质： 当样本被错误分类且 $p_t$ 值较小时，调制因子接近于 1，loss 几乎不受影响；当 $p_t$ 接近于 1，调质因子（factor）也接近于 0，容易分类样本的损失被减少了权重，整体而言，相当于增加了分类不准确样本在损失函数中的权重。 $\\gamma$ 参数平滑地调整容易样本的权重下降率，当 $\\gamma = 0$ 时，Focal Loss 等同于 CE Loss。 $\\gamma$ 在增加，调制因子的作用也就增加，实验证明 $\\gamma = 2$ 时，模型效果最好。 直观地说，调制因子减少了简单样本的损失贡献，并扩大了样本获得低损失的范围。例如，当$\\gamma = 2$ 时，与 $CE$ 相比，分类为 $p_t = 0.9$ 的样本的损耗将降低 100 倍，而当 $p_t = 0.968$ 时，其损耗将降低 1000 倍。这反过来又增加了错误分类样本的重要性（对于 $pt≤0.5$ 和 $\\gamma = 2$，其损失最多减少 4 倍）。在训练过程关注对象的排序为正难 > 负难 > 正易 > 负易。 在实践中，我们常采用带 $\\alpha$ 的 Focal Loss： $$FL(p_t) = -\\alpha (1-p_t)^\\gamma log(p_t)$$ 作者在实验中采用这种形式，发现它比非 $\\alpha$ 平衡形式（non-$\\alpha$-balanced）的精确度稍有提高。实验表明 $\\gamma$ 取 2，$\\alpha$ 取 0.25 的时候效果最佳。 网上有各种版本的 Focal Loss 实现代码，大多都是基于某个深度学习框架实现的，如 Pytorch和 TensorFlow，我选取了一个较为清晰的通用版本代码作为参考，代码来自 这里。 后续有必要自己实现以下，有时间还要去看看 Caffe 的实现。这里的 Focal Loss 代码与后文不同，这里只是纯粹的用于分类的 Focal_loss 代码，不包含 BBox 的编码过程。 # -*- coding: utf-8 -*- # @Author : LG from torch import nn import torch from torch.nn import functional as F class focal_loss(nn.Module): def __init__(self, alpha=0.25, gamma=2, num_classes = 3, size_average=True): \"\"\" focal_loss损失函数, -α(1-yi)**γ *ce_loss(xi,yi) 步骤详细的实现了 focal_loss损失函数. :param alpha: 阿尔法α,类别权重. 当α是列表时,为各类别权重,当α为常数时,类别权重为[α, 1-α, 1-α, ....],常用于 目标检测算法中抑制背景类 , retainnet中设置为0.25 :param gamma: 伽马γ,难易样本调节参数. retainnet中设置为2 :param num_classes: 类别数量 :param size_average: 损失计算方式,默认取均值 \"\"\" super(focal_loss,self).__init__() self.size_average = size_average if isinstance(alpha,list): assert len(alpha)==num_classes # α可以以list方式输入,size:[num_classes] 用于对不同类别精细地赋予权重 print(\" --- Focal_loss alpha = {}, 将对每一类权重进行精细化赋值 --- \".format(alpha)) self.alpha = torch.Tensor(alpha) else: assert alpha mmdetection 框架给出的 focal loss 代码如下（有所删减）： # This method is only for debugging def py_sigmoid_focal_loss(pred, target, weight=None, gamma=2.0, alpha=0.25, reduction='mean', avg_factor=None): \"\"\"PyTorch version of `Focal Loss `_. Args: pred (torch.Tensor): The prediction with shape (N, C), C is the number of classes target (torch.Tensor): The learning label of the prediction. weight (torch.Tensor, optional): Sample-wise loss weight. gamma (float, optional): The gamma for calculating the modulating factor. Defaults to 2.0. alpha (float, optional): A balanced form for Focal Loss. Defaults to 0.25. reduction (str, optional): The method used to reduce the loss into a scalar. Defaults to 'mean'. avg_factor (int, optional): Average factor that is used to average the loss. Defaults to None. \"\"\" pred_sigmoid = pred.sigmoid() target = target.type_as(pred) pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target) focal_weight = (alpha * target + (1 - alpha) * (1 - target)) * pt.pow(gamma) loss = F.binary_cross_entropy_with_logits( pred, target, reduction='none') * focal_weigh return loss 5，代码解读 代码来源这里。 5.1，Backbone RetinaNet 算法采用了 ResNet50 作为 Backbone, 并且考虑到整个目标检测网络比较大，前面部分网络没有进行训练，BN 也不会进行参数更新（来自 OpenMMLab 的经验）。 ResNet 不仅提出了残差结构，而且还提出了骨架网络设计范式即 stem + n stage+ cls head，对于 ResNet 而言，其实际 forward 流程是 stem -> 4 个 stage -> 分类 head，stem 的输出 stride 是 4，而 4 个 stage 的输出 stride 是 4,8,16,32。 stride 表示模型的下采样率，假设图片输入是 320x320，stride=10，那么输出特征图大小是 32x32 ，假设每个位置 anchor 是 9 个，那么这个输出特征图就一共有 32x32x9 个 anchor。 5.2，Neck ResNet 输出 4 个不同尺度的特征图（c2,c3,c4,c5），stride 分别是（4,8,16,32），通道数为（256,512,1024,2048）。 Neck 使用的是 FPN 网络，且输入是 3 个来自 ResNet 输出的特征图（c3,c4,c5），并输出 5 个特征图（p3,p4,p5,p6,p7），额外输出的 2 个特征图的来源是骨架网络输出，而不是 FPN 层本身输出又作为后面层的输入，并且 FPN 网络输出的 5 个特征图通道数都是 256。值得注意的是，Neck 模块输出特征图的大小是由 Backbone 决定的，即输出的 stride 列表由 Backbone 确定。 FPN 结构的代码如下。 class PyramidFeatures(nn.Module): def __init__(self, C3_size, C4_size, C5_size, feature_size=256): super(PyramidFeatures, self).__init__() # upsample C5 to get P5 from the FPN paper self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0) self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest') self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # add P5 elementwise to C4 self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0) self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest') self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # add P4 elementwise to C3 self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0) self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # \"P6 is obtained via a 3x3 stride-2 conv on C5\" self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1) # \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\" self.P7_1 = nn.ReLU() self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1) def forward(self, inputs): C3, C4, C5 = inputs P5_x = self.P5_1(C5) P5_upsampled_x = self.P5_upsampled(P5_x) P5_x = self.P5_2(P5_x) P4_x = self.P4_1(C4) P4_x = P5_upsampled_x + P4_x P4_upsampled_x = self.P4_upsampled(P4_x) P4_x = self.P4_2(P4_x) P3_x = self.P3_1(C3) P3_x = P3_x + P4_upsampled_x P3_x = self.P3_2(P3_x) P6_x = self.P6(C5) P7_x = self.P7_1(P6_x) P7_x = self.P7_2(P7_x) return [P3_x, P4_x, P5_x, P6_x, P7_x] 5.3，Head RetinaNet 在特征提取网络 ResNet-50 和特征融合网络 FPN 后，对获得的五张特征图 [P3_x, P4_x, P5_x, P6_x, P7_x]，通过具有相同权重的框回归和分类子网络，获得所有框位置和类别信息。 目标边界框回归和分类子网络（head 网络）定义如下： class RegressionModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, feature_size=256): super(RegressionModel, self).__init__() self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() # 最后的输出层输出通道数为 num_anchors * 4 self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1) def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) # out is B x C x W x H, with C = 4*num_anchors = 4*9 out = out.permute(0, 2, 3, 1) return out.contiguous().view(out.shape[0], -1, 4) class ClassificationModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256): super(ClassificationModel, self).__init__() self.num_classes = num_classes self.num_anchors = num_anchors self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() # 最后的输出层输出通道数为 num_anchors * num_classes(coco数据集9*80) self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1) self.output_act = nn.Sigmoid() def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) out = self.output_act(out) # out is B x C x W x H, with C = n_classes + n_anchors out1 = out.permute(0, 2, 3, 1) batch_size, width, height, channels = out1.shape out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes) return out2.contiguous().view(x.shape[0], -1, self.num_classes) 5.4，先验框Anchor赋值 1，生成各个特征图对应原图大小的所有 Anchors 坐标的代码如下。 import numpy as np import torch import torch.nn as nn class Anchors(nn.Module): def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None): super(Anchors, self).__init__() if pyramid_levels is None: self.pyramid_levels = [3, 4, 5, 6, 7] if strides is None: self.strides = [2 ** x for x in self.pyramid_levels] if sizes is None: self.sizes = [2 ** (x + 2) for x in self.pyramid_levels] if ratios is None: self.ratios = np.array([0.5, 1, 2]) if scales is None: self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) def forward(self, image): image_shape = image.shape[2:] image_shape = np.array(image_shape) image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels] # compute anchors over all pyramid levels all_anchors = np.zeros((0, 4)).astype(np.float32) for idx, p in enumerate(self.pyramid_levels): anchors = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales) shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors) all_anchors = np.append(all_anchors, shifted_anchors, axis=0) all_anchors = np.expand_dims(all_anchors, axis=0) if torch.cuda.is_available(): return torch.from_numpy(all_anchors.astype(np.float32)).cuda() else: return torch.from_numpy(all_anchors.astype(np.float32)) def generate_anchors(base_size=16, ratios=None, scales=None): \"\"\"生成的 `9` 个 `base anchors` Generate anchor (reference) windows by enumerating aspect ratios X scales w.r.t. a reference window. \"\"\" if ratios is None: ratios = np.array([0.5, 1, 2]) if scales is None: scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) num_anchors = len(ratios) * len(scales) # initialize output anchors anchors = np.zeros((num_anchors, 4)) # scale base_size anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T # compute areas of anchors areas = anchors[:, 2] * anchors[:, 3] # correct for ratios anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales))) anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales)) # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2) anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T return anchors def shift(shape, stride, anchors): shift_x = (np.arange(0, shape[1]) + 0.5) * stride shift_y = (np.arange(0, shape[0]) + 0.5) * stride shift_x, shift_y = np.meshgrid(shift_x, shift_y) shifts = np.vstack(( shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel() )).transpose() # add A anchors (1, A, 4) to # cell K shifts (K, 1, 4) to get # shift anchors (K, A, 4) # reshape to (K*A, 4) shifted anchors A = anchors.shape[0] K = shifts.shape[0] all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))) all_anchors = all_anchors.reshape((K * A, 4)) return all_anchors shift 函数是将 generate_anchors 函数生成的 9 个 base anchors 按固定长度进行平移，然后和其对应特征图的 cell进行对应。经过对每个特征图（5 个）都做类似的变换，就能生成全部anchor。具体过程如下图所示。 anchor 平移图来源这里 2，计算得到输出特征图上面每个点对应的原图 anchor坐标输出特征图上面每个点对应的原图 anchor坐标后，就可以和 gt 信息计算每个 anchor 的正负样本属性。具体过程总结如下： 如果 anchor 和所有 gt bbox 的最大 iou 值小于 0.4，那么该 anchor 就是背景样本； 如果 anchor 和所有 gt bbox 的最大 iou 值大于等于 0.5，那么该 anchor 就是高质量正样本； 如果 gt bbox 和所有 anchor 的最大 iou 值大于等于 0(可以看出每个 gt bbox 都一定有至少一个 anchor 匹配)，那么该 gt bbox 所对应的 anchor 也是正样本； 其余样本全部为忽略样本即 anchor 和所有 gt bbox 的最大 iou 值处于 [0.4,0.5) 区间的 anchor 为忽略样本，不计算 loss 5.5，BBox Encoder Decoder 在 anchor-based 算法中，为了利用 anchor 信息进行更快更好的收敛，一般会对 head 输出的 bbox 分支 4 个值进行编解码操作，作用有两个： 更好的平衡分类和回归分支 loss，以及平衡 bbox 四个预测值的 loss。 训练过程中引入 anchor 信息，加快收敛。 RetinaNet 采用的编解码函数是主流的 DeltaXYWHBBoxCoder，在 OpenMMlab 代码中的配置如下： bbox_coder=dict( type='DeltaXYWHBBoxCoder', target_means=[.0, .0, .0, .0], target_stds=[1.0, 1.0, 1.0, 1.0]), target_means 和 target_stds 相当于对 bbox 回归的 4 个 tx ty tw th 进行变换。在不考虑 target_means 和 target_stds 情况下，其编码公式如下： $$t{x}^{\\ast } = (x^{\\ast }-x{a})/w{a}, t{y}^{\\ast}=(y^{\\ast}-y{a})/h{a} \\\\ t{w}^{\\ast } = log(w^{\\ast }/w{a}), t{h}^{\\ast }=log(h^{\\ast }/h{a}) $$ ${x}^{\\ast },y^{\\ast}$ 是 gt bbox 的中心 xy 坐标， $w^{\\ast },h^{\\ast }$ 是 gt bbox 的 wh 值， $x{a},y{a}$ 是 anchor 的中心 xy 坐标， $w{a},h{a}$ 是 anchor 的 wh 值， $t^{\\ast }$ 是预测头的 bbox 分支输出的 4 个值对应的 targets。可以看出 $t_x,t_y$ 预测值表示 gt bbox 中心相对于 anchor 中心点的偏移，并且通过除以 anchor 的 $wh$ 进行归一化；而 $t_w,t_h$ 预测值表示 gt bbox 的 $wh$ 除以 anchor 的 $wh$，然后取 log 非线性变换即可。 Variables $x$, $x_a$, and $x^{\\ast }$ are for the predicted box, anchor box, and groundtruth box respectively (likewise for y; w; h). 1，考虑编码过程存在 target_means 和 target_stds 情况下，则 anchor 的 bbox 对应的 target 编码的核心代码如下： dx = (gx - px) / pw dy = (gy - py) / ph dw = torch.log(gw / pw) dh = torch.log(gh / ph) deltas = torch.stack([dx, dy, dw, dh], dim=-1) # 最后减掉均值，处于标准差 means = deltas.new_tensor(means).unsqueeze(0) stds = deltas.new_tensor(stds).unsqueeze(0) deltas = deltas.sub_(means).div_(stds) 2，解码过程是编码过程的反向，比较容易理解，其核心代码如下： # 先乘上 std，加上 mean means = deltas.new_tensor(means).view(1, -1).repeat(1, deltas.size(1) // 4) stds = deltas.new_tensor(stds).view(1, -1).repeat(1, deltas.size(1) // 4) denorm_deltas = deltas * stds + means dx = denorm_deltas[:, 0::4] dy = denorm_deltas[:, 1::4] dw = denorm_deltas[:, 2::4] dh = denorm_deltas[:, 3::4] # wh 解码 gw = pw * dw.exp() gh = ph * dh.exp() # 中心点 xy 解码 gx = px + pw * dx gy = py + ph * dy # 得到 x1y1x2y2 的 gt bbox 预测坐标 x1 = gx - gw * 0.5 y1 = gy - gh * 0.5 x2 = gx + gw * 0.5 y2 = gy + gh * 0.5 5.6，Focal Loss Focal Loss 属于 CE Loss 的动态加权版本，其可以根据样本的难易程度(预测值和 label 的差距可以反映)对每个样本单独加权，易学习样本在总的 loss 中的权重比较低，难样本权重比较高。特征图上输出的 anchor 坐标列表的大部分都是属于背景且易学习的样本，虽然单个 loss 比较小，但是由于数目众多最终会主导梯度，从而得到次优模型，而 Focal Loss 通过指数效应把大量易学习样本的权重大大降低，从而避免上述问题。 为了便于理解，先给出 Focal Loss 的核心代码。 pred_sigmoid = pred.sigmoid() # one-hot 格式 target = target.type_as(pred) pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target) focal_weight = (alpha * target + (1 - alpha) * (1 - target)) * pt.pow(gamma) loss = F.binary_cross_entropy_with_logits( pred, target, reduction='none') * focal_weight loss = weight_reduce_loss(loss, weight, reduction, avg_factor) return loss 结合 BBox Assigner（BBox 正负样本确定） 和 BBox Encoder （BBox target 计算）的代码，可得完整的 Focla Loss 代码如下所示。 class FocalLoss(nn.Module): #def __init__(self): def forward(self, classifications, regressions, anchors, annotations): alpha = 0.25 gamma = 2.0 batch_size = classifications.shape[0] classification_losses = [] regression_losses = [] anchor = anchors[0, :, :] anchor_widths = anchor[:, 2] - anchor[:, 0] anchor_heights = anchor[:, 3] - anchor[:, 1] anchor_ctr_x = anchor[:, 0] + 0.5 * anchor_widths anchor_ctr_y = anchor[:, 1] + 0.5 * anchor_heights for j in range(batch_size): classification = classifications[j, :, :] regression = regressions[j, :, :] bbox_annotation = annotations[j, :, :] bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1] classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4) if bbox_annotation.shape[0] == 0: if torch.cuda.is_available(): alpha_factor = torch.ones(classification.shape).cuda() * alpha alpha_factor = 1. - alpha_factor focal_weight = classification focal_weight = alpha_factor * torch.pow(focal_weight, gamma) bce = -(torch.log(1.0 - classification)) # cls_loss = focal_weight * torch.pow(bce, gamma) cls_loss = focal_weight * bce classification_losses.append(cls_loss.sum()) regression_losses.append(torch.tensor(0).float().cuda()) else: alpha_factor = torch.ones(classification.shape) * alpha alpha_factor = 1. - alpha_factor focal_weight = classification focal_weight = alpha_factor * torch.pow(focal_weight, gamma) bce = -(torch.log(1.0 - classification)) # cls_loss = focal_weight * torch.pow(bce, gamma) cls_loss = focal_weight * bce classification_losses.append(cls_loss.sum()) regression_losses.append(torch.tensor(0).float()) continue IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4]) # num_anchors x num_annotations IoU_max, IoU_argmax = torch.max(IoU, dim=1) # num_anchors x 1 #import pdb #pdb.set_trace() # compute the loss for classification targets = torch.ones(classification.shape) * -1 if torch.cuda.is_available(): targets = targets.cuda() targets[torch.lt(IoU_max, 0.4), :] = 0 positive_indices = torch.ge(IoU_max, 0.5) num_positive_anchors = positive_indices.sum() assigned_annotations = bbox_annotation[IoU_argmax, :] targets[positive_indices, :] = 0 targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1 if torch.cuda.is_available(): alpha_factor = torch.ones(targets.shape).cuda() * alpha else: alpha_factor = torch.ones(targets.shape) * alpha alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor) focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification) focal_weight = alpha_factor * torch.pow(focal_weight, gamma) bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification)) # cls_loss = focal_weight * torch.pow(bce, gamma) cls_loss = focal_weight * bce if torch.cuda.is_available(): cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, torch.zeros(cls_loss.shape).cuda()) else: cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, torch.zeros(cls_loss.shape)) classification_losses.append(cls_loss.sum()/torch.clamp(num_positive_anchors.float(), min=1.0)) # compute the loss for regression if positive_indices.sum() > 0: assigned_annotations = assigned_annotations[positive_indices, :] anchor_widths_pi = anchor_widths[positive_indices] anchor_heights_pi = anchor_heights[positive_indices] anchor_ctr_x_pi = anchor_ctr_x[positive_indices] anchor_ctr_y_pi = anchor_ctr_y[positive_indices] gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0] gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1] gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights # clip widths to 1 gt_widths = torch.clamp(gt_widths, min=1) gt_heights = torch.clamp(gt_heights, min=1) targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi targets_dw = torch.log(gt_widths / anchor_widths_pi) targets_dh = torch.log(gt_heights / anchor_heights_pi) targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh)) targets = targets.t() if torch.cuda.is_available(): targets = targets/torch.Tensor([[0.1, 0.1, 0.2, 0.2]]).cuda() else: targets = targets/torch.Tensor([[0.1, 0.1, 0.2, 0.2]]) negative_indices = 1 + (~positive_indices) regression_diff = torch.abs(targets - regression[positive_indices, :]) regression_loss = torch.where( torch.le(regression_diff, 1.0 / 9.0), 0.5 * 9.0 * torch.pow(regression_diff, 2), regression_diff - 0.5 / 9.0 ) regression_losses.append(regression_loss.mean()) else: if torch.cuda.is_available(): regression_losses.append(torch.tensor(0).float().cuda()) else: regression_losses.append(torch.tensor(0).float()) return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(dim=0, keepdim=True) 参考资料 https://github.com/yhenon/pytorch-retinanet RetinaNet 论文和代码详解 轻松掌握 MMDetection 中常用算法(一)：RetinaNet 及配置详解 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/2D目标检测/7-YOLOv1-v5论文解读.html":{"url":"6-computer_vision/2D目标检测/7-YOLOv1-v5论文解读.html","title":"7-YOLOv1-v5论文解读","keywords":"","body":" 一，YOLOv1 Abstract 1. Introduction 2. Unified Detectron 2.1. Network Design 2.2 Training 2.4. Inferences 4.1 Comparison to Other Real-Time Systems 5，代码实现思考 二，YOLOv2 摘要 YOLOv2 的改进 1，中心坐标位置预测的改进 2，1 个 gird 只能对应一个目标的改进 3，backbone 的改进 4，多尺度训练 损失函数 三，YOLOv3 摘要 1，介绍 2，改进 2.1，边界框预测 2.2，分类预测 2.3，跨尺度预测 2.4，新的特征提取网络 2.5，训练 2.5，推理 3，实验结果 4，失败的尝试 5，改进的意义 四，YOLOv4 1，摘要及介绍 2，相关工作 2.1，目标检测方法 2.2，Bag of freebies（免费技巧） 2.3，Bag of specials（即插即用模块+后处理方法） 3，方法 3.1，架构选择 3.2，Selection of BoF and BoS 3.3，额外的改进 3.4 YOLOv4 4，实验 4.1，实验设置 4.2，对于分类器训练过程中不同特性的影响 4.3，对于检测器训练过程中不同特性的影响 4.4，不同骨干和预训练权重对检测器训练的影响 4.5，不同小批量的大小对检测器训练的影响 5，结果 6，YOLOv4 主要改进点 6.1，Backbone 改进 6.1.1，CSPDarknet53 6.1.2，Mish 激活 6.1.3，Dropblock 6.2，Neck 网络改进 6.3，预测的改进 6.3.1，使用CIoU Loss 6.3.2，使用DIoU_NMS 6.4，输入端改进 6.4.1，Mosaic 数据增强 五，YOLOv5 5.1，网络架构 5.2，创新点 5.2.1，自适应anchor 5.2.2， 自适应图片缩放 5.2.3，Focus结构 5.3，四种网络结构 5.4，实验结果 参考资料 一，YOLOv1 YOLOv1 出自 2016 CVPR 论文 You Only Look Once:Unified, Real-Time Object Detection. YOLO 系列算法的核心思想是将输入的图像经过 backbone 提取特征后，将得到特征图划分为 S x S 的网格，物体的中心落在哪一个网格内，这个网格就负责预测该物体的置信度、类别以及坐标位置。 Abstract 作者提出了一种新的目标检测方法 YOLO，之前的目标检测工作都是重新利用分类器来执行检测。作者的神经网络模型是端到端的检测，一次运行即可同时得到所有目标的边界框和类别概率。 YOLO 架构的速度是非常快的，base 版本实时帧率为 45 帧，smaller 版本能达到每秒 155 帧，性能由于 DPM 和 R-CNN 等检测方法。 1. Introduction 之前的目标检测器是重用分类器来执行检测，为了检测目标，这些系统在图像上不断遍历一个框，并利用分类器去判断这个框是不是目标。像可变形部件模型（DPM）使用互动窗口方法，其分类器在整个图像的均匀间隔的位置上运行。 作者将目标检测看作是单一的回归问题，直接从图像像素得到边界框坐标和类别概率。 YOLO 检测系统如图 1 所示。单个检测卷积网络可以同时预测多个目标的边界框和类别概率。YOLO 和传统的目标检测方法相比有诸多优点。 首先，YOLO 速度非常快，我们将检测视为回归问题，所以检测流程也简单。其次，YOLO 在进行预测时，会对图像进行全面地推理。第三，YOLO 模型具有泛化能力，其比 DPM 和R-CNN 更好。最后，虽然 YOLO 模型在精度上依然落后于最先进（state-of-the-art）的检测系统，但是其速度更快。 2. Unified Detectron YOLO 系统将输入图像划分成 $S\\times S$ 的网格（grid），然后让每个gird 负责检测那些中心点落在 grid 内的目标。 检测任务：每个网络都会预测 $B$ 个边界框及边界框的置信度分数，所谓置信度分数其实包含两个方面：一个是边界框含有目标的可能性，二是边界框的准确度。前者记为 $Pr(Object)$，当边界框包含目标时，$Pr(Object)$ 值为 1，否则为 0；后者记为 $IOU{pred}^{truth}$，即预测框与真实框的 IOU。因此形式上，我们将置信度定义为 $Pr(Object)*IOU{pred}^{truth}$。如果 grid 不存在目标，则置信度分数置为 0，否则，置信度分数等于预测框和真实框之间的交集（IoU）。 每个边界框（bounding box）包含 5 个预测变量：$x$，$y$，$w$，$h$ 和 confidence。$(x,y)$ 坐标不是边界框中心的实际坐标，而是相对于网格单元左上角坐标的偏移（需要看代码才能懂，论文只描述了出“相对”的概念）。而边界框的宽度和高度是相对于整个图片的宽与高的比例，因此理论上以上 4 预测量都应该在 $[0,1]$ 范围之内。最后，置信度预测表示预测框与实际边界框之间的 IOU。 值得注意的是，中心坐标的预测值 $(x,y)$ 是相对于每个单元格左上角坐标点的偏移值，偏移量 = 目标位置 - grid的位置。 分类任务：每个网格单元（grid）还会预测 $C$ 个类别的概率 $Pr(Class_i)|Object)$。grid 包含目标时才会预测 $Pr$，且只预测一组类别概率，而不管边界框 $B$ 的数量是多少。 在推理时，我们乘以条件概率和单个 box 的置信度。 $$Pr(Classi)|Object)Pr(Object)IOU{pred}^{truth} = Pr(Classi)*IOU{pred}^{truth}$$ 它为我们提供了每个框特定类别的置信度分数。这些分数编码了该类出现在框中的概率以及预测框拟合目标的程度。 在 Pscal VOC 数据集上评测 YOLO 模型时，我们设置 $S=7$, $B=2$（即每个 grid 会生成 2 个边界框）。Pscal VOC 数据集有 20 个类别，所以 $C=20$。所以，模型最后预测的张量维度是 $7 \\times 7\\times (20+5*2) = 1470$。 总结：YOLO 系统将检测建模为回归问题。它将图像分成 $S \\times S$ 的 gird，每个 grid 都会预测 $B$ 个边界框，同时也包含 $C$ 个类别的概率，这些预测对应的就是 $S \\times S \\times (C + 5*B)$。 这里其实就是在描述 YOLOv1 检测头如何设计：回归网络的设计 + 训练集标签如何构建（即 yoloDataset 类的构建），下面给出一份针对 voc 数据集编码为 yolo 模型的输入标签数据的函数，读懂了这个代码，就能理解前面部分的描述。 代码来源这里。 def encoder(self, boxes, labels): ''' boxes (tensor) [[x1,y1,x2,y2],[]] 目标的边界框坐标信息 labels (tensor) [...] 目标的类别信息 return 7x7x30 ''' grid_num = 7 # 论文中设为7 target = torch.zeros((grid_num, grid_num, 30)) # 和模型输出张量维尺寸一样都是 14*14*30 cell_size = 1./grid_num # 之前已经把目标框的坐标进行了归一化（这里与原论文有区别），故这里用1.作为除数 # 计算目标框中心点坐标和宽高 wh = boxes[:, 2:]-boxes[:, :2] cxcy = (boxes[:, 2:]+boxes[:, :2])/2 # 1，遍历各个目标框； for i in range(cxcy.size()[0]): # 对应于数据集中的每个框 这里cxcy.size()[0] == num_samples # 2，计算第 i 个目标中心点落在哪个 `grid` 上，`target` 相应位置的两个框的置信度值设为 `1`，同时对应类别值也置为 `1`； cxcy_sample = cxcy[i] ij = (cxcy_sample/cell_size).ceil()-1 # ij 是一个list, 表示目标中心点cxcy在归一化后的图片中所处的x y 方向的第几个网格 # [0,1,2,3,4,5,6,7,8,9, 10-19] 对应索引 # [x,y,w,h,c,x,y,w,h,c, 20 个类别的 one-hot编码] 与原论文输出张量维度各个索引对应目标有所区别 target[int(ij[1]), int(ij[0]), 4] = 1 # 第一个框的置信度 target[int(ij[1]), int(ij[0]), 9] = 1 # 第二个框的置信度 target[int(ij[1]), int(ij[0]), int(labels[i])+9] = 1 # 第 int(labels[i])+9 个类别为 1 # 3，计算目标中心所在 `grid`（网格）的左上角相对坐标：`ij*cell_size`，然后目标中心坐标相对于子网格左上角的偏移比例 `delta_xy`； xy = ij*cell_size delta_xy = (cxcy_sample -xy)/cell_size # 4，最后将 `target` 对应网格位置的 (x, y, w, h) 分别赋相应 `wh`、`delta_xy` 值。 target[int(ij[1]), int(ij[0]), 2:4] = wh[i] # 范围为(0,1) target[int(ij[1]), int(ij[0]), :2] = delta_xy target[int(ij[1]), int(ij[0]), 7:9] = wh[i] target[int(ij[1]), int(ij[0]), 5:7] = delta_xy return target 代码分析，一张图片对应的标签张量 target 的维度是 $7 \\times 7 \\times 30$。然后分别对各个目标框的 boxes: $(x1,y1,x2,y2)$ 和 labels：(0,0,...,1,0)(one-hot 编码的目标类别信息）进行处理，符合检测系统要求的输入形式。算法步骤如下： 计算目标框中心点坐标和宽高，并遍历各个目标框； 计算目标中心点落在哪个 grid 上，target 相应位置的两个框的置信度值设为 1，同时对应类别值也置为 1； 计算目标中心所在 grid（网格）的左上角相对坐标：ij*cell_size，然后目标中心坐标相对于子网格左上角的偏移比例 delta_xy； 最后将 target 对应网格位置的 $(x, y, w, h)$ 分别赋相应 wh、delta_xy 值。 2.1. Network Design YOLO 模型使用卷积神经网络来实现，卷积层负责从图像中提取特征，全连接层预测输出类别概率和坐标。 YOLO 的网络架构受 GooLeNet 图像分类模型的启发。网络有 24 个卷积层，最后面是 2 个全连接层。整个网络的卷积只有 $1 \\times 1$ 和 $3 \\times 3$ 卷积层，其中 $1 \\times 1$ 卷积负责降维 ，而不是 GoogLeNet 的 Inception 模块。 图3：网络架构。作者在 ImageNet 分类任务上以一半的分辨率（输入图像大小 $224\\times 224$）训练卷积层，但预测时分辨率加倍。 Fast YOLO 版本使用了更少的卷积，其他所有训练参数及测试参数都和 base YOLO 版本是一样的。 网络的最终输出是 $7\\times 7\\times 30$ 的张量。这个张量所代表的具体含义如下图所示。对于每一个单元格，前 20 个元素是类别概率值，然后 2 个元素是边界框置信度，两者相乘可以得到类别置信度，最后 8 个元素是边界框的 $(x,y,w,h)$ 。之所以把置信度 $c$ 和 $(x,y,w,h)$ 都分开排列，而不是按照$(x,y,w,h,c)$ 这样排列，存粹是为了后续计算时方便。 划分 $7 \\times 7$ 网格，共 98 个边界框，2 个框对应一个类别，所以 YOLOv1 只能在一个网格中检测出一个目标、单张图片最多预测 49 个目标。 2.2 Training 模型训练最重要的无非就是超参数的调整和损失函数的设计。 因为 YOLO 算法将检测问题看作是回归问题，所以自然地采用了比较容易优化的均方误差作为损失函数，但是面临定位误差和分类误差权重一样的问题；同时，在每张图像中，许多网格单元并不包含对象，即负样本（不包含物体的网格）远多于正样本（包含物体的网格），这通常会压倒了正样本的梯度，导致训练早期模型发散。 为了改善这点，引入了两个参数：$\\lambda{coord}=5$ 和 $\\lambda{noobj} =0.5$。对于边界框坐标预测损失（定位误差），采用较大的权重 $\\lambda{coord} =5$，然后区分不包含目标的边界框和含有目标的边界框，前者采用较小权重 $\\lambda{noobj} =0.5$。其他权重则均设为 0。 对于大小不同的边界框，因为较小边界框的坐标误差比较大边界框要更敏感，所以为了部分解决这个问题，将网络的边界框的宽高预测改为对其平方根的预测，即预测值变为 $(x, y, \\sqrt w, \\sqrt h)$。 YOLOv1 每个网格单元预测多个边界框。在训练时，每个目标我们只需要一个边界框预测器来负责。我们指定一个预测器“负责”根据哪个预测与真实值之间具有当前最高的 IOU 来预测目标。这导致边界框预测器之间的专业化。每个预测器可以更好地预测特定大小，方向角，或目标的类别，从而改善整体召回率。 YOLO 由于每个网格仅能预测 2 个边界框且仅可以包含一个类别，因此是对于一个单元格存在多个目标的问题，YOLO 只能选择一个来预测。这使得它在预测临近物体的数量上存在不足，如钢筋、人脸和鸟群检测等。 最终网络总的损失函数计算公式如下： $I{ij}^{obj}$ 指的是第 $i$ 个单元格存在目标，且该单元格中的第 $j$ 个边界框负责预测该目标。 $I{i}^{obj}$ 指的是第 $i$ 个单元格存在目标。 前 2 行计算前景的 geo_loss（定位 loss）。 第 3 行计算前景的 confidence_loss（包含目标的边界框的置信度误差项）。 第 4 行计算背景的 confidence_loss。 第 5 行计算分类损失 class_loss。 值得注意的是，对于不存在对应目标的边界框，其误差项就是只有置信度，坐标项误差是没法计算的。而只有当一个单元格内确实存在目标时，才计算分类误差项，否则该项也是无法计算的。 2.4. Inferences 同样采用了 NMS 算法来抑制多重检测，对应的模型推理结果解码代码如下，这里要和前面的 encoder 函数结合起来看。 # 对于网络输出预测 改为再图片上画出框及score def decoder(pred): \"\"\" pred (tensor) torch.Size([1, 7, 7, 30]) return (tensor) box[[x1,y1,x2,y2]] label[...] \"\"\" grid_num = 7 boxes = [] cls_indexs = [] probs = [] cell_size = 1./grid_num pred = pred.data # torch.Size([1, 14, 14, 30]) pred = pred.squeeze(0) # torch.Size([14, 14, 30]) # 0 1 2 3 4 5 6 7 8 9 # [中心坐标,长宽,置信度,中心坐标,长宽,置信度, 20个类别] x 7x7 contain1 = pred[:, :, 4].unsqueeze(2) # torch.Size([14, 14, 1]) contain2 = pred[:, :, 9].unsqueeze(2) # torch.Size([14, 14, 1]) contain = torch.cat((contain1, contain2), 2) # torch.Size([14, 14, 2]) mask1 = contain > 0.1 # 大于阈值, torch.Size([14, 14, 2]) content: tensor([False, False]) mask2 = (contain == contain.max()) # we always select the best contain_prob what ever it>0.9 mask = (mask1+mask2).gt(0) # min_score,min_index = torch.min(contain, 2) # 每个 cell 只选最大概率的那个预测框 for i in range(grid_num): for j in range(grid_num): for b in range(2): # index = min_index[i,j] # mask[i,j,index] = 0 if mask[i, j, b] == 1: box = pred[i, j, b*5:b*5+4] contain_prob = torch.FloatTensor([pred[i, j, b*5+4]]) xy = torch.FloatTensor([j, i])*cell_size # cell左上角 up left of cell box[:2] = box[:2]*cell_size + xy # return cxcy relative to image box_xy = torch.FloatTensor(box.size()) # 转换成xy形式 convert[cx,cy,w,h] to [x1,y1,x2,y2] box_xy[:2] = box[:2] - 0.5*box[2:] box_xy[2:] = box[:2] + 0.5*box[2:] max_prob, cls_index = torch.max(pred[i, j, 10:], 0) if float((contain_prob*max_prob)[0]) > 0.1: boxes.append(box_xy.view(1, 4)) cls_indexs.append(cls_index.item()) probs.append(contain_prob*max_prob) if len(boxes) == 0: boxes = torch.zeros((1, 4)) probs = torch.zeros(1) cls_indexs = torch.zeros(1) else: boxes = torch.cat(boxes, 0) # (n,4) # print(type(probs)) # print(len(probs)) # print(probs) probs = torch.cat(probs, 0) # (n,) # print(probs) # print(type(cls_indexs)) # print(len(cls_indexs)) # print(cls_indexs) cls_indexs = torch.IntTensor(cls_indexs) # (n,) # 去除冗余的候选框，得到最佳检测框（bbox） keep = nms(boxes, probs) # print(\"keep:\", keep) a = boxes[keep] b = cls_indexs[keep] c = probs[keep] return a, b, c 4.1 Comparison to Other Real-Time Systems 基于 GPU Titan X 硬件环境下，与他检测算法的性能比较如下。 5，代码实现思考 一些思考：快速的阅读了网上的一些 YOLOv1 代码实现，发现整个 YOLOv1 检测系统的代码可以分为以下几个部分： 模型结构定义：特征提器模块 + 检测头模块（两个全连接层）。 数据预处理，最难写的代码，需要对原有的 VOC 数据做预处理，编码成 YOLOv1 要求的格式输入，训练集的 label 的 shape 为 (bach_size, 7, 7, 30)。 模型训练，主要由损失函数的构建组成，损失函数包括 5 个部分。 模型预测，主要在于模型输出的解析，即解码成可方便显示的形式。 二，YOLOv2 YOLO9000 是 CVPR2017 的最佳论文提名，但是这篇论文其实提出了 YOLOv2 和 YOLO9000 两个模型，二者略有不同。前者主要是 YOLO 的升级版，后者的主要检测网络也是 YOLOv2，同时对数据集做了融合，使得模型可以检测 9000 多类物体。 摘要 YOLOv2 其实就是 YOLO9000，作者在 YOLOv1 基础上改进的一种新的 state-of-the-art 目标检测模型，它能检测多达 9000 个目标！利用了多尺度（multi-scale）训练方法，YOLOv2 可以在不同尺寸的图片上运行，并取得速度和精度的平衡。 在速度达到在 40 FPS 同时，YOLOv2 获得 78.6 mAP 的精度，性能优于backbone 为 ResNet 的 Faster RCNN 和 SSD 等当前最优（state-of-the-art） 模型。最后作者提出一种联合训练目标检测和分类的方法，基于这种方法，YOLO9000 能实时检测多达 9000 种目标。 YOLOv1 虽然速度很快，但是还有很多缺点： 虽然每个 grid 预测两个框，但是只能对应一个目标，对于同一个 grid 有着两个目标的情况下，YOLOv1 是检测不全的，且模型最多检测 $7 \\times 7 = 49$ 个目标，即表现为模型查全率低。 预测框不够准确，之前回归 $(x,y,w,h)$ 的方法不够精确，即表现为模型精确率低。 回归参数网络使用全连接层参数量太大，即模型检测头还不够块。 YOLOv2 的改进 1，中心坐标位置预测的改进 YOLOv1 模型预测的边界框中心坐标 $(x,y)$ 是基于 grid 的偏移，这里 grid 的位置是固定划分出来的，偏移量 = 目标位置 - grid 的位置。 边界框的编码过程：YOLOv2 参考了两阶段网络的 anchor boxes 来预测边界框相对先验框的偏移，同时沿用 YOLOv1 的方法预测边界框中心点相对于 grid 左上角位置的相对偏移值。$(x,y,w,h)$ 的偏移值和实际坐标值的关系如下图所示。 各个字母的含义如下： $b_x,b_y,b_w,b_h$ ：模型预测结果转化为 box 中心坐标和宽高后的值 $t_x,t_y,t_w,t_h$ ：模型要预测的偏移量。 $c_x,c_y$ ：grid 的左上角坐标，如上图所示。 $p_w,p_h$ ：anchor 的宽和高，这里的 anchor 是人为定好的一个框，宽和高是固定的。 通过以上定义我们从直接预测位置改为预测一个偏移量，即基于 anchor 框的宽高和 grid 的先验位置的偏移量，位置上使用 grid，宽高上使用 anchor 框，得到最终目标的位置，这种方法叫作 location prediction。 预测偏移不直接预测位置，是因为作者发现直接预测位置会导致神经网络在一开始训练时不稳定，使用偏移量会使得训练过程更加稳定，性能指标提升了 5% 左右。 在数据集的预处理过程中，关键的边界框编码函数如下（代码来自 github，这个版本更清晰易懂）： def encode(self, boxes, labels, input_size): '''Encode target bounding boxes and class labels into YOLOv2 format. Args: boxes: (tensor) bounding boxes of (xmin,ymin,xmax,ymax) in range [0,1], sized [#obj, 4]. labels: (tensor) object class labels, sized [#obj,]. input_size: (int) model input size. Returns: loc_targets: (tensor) encoded bounding boxes, sized [5,4,fmsize,fmsize]. cls_targets: (tensor) encoded class labels, sized [5,20,fmsize,fmsize]. box_targets: (tensor) truth boxes, sized [#obj,4]. ''' num_boxes = len(boxes) # input_size -> fmsize # 320->10, 352->11, 384->12, 416->13, ..., 608->19 fmsize = (input_size - 320) / 32 + 10 grid_size = input_size / fmsize boxes *= input_size # scale [0,1] -> [0,input_size] bx = (boxes[:,0] + boxes[:,2]) * 0.5 / grid_size # in [0,fmsize] by = (boxes[:,1] + boxes[:,3]) * 0.5 / grid_size # in [0,fmsize] bw = (boxes[:,2] - boxes[:,0]) / grid_size # in [0,fmsize] bh = (boxes[:,3] - boxes[:,1]) / grid_size # in [0,fmsize] tx = bx - bx.floor() ty = by - by.floor() xy = meshgrid(fmsize, swap_dims=True) + 0.5 # grid center, [fmsize*fmsize,2] wh = torch.Tensor(self.anchors) # [5,2] xy = xy.view(fmsize,fmsize,1,2).expand(fmsize,fmsize,5,2) wh = wh.view(1,1,5,2).expand(fmsize,fmsize,5,2) anchor_boxes = torch.cat([xy-wh/2, xy+wh/2], 3) # [fmsize,fmsize,5,4] ious = box_iou(anchor_boxes.view(-1,4), boxes/grid_size) # [fmsize*fmsize*5,N] ious = ious.view(fmsize,fmsize,5,num_boxes) # [fmsize,fmsize,5,N] loc_targets = torch.zeros(5,4,fmsize,fmsize) # 5boxes * 4coords cls_targets = torch.zeros(5,20,fmsize,fmsize) for i in range(num_boxes): cx = int(bx[i]) cy = int(by[i]) _, max_idx = ious[cy,cx,:,i].max(0) j = max_idx[0] cls_targets[j,labels[i],cy,cx] = 1 tw = bw[i] / self.anchors[j][0] th = bh[i] / self.anchors[j][1] loc_targets[j,:,cy,cx] = torch.Tensor([tx[i], ty[i], tw, th]) return loc_targets, cls_targets, boxes/grid_size 边界框的解码过程：虽然模型预测的是边界框的偏移量 $(t_x,t_y,t_w,t_h)$，但是可通过以下公式计算出边界框的实际位置。 $$ bx = \\sigma(t_x) + c_x \\\\ b_y = \\sigma(t_y) + c_y \\\\ b_w = p{w}e^{tw} \\\\ b_h = p{h}e^{t_h} $$ 其中，$(c_x, c_y)$ 为 grid 的左上角坐标，因为 $\\sigma$ 表示的是 sigmoid 函数，所以边界框的中心坐标会被约束在 grid 内部，防止偏移过多。$p_w$、$p_h$ 是先验框（anchors）的宽度与高度，其值相对于特征图大小 $W\\times H$ = $13\\times 13$ 而言的，因为划分为 $13 \\times 13$ 个 grid，所以最后输出的特征图中每个 grid 的长和宽均是 1。知道了特征图的大小，就可以将边界框相对于整个特征图的位置和大小计算出来（均取值 ${0,1}$）。 $$ bx = (\\sigma(t_x) + c_x)/W \\\\ b_y = (\\sigma(t_y) + c_y)/H \\\\ b_w = p{w}e^{tw}/W \\\\ b_h = p{h}e^{t_h}/H $$ 在模型推理的时候，将以上 4 个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的实际中心坐标和大小。 在模型推理过程中，模型输出张量的解析，即边界框的解码函数如下： def decode(self, outputs, input_size): '''Transform predicted loc/conf back to real bbox locations and class labels. Args: outputs: (tensor) model outputs, sized [1,125,13,13]. input_size: (int) model input size. Returns: boxes: (tensor) bbox locations, sized [#obj, 4]. labels: (tensor) class labels, sized [#obj,1]. ''' fmsize = outputs.size(2) outputs = outputs.view(5,25,13,13) loc_xy = outputs[:,:2,:,:] # [5,2,13,13] grid_xy = meshgrid(fmsize, swap_dims=True).view(fmsize,fmsize,2).permute(2,0,1) # [2,13,13] box_xy = loc_xy.sigmoid() + grid_xy.expand_as(loc_xy) # [5,2,13,13] loc_wh = outputs[:,2:4,:,:] # [5,2,13,13] anchor_wh = torch.Tensor(self.anchors).view(5,2,1,1).expand_as(loc_wh) # [5,2,13,13] box_wh = anchor_wh * loc_wh.exp() # [5,2,13,13] boxes = torch.cat([box_xy-box_wh/2, box_xy+box_wh/2], 1) # [5,4,13,13] boxes = boxes.permute(0,2,3,1).contiguous().view(-1,4) # [845,4] iou_preds = outputs[:,4,:,:].sigmoid() # [5,13,13] cls_preds = outputs[:,5:,:,:] # [5,20,13,13] cls_preds = cls_preds.permute(0,2,3,1).contiguous().view(-1,20) cls_preds = softmax(cls_preds) # [5*13*13,20] score = cls_preds * iou_preds.view(-1).unsqueeze(1).expand_as(cls_preds) # [5*13*13,20] score = score.max(1)[0].view(-1) # [5*13*13,] print(iou_preds.max()) print(cls_preds.max()) print(score.max()) ids = (score>0.5).nonzero().squeeze() keep = box_nms(boxes[ids], score[ids]) # NMS 算法去除重复框 return boxes[ids][keep] / fmsize 2，1 个 gird 只能对应一个目标的改进 或者说很多目标预测不到，查全率低的改进 YOLOv2 首先把 $7 \\times 7$ 个区域改为 $13 \\times 13$ 个 grid（区域），每个区域有 5 个anchor，且每个 anchor 对应着 1 个类别，那么，输出的尺寸就应该为：[N,13,13,125] $125 = 5 \\times (5 + 20)$ 值得注意的是之前 YOLOv1 的每个 grid 只能预测一个目标的分类概率值，两个 boxes 共享这个置信度概率。现在 YOLOv2 使用了 anchor 先验框后，每个 grid 的每个 anchor 都单独预测一个目标的分类概率值。 之所以每个 grid 取 5 个 anchor，是因为作者对 VOC/COCO 数据集进行 K-means 聚类实验，发现当 k=5 时，模型 recall vs. complexity 取得了较好的平衡。当然，$k$ 越好，mAP 肯定越高，但是为了平衡模型复杂度，作者选择了 5 个聚类簇，即划分成 5 类先验框。设置先验框的主要目的是为了使得预测框与 ground truth 的 IOU 更好，所以聚类分析时选用 box 与聚类中心 box 之间的 IOU 值作为距离指标： $$d(box, centroid) = 1-IOU(box, centroid)$$ 与 Faster RCNN 手动设置 anchor 的大小和宽高比不同，YOLOv2 的 anchor 是从数据集中统计得到的。 3，backbone 的改进 作者提出了一个全新的 backbone 网络：Darknet-19，它是基于前人经典工作和该领域常识的基础上进行设计的。Darknet-19 网络和 VGG 网络类似，主要使用 $3 \\times 3$ 卷积，并且每个 $2 \\times 2$ pooling 操作之后将特征图通道数加倍。借鉴 NIN 网络的工作，作者使用 global average pooling 进行预测，并在 $3 \\times 3$ 卷积之间使用 $1 \\times 1$ 卷积来降低特征图通道数从而降低模型计算量和参数量。Darknet-19 网络的每个卷积层后面都是用了 BN 层来加快模型收敛，防止模型过拟合。 Darknet-19 网络总共有 19 个卷积层（convolution）、5 最大池化层（maxpooling）。Darknet-19 以 5.58 T的计算量在 ImageNet 数据集上取得了 72.9% 的 top-1 精度和 91.2% 的 top-5 精度。Darket19 网络参数表如下图所示。 检测训练。在 Darknet19 网络基础上进行修改后用于目标检测。首先，移除网络的最后一个卷积层，然后添加滤波器个数为 1024 的 $3 \\times 3$ 卷积层，最后添加一个 $1 \\times 1$ 卷积层，其滤波器个数为模型检测需要输出的变量个数。对于 VOC 数据集，每个 grid 预测 5 个边界框，每个边界框有 5 个坐标（$t_x, t_y, t_w, t_h \\ 和\\ t_o$）和 20 个类别，所以共有 125 个滤波器。我们还添加了从最后的 3×3×512 层到倒数第二层卷积层的直通层，以便模型可以使用细粒度特征。 $$P_r(object)*IOU(b; object) = \\sigma (t_o)$$ Yolov2 整个模型结构代码如下： 代码来源 这里。 '''Darknet in PyTorch.''' import torch import torch.nn as nn import torch.nn.init as init import torch.nn.functional as F from torch.autograd import Variable class Darknet(nn.Module): # (64,1) means conv kernel size is 1, by default is 3. cfg1 = [32, 'M', 64, 'M', 128, (64,1), 128, 'M', 256, (128,1), 256, 'M', 512, (256,1), 512, (256,1), 512] # conv1 - conv13 cfg2 = ['M', 1024, (512,1), 1024, (512,1), 1024] # conv14 - conv18 def __init__(self): super(Darknet, self).__init__() self.layer1 = self._make_layers(self.cfg1, in_planes=3) self.layer2 = self._make_layers(self.cfg2, in_planes=512) #### Add new layers self.conv19 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1) self.bn19 = nn.BatchNorm2d(1024) self.conv20 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1) self.bn20 = nn.BatchNorm2d(1024) # Currently I removed the passthrough layer for simplicity self.conv21 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1) self.bn21 = nn.BatchNorm2d(1024) # Outputs: 5boxes * (4coordinates + 1confidence + 20classes) self.conv22 = nn.Conv2d(1024, 5*(5+20), kernel_size=1, stride=1, padding=0) def _make_layers(self, cfg, in_planes): layers = [] for x in cfg: if x == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)] else: out_planes = x[0] if isinstance(x, tuple) else x ksize = x[1] if isinstance(x, tuple) else 3 layers += [nn.Conv2d(in_planes, out_planes, kernel_size=ksize, padding=(ksize-1)//2), nn.BatchNorm2d(out_planes), nn.LeakyReLU(0.1, True)] in_planes = out_planes return nn.Sequential(*layers) def forward(self, x): out = self.layer1(x) out = self.layer2(out) out = F.leaky_relu(self.bn19(self.conv19(out)), 0.1) out = F.leaky_relu(self.bn20(self.conv20(out)), 0.1) out = F.leaky_relu(self.bn21(self.conv21(out)), 0.1) out = self.conv22(out) return out def test(): net = Darknet() y = net(Variable(torch.randn(1,3,416,416))) print(y.size()) # 模型最后输出张量大小 [1,125,13,13] if __name__ == \"__main__\": test() 4，多尺度训练 YOLOv1 输入图像分辨率为 $449 \\times 448$，因为使用了 anchor boxes，所以 YOLOv2 将输入分辨率改为 $416 \\times 416$。又因为 YOLOv2 模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于 $416 \\times 416$ 大小的图片。为了增强模型的鲁棒性，YOLOv2 采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的 iterations 之后改变模型的输入图片大小。由于 YOLOv2 的下采样总步长为 32，所以输入图片大小选择一系列为 32 倍数的值： $\\lbrace 320, 352,...,608 \\rbrace$ ，因此输入图片分辨率最小为 $320\\times 320$，此时对应的特征图大小为 $10\\times 10$（不是奇数），而输入图片最大为 $608\\times 608$ ，对应的特征图大小为 $19\\times 19$ 。在训练过程，每隔 10 个 iterations 随机选择一种输入图片大小，然后需要修最后的检测头以适应维度变化后，就可以重新训练。 采用 Multi-Scale Training 策略，YOLOv2 可以适应不同输入大小的图片，并且预测出很好的结果。在测试时，YOLOv2 可以采用不同大小的图片作为输入，在 VOC 2007 数据集上的测试结果如下图所示。 损失函数 YOLOv2 的损失函数的计算公式归纳如下 第 2,3 行：$t$ 是迭代次数，即前 12800 步我们计算这个损失，后面不计算了。即前 12800 步我们会优化预测的 $(x,y,w,h)$ 与 anchor 的 $(x,y,w,h)$ 的距离 + 预测的 $(x,y,w,h)$ 与 GT 的 $(x,y,w,h)$ 的距离，12800 步之后就只优化预测的 $(x,y,w,h)$与 GT 的 $(x,y,w,h)$ 的距离，原因是这时的预测结果已经较为准确了，anchor已经满足检测系统的需要，而在一开始预测不准的时候，用上 anchor 可以加速训练。 YOLOv2 的损失函数实现代码如下，损失函数计算过程中的模型预测结果的解码函数和前面的解码函数略有不同，其包含关键部分目标 bbox 的解析。 from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F from torch.autograd import Variable from utils import box_iou, meshgrid class YOLOLoss(nn.Module): def __init__(self): super(YOLOLoss, self).__init__() def decode_loc(self, loc_preds): '''Recover predicted locations back to box coordinates. Args: loc_preds: (tensor) predicted locations, sized [N,5,4,fmsize,fmsize]. Returns: box_preds: (tensor) recovered boxes, sized [N,5,4,fmsize,fmsize]. ''' anchors = [(1.3221,1.73145),(3.19275,4.00944),(5.05587,8.09892),(9.47112,4.84053),(11.2364,10.0071)] N, _, _, fmsize, _ = loc_preds.size() loc_xy = loc_preds[:,:,:2,:,:] # [N,5,2,13,13] grid_xy = meshgrid(fmsize, swap_dims=True).view(fmsize,fmsize,2).permute(2,0,1) # [2,13,13] grid_xy = Variable(grid_xy.cuda()) box_xy = loc_xy.sigmoid() + grid_xy.expand_as(loc_xy) # [N,5,2,13,13] loc_wh = loc_preds[:,:,2:4,:,:] # [N,5,2,13,13] anchor_wh = torch.Tensor(anchors).view(1,5,2,1,1).expand_as(loc_wh) # [N,5,2,13,13] anchor_wh = Variable(anchor_wh.cuda()) box_wh = anchor_wh * loc_wh.exp() # [N,5,2,13,13] box_preds = torch.cat([box_xy-box_wh/2, box_xy+box_wh/2], 2) # [N,5,4,13,13] return box_preds def forward(self, preds, loc_targets, cls_targets, box_targets): ''' Args: preds: (tensor) model outputs, sized [batch_size,150,fmsize,fmsize]. loc_targets: (tensor) loc targets, sized [batch_size,5,4,fmsize,fmsize]. cls_targets: (tensor) conf targets, sized [batch_size,5,20,fmsize,fmsize]. box_targets: (list) box targets, each sized [#obj,4]. Returns: (tensor) loss = SmoothL1Loss(loc) + SmoothL1Loss(iou) + SmoothL1Loss(cls) ''' batch_size, _, fmsize, _ = preds.size() preds = preds.view(batch_size, 5, 4+1+20, fmsize, fmsize) ### loc_loss xy = preds[:,:,:2,:,:].sigmoid() # x->sigmoid(x), y->sigmoid(y) wh = preds[:,:,2:4,:,:].exp() loc_preds = torch.cat([xy,wh], 2) # [N,5,4,13,13] pos = cls_targets.max(2)[0].squeeze() > 0 # [N,5,13,13] num_pos = pos.data.long().sum() mask = pos.unsqueeze(2).expand_as(loc_preds) # [N,5,13,13] -> [N,5,1,13,13] -> [N,5,4,13,13] loc_loss = F.smooth_l1_loss(loc_preds[mask], loc_targets[mask], size_average=False) ### iou_loss iou_preds = preds[:,:,4,:,:].sigmoid() # [N,5,13,13] iou_targets = Variable(torch.zeros(iou_preds.size()).cuda()) # [N,5,13,13] box_preds = self.decode_loc(preds[:,:,:4,:,:]) # [N,5,4,13,13] box_preds = box_preds.permute(0,1,3,4,2).contiguous().view(batch_size,-1,4) # [N,5*13*13,4] for i in range(batch_size): box_pred = box_preds[i] # [5*13*13,4] box_target = box_targets[i] # [#obj, 4] iou_target = box_iou(box_pred, box_target) # [5*13*13, #obj] iou_targets[i] = iou_target.max(1)[0].view(5,fmsize,fmsize) # [5,13,13] mask = Variable(torch.ones(iou_preds.size()).cuda()) * 0.1 # [N,5,13,13] mask[pos] = 1 iou_loss = F.smooth_l1_loss(iou_preds*mask, iou_targets*mask, size_average=False) ### cls_loss cls_preds = preds[:,:,5:,:,:] # [N,5,20,13,13] cls_preds = cls_preds.permute(0,1,3,4,2).contiguous().view(-1,20) # [N,5,20,13,13] -> [N,5,13,13,20] -> [N*5*13*13,20] cls_preds = F.softmax(cls_preds) # [N*5*13*13,20] cls_preds = cls_preds.view(batch_size,5,fmsize,fmsize,20).permute(0,1,4,2,3) # [N*5*13*13,20] -> [N,5,20,13,13] pos = cls_targets > 0 cls_loss = F.smooth_l1_loss(cls_preds[pos], cls_targets[pos], size_average=False) print('%f %f %f' % (loc_loss.data[0]/num_pos, iou_loss.data[0]/num_pos, cls_loss.data[0]/num_pos), end=' ') return (loc_loss + iou_loss + cls_loss) / num_pos YOLOv2 在 VOC2007 数据集上和其他 state-of-the-art 模型的测试结果的比较如下曲线所示。 三，YOLOv3 YOLOv3 的论文写得不是很好，需要完全看懂，还是要看代码，C/C++ 基础不好的建议看 Pytorch 版本的复现。下文是我对原论文的精简翻译和一些难点的个人理解，以及一些关键代码解析。 摘要 我们对 YOLO 再次进行了更新，包括一些小的设计和更好的网络结构。在输入图像分辨率为 $320 \\times 320$ 上运行 YOLOv3 模型，时间是 22 ms 的同时获得了 28.2 的 mAP，精度和 SSD 类似，但是速度更快。和其他阈值相比，YOLOv3 尤其在 0.5 IOU（也就是 $AP_{50}$）这个指标上表现非常良好。在 Titan X 环境下，YOLOv3 的检测精度为 57.9 AP50，耗时 51 ms；而 RetinaNet 的精度只有 57.5 AP50，但却需要 198 ms，相当于 YOLOv3的 3.8 倍。 一般可以认为检测模型 = 特征提取器 + 检测头。 1，介绍 这篇论文其实也是一个技术报告，首先我会告诉你们 YOLOv3 的更新（改进）情况，然后介绍一些我们失败的尝试，最后是这次更新方法意义的总结。 2，改进 YOLOv3 大部分有意的改进点都来源于前人的工作，当然我们也训练了一个比其他人更好的分类器网络。 2.1，边界框预测 这部分内容和 YOLOv2 几乎一致，但是内容更细致，且阈值的取值有些不一样。 和 YOLOv2 一样，我们依然使用维度聚类的方法来挑选 anchor boxes 作为边界框预测的先验框。每个边界框都会预测 $4$ 个偏移坐标 $(t_x,t_y,t_w,t_h)$。假设 $(c_x, c_y)$ 为 grid 的左上角坐标，$p_w$、$p_h$ 是先验框（anchors）的宽度与高度，那么网络预测值和边界框真实位置的关系如下所示： 假设某一层的 feature map 的大小为 $13 \\times 13$， 那么 grid cell 就有 $13 \\times 13$ 个，则第 $n$ 行第 $n$ 列的 grid cell 的坐标 $(x_x, c_y)$ 就是 $(n-1,n)$。 $$ bx = \\sigma(t_x) + c_x \\\\ b_y = \\sigma(t_y) + c_y \\\\ b_w = p{w}e^{tw} \\\\ b_h = p{h}e^{t_h} $$ $bx,b_y,b_w,b_h$ 是边界框的实际中心坐标和宽高值。在训练过程中，我们使用平方误差损失函数。利用上面的公式，可以轻松推出这样的结论：如果预测坐标的真实值（ground truth）是 $\\hat{t}{\\ast}$，那么梯度就是真实值减去预测值 $\\hat{t}{\\ast} - t{\\ast }$。 梯度变成 $\\hat{t}{\\ast} - t{\\ast }$ 有什么好处呢？ 注意，计算损失的时候，模型预测输出的 $t_x,t_y$ 外面要套一个 sigmoid 函数 ，否则坐标就不是 $(0,1)$ 范围内的，一旦套了 sigmoid，就只能用 BCE 损失函数去反向传播，这样第一步算出来的才是 $t_x-\\hat{t}_x$；$(t_w,t_h)$ 的预测没有使用 sigmoid 函数，所以损失使用 $MSE$。 $\\hat{t}_x$ 是预测坐标偏移的真实值（ground truth）。 YOLOv3 使用逻辑回归来预测每个边界框的 objectness score（置信度分数）。如果当前先验框和 ground truth 的 IOU 超过了前面的先验框，那么它的分数就是 1。和 Faster RCNN 论文一样，如果先验框和 ground truth 的 IOU不是最好的，那么即使它超过了阈值，我们还是会忽略掉这个 box，正负样本判断的阈值取 0.5。YOLOv3 检测系统只为每个 ground truth 对象分配一个边界框。如果先验框（bonding box prior，其实就是聚类得到的 anchors）未分配给 ground truth 对象，则不会造成位置和分类预测损失，只有置信度损失（only objectness）。 将 coco 数据集的标签编码成 $(t_x,t_y,t_w,t_h)$ 形式的代码如下： def get_target(self, target, anchors, in_w, in_h, ignore_threshold): \"\"\" Maybe have problem. target: original coco dataset label. in_w, in_h: feature map size. \"\"\" bs = target.size(0) mask = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False) noobj_mask = torch.ones(bs, self.num_anchors, in_h, in_w, requires_grad=False) tx = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False) ty = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False) tw = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False) th = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False) tconf = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False) tcls = torch.zeros(bs, self.num_anchors, in_h, in_w, self.num_classes, requires_grad=False) for b in range(bs): for t in range(target.shape[1]): if target[b, t].sum() == 0: continue # Convert to position relative to box gx = target[b, t, 1] * in_w gy = target[b, t, 2] * in_h gw = target[b, t, 3] * in_w gh = target[b, t, 4] * in_h # Get grid box indices gi = int(gx) gj = int(gy) # Get shape of gt box gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0) # Get shape of anchor box anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((self.num_anchors, 2)), np.array(anchors)), 1)) # Calculate iou between gt and anchor shapes anch_ious = bbox_iou(gt_box, anchor_shapes) # Where the overlap is larger than threshold set mask to zero (ignore) noobj_mask[b, anch_ious > ignore_threshold, gj, gi] = 0 # Find the best matching anchor box best_n = np.argmax(anch_ious) # Masks mask[b, best_n, gj, gi] = 1 # Coordinates tx[b, best_n, gj, gi] = gx - gi ty[b, best_n, gj, gi] = gy - gj # Width and height tw[b, best_n, gj, gi] = math.log(gw/anchors[best_n][0] + 1e-16) th[b, best_n, gj, gi] = math.log(gh/anchors[best_n][1] + 1e-16) # object tconf[b, best_n, gj, gi] = 1 # One-hot encoding of label tcls[b, best_n, gj, gi, int(target[b, t, 0])] = 1 return mask, noobj_mask, tx, ty, tw, th, tconf, tcls 另一个复习版本关于数据集标签的处理代码如下： def build_targets(p, targets, model): # Build targets for compute_loss(), input targets(image,class,x,y,w,h) na, nt = 3, targets.shape[0] # number of anchors, targets #TODO tcls, tbox, indices, anch = [], [], [], [] gain = torch.ones(7, device=targets.device) # normalized to gridspace gain # Make a tensor that iterates 0-2 for 3 anchors and repeat that as many times as we have target boxes ai = torch.arange(na, device=targets.device).float().view(na, 1).repeat(1, nt) # Copy target boxes anchor size times and append an anchor index to each copy the anchor index is also expressed by the new first dimension targets = torch.cat((targets.repeat(na, 1, 1), ai[:, :, None]), 2) for i, yolo_layer in enumerate(model.yolo_layers): # Scale anchors by the yolo grid cell size so that an anchor with the size of the cell would result in 1 anchors = yolo_layer.anchors / yolo_layer.stride # Add the number of yolo cells in this layer the gain tensor # The gain tensor matches the collums of our targets (img id, class, x, y, w, h, anchor id) gain[2:6] = torch.tensor(p[i].shape)[[3, 2, 3, 2]] # xyxy gain # Scale targets by the number of yolo layer cells, they are now in the yolo cell coordinate system t = targets * gain # Check if we have targets if nt: # Calculate ration between anchor and target box for both width and height r = t[:, :, 4:6] / anchors[:, None] # Select the ratios that have the highest divergence in any axis and check if the ratio is less than 4 j = torch.max(r, 1. / r).max(2)[0] 关于更多模型推理部分代码的复现和理解，可阅读这个 github项目代码。 2.2，分类预测 每个框使用多标签分类来预测边界框可能包含的类。我们不使用 softmax 激活函数，因为我们发现它对模型的性能影响不好。相反，我们只是使用独立的逻辑分类器。在训练过程中，我们使用二元交叉熵损失来进行类别预测。 在这个数据集 Open Images Dataset 中有着大量的重叠标签。如果使用 softmax ，意味着强加了一个假设，即每个框只包含一个类别，但通常情况并非如此。多标签方法能更好地模拟数据。 2.3，跨尺度预测 YOLOv3 可以预测 3 种不同尺度（scale）的框。 总的来说是，引入了类似 FPN 的多尺度特征图融合，从而加强小目标检测。与原始的 FPN 不同，YOLOv3 的 Neck 网络只输出 3 个分支，分别对应 3 种尺度，高层网络输出的特征图经过上采样后和低层网络输出的特征图融合是使用 concat 方式拼接，而不是使用 element-wise add 的方法。 首先检测系统利用和特征金字塔网络[8]（FPN 网络）类似的概念，来提取不同尺度的特征。我们在基础的特征提取器基础上添加了一些卷积层。这些卷积层的最后会预测一个 3 维张量，其是用来编码边界框，框中目标和分类预测。在 COCO 数据集的实验中，我们每个输出尺度都预测 3 个 boxes，所以模型最后输出的张量大小是 $N \\times N \\times [3*(4+1+80)]$，其中包含 4 个边界框offset、1 个 objectness 预测（前景背景预测）以及 80 种分类预测。 objectness 预测其实就是前景背景预测，有些类似 YOLOv2 的置信度 c 的概念。 然后我们将前面两层输出的特征图上采样 2 倍，并和浅层中的特征图，用 concatenation 方式把高低两种分辨率的特征图连接到一起，这样做能使我们同时获得上采样特征的有意义的语义信息和来自早期特征的细粒度信息。之后，再添加几个卷积层来处理这个融合后的特征，并输出大小是原来高层特征图两倍的张量。 按照这种设计方式，来预测最后一个尺度的 boxes。可以知道，对第三种尺度的预测也会从所有先前的计算中（多尺度特征融合的计算中）获益，同时能从低层的网络中获得细粒度（ finegrained ）的特征。 显而易见，低层网络输出的特征图语义信息比较少，但是目标位置准确；高层网络输出的特征图语义信息比较丰富，但是目标位置比较粗略。 依然使用 k-means 聚类来确定我们的先验边界框（box priors，即选择的 anchors），但是选择了 9 个聚类（clusters）和 3 种尺度（scales，大、中、小三种 anchor 尺度），然后在整个尺度上均匀分割聚类。在COCO 数据集上，9 个聚类是：（10×13）;（16×30）;（33×23）;（30×61）;（62×45）;（59×119）;（116×90）;（156×198）;（373×326）。 从上面的描述可知，YOLOv3 的检测头变成了 3 个分支，对于输入图像 shape 为 (3, 416, 416)的 YOLOv3 来说，Head 各分支的输出张量的尺寸如下： [13, 13, 3*(4+1+80)] [26, 2, 3*(4+1+80)] [52, 52, 3*(4+1+80)] 3 个分支分别对应 32 倍、16 倍、8倍下采样，也就是分别预测大、中、小目标。32 倍下采样的特征图的每个点感受野更大，所以用来预测大目标。 每个 sacle 分支的每个 grid 都会预测 3 个框，每个框预测 5 元组+ 80 个 one-hot vector类别，所以一共 size 是：3*(4+1+80)。 根据前面的内容，可以知道，YOLOv3 总共预测 $(13 \\times 13 + 26 \\times 26 + 52 \\times 52) \\times 3 = 10467(YOLOv3) \\gg 845 = 13 \\times 13 \\times 5(YOLOv2)$ 个边界框。 2.4，新的特征提取网络 我们使用一个新的网络来执行特征提取。它是 Darknet-19和新型残差网络方法的融合，由连续的 $3\\times 3$ 和 $1\\times 1$ 卷积层组合而成，并添加了一些 shortcut connection，整体体量更大。因为一共有 $53 = (1+2+8+8+4)\\times 2+4+2+1 $ 个卷积层，所以我们称为 Darknet-53。 总的来说，DarkNet-53 不仅使用了全卷积网络，将 YOLOv2 中降采样作用 pooling 层都换成了 convolution(3x3，stride=2) 层；而且引入了残差（residual）结构，不再使用类似 VGG 那样的直连型网络结构，因此可以训练更深的网络，即卷积层数达到了 53 层。（更深的网络，特征提取效果会更好） Darknet53 网络的 Pytorch 代码如下所示。 代码来源这里。 import torch import torch.nn as nn import math from collections import OrderedDict __all__ = ['darknet21', 'darknet53'] class BasicBlock(nn.Module): \"\"\"basic residual block for Darknet53，卷积层分别是 1x1 和 3x3 \"\"\" def __init__(self, inplanes, planes): super(BasicBlock, self).__init__() self.conv1 = nn.Conv2d(inplanes, planes[0], kernel_size=1, stride=1, padding=0, bias=False) self.bn1 = nn.BatchNorm2d(planes[0]) self.relu1 = nn.LeakyReLU(0.1) self.conv2 = nn.Conv2d(planes[0], planes[1], kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(planes[1]) self.relu2 = nn.LeakyReLU(0.1) def forward(self, x):s residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu1(out) out = self.conv2(out) out = self.bn2(out) out = self.relu2(out) out += residual return out class DarkNet(nn.Module): def __init__(self, layers): super(DarkNet, self).__init__() self.inplanes = 32 self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(self.inplanes) self.relu1 = nn.LeakyReLU(0.1) self.layer1 = self._make_layer([32, 64], layers[0]) self.layer2 = self._make_layer([64, 128], layers[1]) self.layer3 = self._make_layer([128, 256], layers[2]) self.layer4 = self._make_layer([256, 512], layers[3]) self.layer5 = self._make_layer([512, 1024], layers[4]) self.layers_out_filters = [64, 128, 256, 512, 1024] for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() def _make_layer(self, planes, blocks): layers = [] # 每个阶段的开始都要先 downsample，然后才是 basic residual block for Darknet53 layers.append((\"ds_conv\", nn.Conv2d(self.inplanes, planes[1], kernel_size=3, stride=2, padding=1, bias=False))) layers.append((\"ds_bn\", nn.BatchNorm2d(planes[1]))) layers.append((\"ds_relu\", nn.LeakyReLU(0.1))) # blocks self.inplanes = planes[1] for i in range(0, blocks): layers.append((\"residual_{}\".format(i), BasicBlock(self.inplanes, planes))) return nn.Sequential(OrderedDict(layers)) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu1(x) x = self.layer1(x) x = self.layer2(x) out3 = self.layer3(x) out4 = self.layer4(out3) out5 = self.layer5(out4) return out3, out4, out5 def darknet21(pretrained, **kwargs): \"\"\"Constructs a darknet-21 model. \"\"\" model = DarkNet([1, 1, 2, 2, 1]) if pretrained: if isinstance(pretrained, str): model.load_state_dict(torch.load(pretrained)) else: raise Exception(\"darknet request a pretrained path. got [{}]\".format(pretrained)) return model def darknet53(pretrained, **kwargs): \"\"\"Constructs a darknet-53 model. \"\"\" model = DarkNet([1, 2, 8, 8, 4]) if pretrained: if isinstance(pretrained, str): model.load_state_dict(torch.load(pretrained)) else: raise Exception(\"darknet request a pretrained path. got [{}]\".format(pretrained)) return model 3 个预测分支，对应预测 3 种尺度（大、种、小），也都采用了全卷积的结构。 YOLOv3 的 backbone 选择 Darknet-53后，其检测性能远超 Darknet-19，同时效率上也优于 ResNet-101 和 ResNet-152，对比实验结果如下： 在对比实验中，每个网络都使用相同的设置进行训练和测试。运行速度 FPS 是在 Titan X 硬件上，输入图像大小为 $256 \\times 256$ 上测试得到的。从上表可以看出，Darknet-53 和 state-of-the-art 分类器相比，有着更少的 FLOPs 和更快的速度。和 ResNet-101 相比，精度更高并且速度是前者的 1.5 倍；和 ResNet-152 相比，精度相似，但速度是它的 2 倍以上。 Darknet-53 也可以实现每秒最高的测量浮点运算。这意味着其网络结构可以更好地利用 GPU，从而使其评估效率更高，速度更快。这主要是因为 ResNets 的层数太多，效率不高。 2.5，训练 和 YOLOv2 一样，我们依然训练所有图片，没有 hard negative mining or any of that stuff。我们依然使用多尺度训练，大量的数据增强操作和 BN 层以及其他标准操作。我们使用之前的 Darknet 神经网络框架进行训练和测试[12]。 损失函数的计算公式如下。 YOLO v3 使用多标签分类，用多个独立的 logistic 分类器代替 softmax 函数，以计算输入属于特定标签的可能性。在计算分类损失进行训练时，YOLOv3 对每个标签使用二元交叉熵损失。 正负样本的确定： 正样本：与 GT 的 IOU 最大的框。 负样本：与 GT 的 IOU 的框。 忽略的样本：与 GT 的 IOU>0.5 但不是最大的框。 使用 $t_x$ 和 $t_y$ （而不是 $b_x$ 和 $b_y$ ）来计算损失。 注意：每个 GT 目标仅与一个先验边界框相关联。如果没有分配先验边界框，则不会导致分类和定位损失，只会有目标的置信度损失。 YOLOv3 网络结构图如下所示（这里输入图像大小为 608*608，来源 这里 ）。 2.5，推理 总的来说还是将输出的特侦图划分成 SS（这里的S和特征图大小一样） 的网格，*通过设置置信度阈值对网格进行筛选，只有大于指定阈值的网格才认为存在目标，即该网格会输出目标的置信度、bbox 坐标和类别信息，并通过 NMS 操作筛选掉重复的框。 值得注意的是，模型推理的 bbox 的 $xywh$ 值是对应 feature map 尺度的，所以后面还需要将 xywh 的值 * 特征图的下采样倍数。 # 将 bbox 预测值, box 置信度, box 分类结果的矩阵拼接成一个新的矩阵 # * _scale 是为了将预测的 box 对应到原图尺寸, _scale 是特征图下采样倍数。 # 对于大目标检测分支 pred_boxes.view(bs, -1, 4) 后的 shape 为 [1, 507, 4], output 的 shape 为 [1, 507, 85] # bs 是 batch_size，即一次推理多少张图片。 output = torch.cat((pred_boxes.view(bs, -1, 4) * _scale, conf.view(bs, -1, 1), pred_cls.view(bs, -1, self.num_classes)), -1) 3，实验结果 YOLOv3 实验结果非常好！详情见表3。 就 COCO 的 mAP 指标而言，YOLOv3 和 SSD 变体相近，但是速度却比后者快了 3 倍。尽管如此，YOLOv3 还是比 Retinanet 这样的模型在精度上要差一些。 但是当我们以 IOU = 0.5 这样的旧指标对比，YOLOv3 表现更好，几乎和 Retinanet 相近，远超 SSD 变体。这表面它其实是一款非常灵活的检测器，擅长为检测对象生成合适的边界框。然而，随着IOU阈值增加，YOLOv3 的性能开始同步下降，这时它预测的边界框就不能做到完美对齐了。 在过去的 YOLOv1/v2 上，YOLO 一直在小目标检测领域表现不好，现在 YOLOv3 基本解决了这个问题，有着更好的 $AP_S$ 性能。但是它目前在中等尺寸或大尺寸物体上的表现还相对较差，仍需进一步的完善。 当我们基于 $AP_{50}$ 指标绘制精度和速度曲线（Figure 3）时，我们发现YOLOv3与其他检测系统相比具有显着优势，换句话说，它更快更好。 从 Figure 3 可以看出，YOLOv3 的曲线非常靠近曲线坐标的同时又非常高，这意味着 YOLOv3 有着良好速度的同时又有很好的精度，无愧当前最强目标检测模型。 4，失败的尝试 一些没有作用的尝试工作如下。 Anchor box x,y 偏移预测。我们尝试了常规的 Anchor box 预测方法，比如利用线性激活将坐标 $x、y$ 的偏移程度，预测为边界框宽度或高度的倍数。但我们发现这种做法降低了模型的稳定性，且效果不佳。 用线性方法预测 x,y，而不是使用 logistic。我们尝试使用线性激活函数来直接预测 $x，y$ 的偏移，而不是 ligistic 激活函数，但这降低了 mAP。 focal loss。我们尝试使用focal loss，但它使我们的 mAP降低了 2 点。 对于 focal loss 函数试图解决的问题，YOLOv3 已经具有鲁棒性，因为它具有单独的对象性预测（objectness predictions）和条件类别预测。因此，对于大多数示例来说，类别预测没有损失？或者其他的东西？我们并不完全确定。 双 IOU 阈值和真值分配。在训练过程中，Faster RCNN 用了两个IOU 阈值，如果预测的边框与的 ground truth 的 IOU 是 0.7，那它是正样本 ；如果 IOU 在 [0.3—0.7]之间，则忽略这个 box；如果小于 0.3，那它是个负样本。我们尝试了类似的策略，但效果并不好。 5，改进的意义 YOLOv3 是一个很好的检测器，速度很快，很准确。虽然它在 COCO 数据集上的 mAP 指标，即 $AP{50}$ 到 $AP{90}$ 之间的平均值上表现不好，但是在旧指标 $AP_{50}$ 上，它表现非常好。 总结 YOLOv3 的改进点如下： 使用金字塔网络来实现多尺度预测，从而解决小目标检测的问题。 借鉴残差网络来实现更深的 Darknet-53，从而提升模型检测准确率。 使用 sigmoid 函数替代 softmax 激活来实现多标签分类器。 位置预测修改，一个 gird 预测 3 个 box。 四，YOLOv4 因为 YOLOv1-v3 的作者不再更新 YOLO 框架，所以 Alexey Bochkovskiy 接起了传承 YOLO 的重任。相比于它的前代，YOLOv4 不再是原创性且让人眼前一亮的研究，但是却集成了目标检测领域的各种实用 tricks 和即插即用模块 ，称得上是基于 YOLOv3 框架的各种目标检测 tricks 的集大成者。 本文章不会对原论文进行一一翻译，但是做了系统性的总结和关键部分的翻译。 1，摘要及介绍 我们总共使用了：WRC、CSP、CmBN、SAT、Mish 激活和 Mosaic 数据增强、CIoU 损失方法，并组合其中的一部分，使得最终的检测模型在 MS COCO 数据集、Tesla V100 显卡上达到了 43.5% AP 精度 和 65 FPS 速度。 我们的主要贡献在于： 构建了简单高效的 YOLOv4 检测器，修改了 CBN、PAN、SAM 方法使得 YOLOv4 能够在一块 1080Ti 上就能训练。 验证了当前最先进的 Bag-of-Freebies 和 Bag-of-Specials 方法在训练期间的影响。 目前的目标检测网络分为两种：一阶段和两阶段。检测算法的组成：Object detector = backbone + neck + head，具体结构如下图所示。 2，相关工作 2.1，目标检测方法 按照检测头的不同（head）将目标检测模型分为：两阶段检测和一阶段检测模型，各自代表是 Faster RCNN 和 YOLO 等，最近也出了一些无 anchor 的目标检测器，如 CenterNet 等。近几年来的检测器会在Backbone网络（backbone）和头部网络（head）之间插入一些网络层，主要作用是收集不同阶段的特征。，称其为检测器的颈部（neck）。 neck 通常由几个自下而上（bottom-up）的路径和几个自上而下（top-down）的路径组成。 配备此机制的网络包括特征金字塔网络（FPN）[44]，路径聚合网络（PAN）[49]，BiFPN [77]和NAS-FPN [17]。 一般，目标检测器由以下几部分组成： 2.2，Bag of freebies（免费技巧） 不会改变模型大小，主要是针对输入和 loss 等做的优化工作，一切都是为了让模型训练得更好。 最常用的方式是数据增强（data augmentation），目标是为了提升输入图像的可变性（variability），这样模型在不同环境中会有更高的鲁棒性。常用的方法分为两类：光度失真和几何失真（photometric distortions and geometric distortions）。在处理光度失真时，我们调整了图像的亮度、对比度、色调、饱和度和噪声；对于几何失真，我们添加随机缩放，裁剪，翻转和旋转。 上述数据增强方法都是逐像素调整的，并且保留了调整区域中的所有原始像素信息。此外，也有些研究者将重点放在模拟对象遮挡的问题上，并取得了一些成果。例如随机擦除(random-erase)[100] 和 CutOut [11] 方法会随机选择图像中的矩形区域，并填充零的随机或互补值。而捉迷藏(hide-and-seek)[69] 和网格遮罩(grid-mask)[6] 方法则随机或均匀地选择图像中的多个矩形区域，并将它们替换为所有的 zeros。这个概念有些类似于 Dropout、DropConnect 和 DropBlock 这些在 feature 层面操作的方法，如 。此外，一些研究人员提出了使用多个图像一起执行数据增强的方法。 例如，MixUp 方法使用两个图像以不同的系数比值相乘后叠加，然后使用这些叠加的比值来调整标签。 对于 CutMix，它是将裁切后的图像覆盖到其他图像的矩形区域，并根据混合区域的大小调整标签。 除了以上方法之外，还有 style transfer GAN 方法用于数据扩充、减少 CNN 所学习的纹理偏差。 MIX-UP：Mix-up 在分类任务中，将两个图像按照不同的比例相加，例如 $A\\ast 0.1 + B\\ast 0.9=C$，那么 $C$的 label 就是 $[0.1A, 0.9A]$。在目标检测中的做法就是将一些框相加，这些 label 中就多了一些不同置信度的框。 上面的方法是针对数据增强目标，第二类方法是针对解决数据集中语义分布可能存在偏差的问题（semantic distribution in the dataset may have bias）。在处理语义分布偏差问题时，类别不平衡（imbalance between different classes）问题是其中的一个关键，在两阶段对象检测器中通常通过困难负样本挖掘（hard negative example mining）或在线困难样本挖掘（online hard example mining，简称 OHEM）来解决。但样本挖掘方法并不能很好的应用于一阶段检测器，因为它们都是密集检测架构（dense prediction architecture）。因此，何凯明等作者提出了 Focal Loss 用来解决类别不平衡问题。 另外一个关键问题是，很难用 one-hot hard representation 来表达不同类别之间的关联度的关系，但执行标记时又通常使用这种表示方案。 因此在（Rethinking the inception architecture for computer vision）论文中提出标签平滑（label smoothing）的概念，将硬标签转换为软标签进行训练，使模型更健壮。为了获得更好的软标签，论文(Label refinement network for coarse-to-fine semantic segmentation)介绍了知识蒸馏的概念来设计标签细化网络。 最后一类方法是针对边界框（BBox）回归的目标函数。传统的目标检测器通常使用均方误差（$MSE$）对BBox 的中心点坐标以及高度和宽度直接执行回归，即 $\\lbrace x{center}, y{center}, w, h \\rbrace$ 或者 $\\lbrace x{top-left}, y{top-left}, x{bottom-right}, y{bottom-right} \\rbrace$ 坐标。如果基于锚的方法，则估计相应的偏移量，例如 $\\lbrace x{cener-offset}, y{cener-offset}, w, h \\rbrace$ 或者 $\\lbrace x{top-left-offset}, y{top-left-offset}, x{bottom-right-offset}, y{bottom-right-offset} \\rbrace$。这些直接估计 BBox 的各个点的坐标值的方法，是将这些点视为独立的变量，但是实际上这没有考虑对象本身的完整性。为了更好的回归 BBox，一些研究者提出了 IOU 损失[90]。顾名思义，IoU 损失既是使用 Ground Truth 和预测 bounding box（BBox）的交并比作为损失函数。因为 IoU 是尺度不变的表示，所以可以解决传统方法计算 $\\lbrace x，y，w，h \\rbrace$ 的 $L1$ 或 $L2$ 损失时，损失会随着尺度增加的问题。 最近，一些研究人员继续改善 IoU 损失。 例如，GIoU 损失除了覆盖区域外还包括对象的形状和方向，GIoU 损失的分母为同时包含了预测框和真实框的最小框的面积。DIoU 损失还考虑了对象中心的距离，而CIoU 损失同时考虑了重叠区域，中心点之间的距离和纵横比。 CIoU 损失在 BBox 回归问题上可以实现更好的收敛速度和准确性。 [90] 论文: An advanced object detection network. 2.3，Bag of specials（即插即用模块+后处理方法） 对于那些仅增加少量推理成本但可以显著提高目标检测器准确性的插件模块或后处理方法，我们将其称为 “Bag of specials”。一般而言，这些插件模块用于增强模型中的某些属性，例如扩大感受野，引入注意力机制或增强特征集成能力等，而后处理是用于筛选模型预测结果的方法。 增大感受野模块。用来增强感受野的常用模块有 SPP、ASPP 和 RFB。SPP 起源于空间金字塔匹配（SPM）,SPM 的原始方法是将特征图分割为几个 $d\\times d$ 个相等的块，其中 $d$ 可以为 $\\lbrace 1,2,3，.. \\rbrace$，从而形成空间金字塔。SPP 将 SPM 集成到 CNN 中，并使用最大池化操作（max pooling）替代 bag-of-word operation。原始的 SPP 模块是输出一维特征向量，这在 FCN 网络中不可行。 引入注意力机制。在目标检测中经常使用的注意力模块，通常分为 channel-wise 注意力和 point-wise 注意力。代表模型是 SE 和 SAM（Spatial Attention Module ）。虽然 SE 模块可以提高 ReSNet50 在 ImageNet 图像分类任务 1% 的 top-1 准确率而计算量只增加 2%，但是在 GPU 上，通常情况下，它会将增加推理时间的 10% 左右，所以更适合用于移动端。但对于 SAM，它只需要增加 0.1％ 的额外的推理时间，就可以在 ImageNet 图像分类任务上将 ResNet50-SE 的top-1 准确性提高 0.5％。 最好的是，它根本不影响 GPU 上的推理速度。 特征融合或特征集成。早期的实践是使用 skip connection 或 hyper-column 将低层物理特征集成到高层语义特征。 由于诸如 FPN 的多尺度预测方法已变得流行，因此提出了许多集成了不同特征金字塔的轻量级模块。 这种模块包括 SFAM，ASFF和 BiFPN。 SFAM 的主要思想是使用 SE 模块在多尺度级联特征图上执行通道级级别的加权。 对于 ASFF，它使用softmax 作为逐点级别权重，然后添加不同比例的特征图。在BiFPN 中，提出了多输入加权残差连接以执行按比例的级别重新加权，然后添加不同比例的特征图。 激活函数。良好的激活函数可以使梯度在反向传播算法中得以更有效的传播，同时不会引入过多的额外计算成本。2010 年 Nair 和 Hinton 提出的 ReLU 激活函数，实质上解决了传统的tanh 和 sigmoid 激活函数中经常遇到的梯度消失问题。随后，随后，LReLU，PReLU，ReLU6，比例指数线性单位（SELU），Swish，hard-Swish 和 Mish等激活函数也被提出来，用于解决梯度消失问题。LReLU 和 PReLU 的主要目的是解决当输出小于零时 ReLU 的梯度为零的问题。而 ReLU6 和 Hard-Swish 是专门为量化网络设计的。同时，提出了 SELU 激活函数来对神经网络进行自归一化。 最后，要注意 Swish 和 Mish 都是连续可区分的激活函数。 后处理。最开始常用 NMS 来剔除重复检测的 BBox，但是 NMS 会不考虑上下文信息（可能会把一些相邻检测框框给过滤掉），因此 Girshick 提出了 Soft NMS，为相邻检测框设置一个衰减函数而非彻底将其分数置为零。而 DIoU NMS 则是在 soft NMS 的基础上将中心距离的信息添加到 BBox 筛选过程中。值得一提的是，因为上述后处理方法都没有直接涉及捕获的图像特征，因此在后续的 anchor-free 方法中不再需要 NMS 后处理。 3，方法 我们的基本目标是在生产系统中快速对神经网络进行操作和并行计算优化，而不是使用低计算量理论指示器（BFLOP）。 我们提供了两种实时神经网络： 对于 GPU，我们在卷积层中使用少量分组（1-8）：如CSPResNeXt50 / CSPDarknet53 对于 VPU，我们使用分组卷积，但是我们避免使用 SE-特别是以下模型：EfficientNet-lite / MixNet [76] / GhostNet [21] / MobiNetNetV3 3.1，架构选择 我们的目标是在输入图像分辨率、卷积层数量、参数量、层输出（滤波器）数量之间找到最优平衡。我们大量的研究表面，在 ILSVRC2012(ImageNet) 分类数据集上，CSPResNext50 网络优于 CSPDarknet，但是在 MS COCO 目标检测数据集上，却相反。 这是为什么呢，两种网络，一个分类数据集表现更好，一个检测数据集表现更好。 在分类问题上表现最优的参考模型并不一定总是在检测问题上也表现最优。与分类器相比，检测器需要满足以下条件： 更高的输入网络尺寸（分辨率），用于检测多个小型物体。 更多的网络层，用以得到更高的感受野以覆盖更大的输入网络尺寸。 更多参数，用以得到更大的模型容量，从而可以在单个图像中检测到多个不同大小的对象。 表1 显示了 CSPResNeXt50，CSPDarknet53 和EfficientNet B3 网络的信息。CSPResNext50 仅包含16 个 $3\\times 3$ 卷积层，最大感受野为 $425\\times 425$和网络参数量为 20.6 M，而 CSPDarknet53 包含 29 个 $3\\times 3$ 卷积层，最大感受野为 $725\\times 725$ 感受野和参数量为 27.6 M。理论上的论证再结合作者的大量实验结果，表面 CSPDarknet53 更适合作为目标检测器的 backbone。 不同大小的感受野的影响总结如下： 达到对象大小 - 允许查看整个对象 达到网络大小 - 允许查看对象周围的上下文环境 超过网络规模 - 增加图像点和最终激活之间的连接 我们在 CSPDarknet53 上添加了 SPP 模块，因为它显著增加了感受野，分离出最重要的上下文特征，并且几乎没有降低网络运行速度。 我们使用 PANet 作为针对不同检测器级别的来自不同backbone 级别的参数聚合方法，而不是 YOLOv3中使用的FPN。 最后，我们的 YOLOv4 架构体系如下： backbone：CSPDarknet53 + SPP neck: PANet head：YOLOv3 的 head 3.2，Selection of BoF and BoS 为了更好的训练目标检测模型，CNN 通常使用如下方法： 激活函数：ReLU，leaky-ReLU，parameter-ReLU，ReLU6，SELU，Swish 或 Mish； 边界框回归损失：MSE，IoU，GIoU，CIoU，DIoU 损失； 数据扩充：CutOut，MixUp，CutMix 正则化方法：DropOut，DropPath，空间 DropOut 或 DropBlock 通过均值和方差对网络激活进行归一化：批归一化（BN），交叉-GPU 批处理规范化（CGBN 或 SyncBN），过滤器响应规范化（FRN）或交叉迭代批处理规范化（CBN）； 跳跃连接：残差连接，加残差连接，多输入加权残差连接或跨阶段局部连接（CSP） 以上方法中，我们首先提出了难以训练的 PRELU 和 SELU，以及专为量化网络设计的 ReLU6 激活。因为 DropBlock 作者证明了其方法的有效性，所以正则化方法中我们使用 DropBlock。 3.3，额外的改进 这些方法是作者对现有方法做的一些改进。 为了让 YOLOv4 能更好的在单个 GPU 上训练，我们做了以下额外改进： 引入了新的数据增强方法：Mosaic 和自我对抗训练 self-adversarial training（SAT）。 通过遗传算法选择最优超参数。 修改了 SAM、PAN 和 CmBN。 Mosaic 是一种新的数据增强方法，不像 cutmix 仅混合了两张图片，它混合了 $4$ 张训练图像，从而可以检测到超出其正常上下文的对象。 此外，BN 在每层上计算的激活统计都是来自 4 张不同图像，这大大减少了对大 batch size 的需求。 CmBN 仅收集单个批次中的 mini-batch 之间的统计信息。 我们将 SAM 从 spatial-wise attentation 改为 point-wise attention，并将 PAN 的 shortcut 连接改为 concatenation（拼接），分别如图 5 和图 6 所示。 3.4 YOLOv4 YOLOv4 网络由以下部分组成： Backbone: CSPDarknet53 Neck: SPP, PAN Head: YOLOv3 同时，YOLO v4 使用了： 用于 backbone 的 BoF：CutMix 和 Mosaic数据增强，DropBlock正则化，类标签平滑。 用于 backbone 的 BoS：Mish激活，跨阶段部分连接（CSP），多输入加权残余连接（MiWRC）。 用于检测器的 BoF：CIoU 损失，CmBN，DropBlock 正则化，mosaic 数据增强，自我对抗训练，消除网格敏感性，在单个 ground-truth 上使用多个 anchor，余弦退火调度器，最佳超参数，随机训练形状。 用于检测器 BoS：Mish 激活，SPP 模块，SAM 模块，PAN 路径聚集块，DIoU-NMS。 4，实验 4.1，实验设置 略 4.2，对于分类器训练过程中不同特性的影响 图 7 可视化了不同数据增强方法的效果。 表 2 的实验结果告诉我们，CutMix 和 Mosaic 数据增强，类别标签平滑及 Mish 激活可以提高分类器的精度，尤其是 Mish 激活提升效果很明显。 4.3，对于检测器训练过程中不同特性的影响 $S$: Eliminate grid sensitivit。原来的 $b_x = \\sigma(t_x) + c_x$，因为 sigmoid 函数值域范围是 $(0,1)$ 而不是 $[0,1]$，所以 $b_x$ 不能取到 grid 的边界位置。为了解决这个问题，作者提出将 $\\sigma(t_x)$ 乘以一个超过 $1$ 的系数，如 $b_x = 1.1\\sigma(t_x) + c_x$，$b_y$ 的公式类似。 $IT$：之前的 YOLOv3 是 $1$ 个 anchor 负责一个 GT，现在 YOLOv4 改用多个 anchor 负责一个 GT。对于 GT 来说，只要 $IoU(anchor_i, GT_j) > IoU -threshold$ ，就让 $anchor_i$ 去负责 $GT_j$。 $CIoU$：使用了 GIoU，CIoU，DIoU，MSE 这些误差算法来实现边框回归，验证出 CIoU 损失效果最好。 略 同时实验证明，当使用 SPP，PAN 和 SAM 时，检测器将获得最佳性能。 4.4，不同骨干和预训练权重对检测器训练的影响 综合各种改进后的骨干网络对比实验，发现 CSPDarknet53 比 CSPResNext 模型显示出提高检测器精度的更大能力。 4.5，不同小批量的大小对检测器训练的影响 实验证明，在使用了 BoF 和 BoS 训练策略后，小批量大小（mini-batch sizes）对检测器的性能几乎没有影响。实验结果对比表格就不放了，可以看原论文。 5，结果 与其他 state-of-the-art 目标检测算法相比，YOLOv4 在速度和准确性上都表现出了最优。详细的比较实验结果参考论文的图 8、表 8和表 9。 6，YOLOv4 主要改进点 例举出一些我认为比较关键且值得重点学习的改进点。 6.1，Backbone 改进 后续所有网络的结构图来都源于江大白公众号，之后不再一一注明结构图来源。 Yolov4 的整体结构可以拆分成四大板块，结构图如下图所示。 .png) YOLOv4 的五个基本组件如下： CBM：Yolov4 网络结构中的最小组件，由 Conv+Bn+Mish 激活函数三者组成。 CBL：由 Conv+Bn+Leaky_relu 激活函数三者组成。 Res unit：借鉴 Resnet 网络中的残差结构思想，让网络可以构建的更深，和 ResNet 的 basic block 由两个 CBL（ReLU）组成不同，这里的 Resunit 由 2 个 CBM 组成。 CSPX：借鉴 CSPNet 网络结构，由三个卷积层和 X 个 Res unint 模块 Concate 组成。 SPP：采用 1×1，5×5，9×9，13×13 的最大池化的方式，进行多尺度融合。 其他基础操作： Concat：张量拼接，会扩充维度。 add：逐元素相加操作，不改变维度（element-wise add）。 因为每个 CSPX 模块有 $5+2\\ast X$ 个卷积层，因此整个 backbone 中共有 $1 + (5+2\\times 1) + (5+2\\times 2) + (5+2\\times 8) + (5+2\\times 8) + (5+2\\times 4) = 72$ 个卷积层 这里卷积层的数目 72 虽然不等同于 YOLOv3 中 53，但是 backbone 依然是由 [1、2、8、8、4] 个卷积模块组成的，只是这里的 YOLOv4 中的卷积模块替换为了 CSPX 卷积模块，猜想是这个原因所以 YOLOv4 的作者这里依然用来 Darknet53 命名后缀。 6.1.1，CSPDarknet53 YOLOv4 使用 CSPDarknet53 作为 backbone，它是在 YOLOv3 的骨干网络 Darknet53 基础上，同时借鉴 2019 年的 CSPNet 网络，所产生的新 backbone。 CSPDarknet53 包含 5 个 CSP 模块，CSP 中残差单元的数量依次是 $[1, 2,8,8,4]$，这点和 Darknet53 类似。每个 CSP 模块最前面的卷积核的大小都是 $3\\times 3$，stride=2，因此可以起到下采样的作用（特征图大小缩小一倍）。因为 backbone 总共有 5 个 CSP模块，而输入图像是 $608\\times 608$，所以特征图大小变化是：608->304->152->76->38->19，即经过 bckbone 网络后得到 $19\\times 19$ 大小的特征图。CSPDarknet53 网络结构图如下图所示。 CSPNet 作者认为，MobiletNet、ShuffleNet 系列模型是专门为移动端（CPU）平台上设计的，它们所采用的深度可分离卷积技术（DW+PW Convolution）并不兼容用于边缘计算的 ASIC 芯片。 CSP 结构是一种思想，它和ResNet、DenseNet 类似，可以看作是 DenseNet 的升级版，它将 feature map 拆成两个部分，一部分进行卷积操作，另一部分和上一部分卷积操作的结果进行concate。 CSP 结构主要解决了四个问题： 增强 CNN 的学习能力，能够在轻量化的同时保持着准确性； 降低计算成本； 降低内存开销。CSPNet 改进了密集块和过渡层的信息流，优化了梯度反向传播的路径，提升了网络的学习能力，同时在处理速度和内存方面提升了不少。 能很好的和 ResNet、DarkNet 等网络嵌入在一起，增加精度的同时减少计算量和降低内存成本。 6.1.2，Mish 激活 在 YOLOv4 中使用 Mish 函数的原因是它的低成本和它的平滑、非单调、无上界、有下界等特点，在表 2 的对比实验结果中，和其他常用激活函数如 ReLU、Swish 相比，分类器的精度更好。 Mish 激活函数是光滑的非单调激活函数，定义如下： $$ Mish(x) = x\\cdot tanh(In(1 + e^x)) \\\\ Swish(x) = x\\cdot sigmoid(x) \\\\ $$ Mish 函数曲线图和 Swish 类似，如下图所示。 值得注意的是 Yolov4 的 Backbone 中的激活函数都使用了Mish 激活，但后面的 neck + head 网络则还是使用leaky_relu 函数。 $$ Leaky \\ ReLU(x) = \\begin{cases} x, & x > 0 \\\\ \\lambda x, & x \\leq 0 \\end{cases} $$ 6.1.3，Dropblock Yolov4 中使用的 Dropblock ，其实和常见网络中的 Dropout 功能类似，也是缓解过拟合的一种正则化方式。 传统 dropout 功能是随机删除减少神经元的数量，使网络变得更简单（缓解过拟合）。 6.2，Neck 网络改进 在目标检测领域中，为了更好的融合 low-level 和 high-level 特征，通常会在 backbone 和 head 网络之间插入一些网络层，这个中间部分称为 neck 网络，典型的有 FPN 结构。 YOLOv4 的 neck 结构采用了 SPP 模块 和 FPN+PAN 结构。 先看看 YOLOv3 的 neck 网络的立体图是什么样的，如下图所示。 FPN 是自顶向下的，将高层的特征信息经过上采样后和低层的特征信息进行传递融合，从而得到进行预测的特征图 ①②③。 再看下图 YOLOv4 的 Neck 网络的立体图像，可以更清楚的理解 neck 是如何通过 FPN+PAN 结构进行融合的。 FPN 层自顶向下传达强语义特征，而特征金字塔则自底向上传达强定位特征，两两联手，从不同的主干层对不同的检测层进行参数聚合，这种正向反向同时结合的操作确实 6 啊。 值得注意的是，Yolov3 的 FPN 层输出的三个大小不一的特征图①②③直接进行预测。但Yolov4 输出特征图的预测是使用 FPN 层从最后的一个 76*76 特征图 ① 和而经过两次PAN 结构的特征图 ② 和 ③ 。 另外一点是，原本的 PANet 网络的 PAN 结构中，两个特征图结合是采用 shortcut + element-wise 操作，而 Yolov4 中则采用 concat（route）操作，特征图融合后的尺寸会变化。原本 PAN 和修改后的 PAN 结构对比图如下图所示。 6.3，预测的改进 6.3.1，使用CIoU Loss Bounding Box Regeression 的 Loss 近些年的发展过程是：Smooth L1 Loss-> IoU Loss（2016）-> GIoU Loss（2019）-> DIoU Loss（2020）->CIoU Loss（2020） 6.3.2，使用DIoU_NMS 6.4，输入端改进 6.4.1，Mosaic 数据增强 YOLOv4 原创的 Mosaic 数据增强方法是基于 2019 年提出的 CutMix 数据增强方法做的优化。CutMix 只对两张图片进行拼接，而 Mosaic 更激进，采用 4 张图片，在各自随机缩放、裁剪和排布后进行拼接。 Mosaic数据增强\"> 在目标检测器训练过程中，小目标的 AP 一般比中目标和大目标低很多。而 COCO 数据集中也包含大量的小目标，但比较麻烦的是小目标的分布并不均匀。在整体的数据集中，它们的占比并不平衡。 如上表所示，在 COCO 数据集中，小目标占比达到 41.4%，数量比中目标和大目标要大得多，但是在所有的训练集图片中，只有 52.3% 的图片有小目标，即小物体数量很多、但分布非常不均匀，而中目标和大目标的分布相对来说更加均匀一些。 少部分图片却包含了大量的小目标。 针对这种状况，Yolov4 的作者采用了 Mosaic 数据增强的方式。器主要有几个优点： 丰富数据集：随机使用 4 张图片，随机缩放，再随机分布进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。 减少训练所需 GPU 数量： Mosaic 增强训练时，可以直接计算 4 张图片的数据，使得 Mini-batch 大小并不需要很大，一个 GPU 就可以训练出较好的模型。 五，YOLOv5 YOLOv5 仅在 YOLOv4 发表一个月之后就公布了，这导致很多人对 YOLOv5 的命名有所质疑，因为相比于它的前代 YOLOv4，它在理论上并没有明显的差异，虽然集成了最近的很多新的创新，但是这些集成点又和 YOLOv4 类似。我个人觉得之所以出现这种命名冲突应该是发布的时候出现了 “撞车”，毕竟 YOLOv4 珠玉在前（早一个月），YOLOv5 也只能命名为 5 了。但是，我依然认为 YOLOv5 和 YOLOv4 是不同的，至少在工程上是不同的，它的代码是用 Python(Pytorch) 写的，与 YOLOv4 的 C代码 （基于 darknet 框架）有所不同，所以代码更简单、易懂，也更容易传播。 另外，值得一提的是，YOLOv4 中提出的关键的 Mosaic 数据增强方法，作者之一就是 YOLOv5 的作者 Glenn Jocher。同时，YOLOv5 没有发表任何论文，只是在 github 上开源了代码。 5.1，网络架构 通过解析代码仓库中的 .yaml 文件中的结构代码，YOLOv5 模型可以概括为以下几个部分： Backbone: Focus structure, CSP network Neck: SPP block, PANet Head: YOLOv3 head using GIoU-loss 5.2，创新点 5.2.1，自适应anchor 在训练模型时，YOLOv5 会自己学习数据集中的最佳 anchor boxes，而不再需要先离线运行 K-means 算法聚类得到 k 个 anchor box 并修改 head 网络参数。总的来说，YOLOv5 流程简单且自动化了。 5.2.2， 自适应图片缩放 在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。 5.2.3，Focus结构 Focus 结构可以简单理解为将 $W\\times H$ 大小的输入图片 4 个像素分别取 1 个（类似于邻近下采样）形成新的图片，这样 1 个通道的输入图片会被划分成 4 个通道，每个通道对应的 WH 尺寸大小都为原来的 1/2，并将这些通道组合在一起。这样就实现了像素信息不丢失的情况下，提高通道数（通道数对计算量影响更小），减少输入图像尺寸，从而大大减少模型计算量。 以 Yolov5s 的结构为例，原始 640x640x3 的图像输入 Focus 结构，采用切片操作，先变成 320×320×12 的特征图，再经过一次 32 个卷积核的卷积操作，最终变成 320×320×32 的特征图。 5.3，四种网络结构 YOLOv5 通过在网络结构问价 yaml 中设置不同的 depth_multiple 和 width_multiple 参数，来创建大小不同的四种 YOLOv5 模型：Yolv5s、Yolv5m、Yolv5l、Yolv5x。 5.4，实验结果 各个版本的 YOLOv5 在 COCO 数据集上和 V100 GPU 平台上的模型精度和速度实验结果曲线如下所示。 参考资料 你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (上) 你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (中) 目标检测|YOLO原理与实现 YOLO论文翻译——中英文对照 Training Object Detection (YOLOv2) from scratch using Cyclic Learning Rates 目标检测|YOLOv2原理与实现(附YOLOv3) YOLO v1/v2/v3 论文 https://github.com/BobLiu20/YOLOv3_PyTorch/blob/master/nets/yolo_loss.py https://github.com/Peterisfar/YOLOV3/blob/03a834f88d57f6cf4c5016a1365d631e8bbbacea/utils/datasets.py#L88 深入浅出Yolo系列之Yolov3&Yolov4&Yolov5&Yolox核心基础知识完整讲解 EVOLUTION OF YOLO ALGORITHM AND YOLOV5: THE STATE-OF-THE-ART OBJECT DETECTION ALGORITHM Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/2D目标检测/8-Scaled-YOLOv4论文解读.html":{"url":"6-computer_vision/2D目标检测/8-Scaled-YOLOv4论文解读.html","title":"8-Scaled-YOLOv4论文解读","keywords":"","body":" 一，Scaled YOLOv4 摘要 1，介绍 2，相关工作 2.1，模型缩放 3，模型缩放原则 3.1，模型缩放的常规原则 3.2，为低端设备缩放的tiny模型 3.3，为高端设备缩放的Large模型 4，Scaled-YOLOv4 4.1，CSP-ized YOLOv4 4.2，YOLOv4-tiny 4.3，YOLOv4-large 5，实验 总结 Reference 参考资料 一，Scaled YOLOv4 Scaled YOLOv4 的二作就是 YOLOv4 的作者 Alexey Bochkovskiy。 摘要 作者提出了一种网络缩放方法，不仅可以修改深度、宽度、分辨率，还可以修改网络的结构。 1，介绍 实验结果表明，基于 CSP 方法的 YOLOv4 目标检测模型在保持最优速度和准确率的前提下，同时也具有向上/向下可伸缩性，可用于不同大小的网络。由此，作者提出了一种网络缩放方法，它不仅改变深度、宽度、分辨率，而且还改变网络的结构。 主要工作。Scaled YOLOv4 的主要工作如下： 设计了一种针对小模型的强大的模型缩放方法，系统地平衡了浅层 CNN 的计算代价和存储带宽; 设计一种简单有效的大型目标检测器缩放策略; 分析各模型缩放因子之间的关系，基于最优组划分进行模型缩放; 实验证实了 FPN 结构本质上是一种 once-for-all 结构; 利用上述方法研制了 YOLOv4-tiny 和 YOLO4v4-large 模型。 以往模型缩放，如 EfficientDet 无非是首先选择网络基础模块，它往往又好又快，然后针对影响目标检测的重要参数如：网络宽度 $w$、深度 $d$、输入图像分辨率size 等进行（满足一定条件下按照一定规律）调参，或者 NAS 自动调参。 2，相关工作 2.1，模型缩放 传统的模型缩放是指改变模型的深度，如 VGG 变体，以及后边可以训练更深层的 ResNet 网络等；后面 agoruyko 等人开始考虑模型的宽度，通过改变卷积层卷积核的数量来实现模型缩放，并设计了 Wide ResNet[43]，同样的精度下，它的参数量尽管比原始 ResNet 多，但是推理速度却更快。随后的 DenseNet 和 ResNeXt 也设计了一个复合缩放版本，将深度和宽度都考虑在内。 何凯明等人提出的 ResNet 网络解决了随着深度增加带来的网络退化问题。 3，模型缩放原则 3.1，模型缩放的常规原则 这段内容，原作者的表达不够严谨，计算过程也没有细节，所以我不再针对原文进行一一翻译，而是在原文的基础上，给出更清晰的表达和一些计算细节。 这里，我们得先知道对一个卷积神经网络来说，其模型一般是由 conv stage、conv block、conv layer 组成的。我以 ResNet50 为例进行分析，大家就能明白了。ResNet50 的卷积过程分成 4 个 stage，分别对应的卷积 blocks 数目是 $[3,4,6,3]$，卷积 block 是 bottleneck 残差单元，bottleneck 残差单元又是 $1\\times 1$、$3\\times 3$ 和 $1\\times 1$ 这样 3 个卷积层组成的，所以 ResNet50 总共的卷积层数目为：$3\\times 3 + 4\\times 3+ 6\\times 3 + 3\\times 3 = 48$，再加上第一层的卷积和最后一层的分类层（全连接层），总共是 50 层，所以命名为 ResNet50。ResNet 模型的组成和结构参数表如下图所示。 大部分 backbone 都是分成 4 个 stage。 对一个基础通道数是 $b$ 的卷积模块（conv block），总共有 $k$ 个这样的模块的 CNN 网络来说，其计算代价是这样的。如 ResNet 的总的卷积层的计算量为 $k\\ast [conv(1\\times 1,b/4)\\rightarrow conv(3\\times 3,b/4)\\rightarrow conv(1\\times 1,b)]$；ResNeXt 的总的卷积层的计算量为 $k\\ast [conv(1\\times 1,b/2)\\rightarrow gconv(3\\times 3/32, b/2)\\rightarrow conv(1\\times 1, b)]$；Darknet 网络总的计算量为 $k\\ast [conv(1\\times 1,b/2)\\rightarrow conv(3\\times 3, b)]$。假设可用于调整图像大小、层数和通道数的缩放因子分别为 $\\alpha$、$\\beta$ 和 $\\gamma$。当调整因子变化时，可得出它们和 FLOPs 的关系如下表所示。 这里以 Res layer 为例，进行计算量分析。首先上表的 $r$ 应该是指每个 stage 中间的残差单元的计算量，而且还是 bottleneck 残差单元，因为只有 stage 中间的 bottleneck conv block 的第一个 $1\\times 1$ 卷积层的输入通道数才是输出通道数的 4 倍，只有这种情况算出来的计算量 $r$ 才符合表 1 的结论。 卷积层 FLOPs 的计算公式如下，这里把乘加当作一次计算，公式理解请参考我之前写的 文章。 $FLOPs=(C_i\\times K^2)\\times H\\times W\\times C_o$ 对于上面说的那个特殊的 bottleneck conv block 来说，卷积过程特征图大小没有发生变化，假设特征图大小为 $wh$，所以 bolck 的 FLOPs 为： $$\\begin{align} r1 &= (b \\times 1^2\\times \\frac{b}{4} + \\frac{b}{4} \\times 3^2\\times \\frac{b}{4} + \\frac{b}{4} \\times 1^2\\times b)\\times hw \\\\ &= \\frac{17}{16}whb^2 \\end{align}$$ 这里值得注意的是，虽然各个 conv block 会略有不同，比如 每个 conv stage 的第一个 conv block 都会将特征图缩小一倍，但是其 FLOPs 和 $r1$ 是线性的关系，所以，对于有 $k$ 个 conv block 的 ResNet 来说，其总的计算量自然就可大概近似为 $17whkb^2/16$。ResNeXt 和 Darknet 卷积层的 FLOPs 计算过程类似，所以不再描述。 由表 1 可以看出，图像大小、深度和宽度都会导致计算代价的增加，它们分别成二次，线性，二次增长。 Wang 等人提出的 CSPNet 可以应用于各种 CNN 架构，同时减少了参数和计算量。此外，它还提高了准确性，减少了推理时间。作者把它应用到 ResNet, ResNeXt，DarkNet 后，发现计算量的变化如表 2 所示。 CNN 转换为 CSPNet 后，新的体系结构可以有效地减少 ResNet、ResNeXt 和 Darknet 的计算量（FLOPs），分别减少了 23.5%、46.7% 和 50.0%。因此，作者使用 CSP-ized 模型作为执行模型缩放的最佳模型。 3.2，为低端设备缩放的tiny模型 对于低端设备，设计模型的推理速度不仅受到计算量和模型大小的影响，更重要的是必须考虑外围硬件资源的限制。因此，在执行 tiny 模型缩放时，我们必须考虑以下因素：内存带宽、内存访问代价（MACs）和 DRAM traffic。为了考虑到以上因素，我们的设计必须遵循以下原则： 1，使计算复杂度少于 $O(whkb^2)$。 作者分析了高效利用参数的网络：DenseNet 和 OSANet 的计算量，分别为 $O(whgbk)$、$O(max(whbg, whkg^2))$。两者的计算复杂度阶数均小于 ResNet 系列的 $O(whkb^2)$。因此，我们基于 OSANet 设计 tiny 模型，因为它具有更小的计算复杂度。 这里的 OSANet 其实是 VoVNet 网络，专门为 GPU 平台设计的更高效的 backbone 网络架，其论文解读可参考我之前写的文章。 2，最小化/平衡 feature map 的大小 说实话，没看明白论文这段内容，这不是跟论文 CSPNet 一样的结论吗，即分割为通道数相等的两条路径。 为了获得在计算速度方面的最佳平衡，我们提出了一个新概念：在CSPOSANet 的计算块之间执行梯度截断。如果我们将原来的 CSPNet 设计应用到 DenseNet 或 ResNet 架构上，由于这两种架构的第 $j$ 层输出是第 $1^{st}$ 层到第 $(j-1)^{th}$ 层输出的积分，我们必须将整个计算块作为一个整体来处理。由于 OSANet 的计算块属于 PlainNet 架构，从计算块的任意层制作 CSPNet 都可以达到梯度截断的效果。我们利用该特性对基层的 $b$ 通道和计算块（computational block）生成的 $kg$ 通道进行重新规划，并将其分割为通道数相等的两条路径，如表 4 所示。 当通道数量为 $b + kg$ 时，如果要将这些通道分割成两条路径，最好将其分割成相等的两部分，即 $(b + kg)/2$。 3，在卷积后保持相同的通道数 评估低端设备的计算成本，必须考虑功耗，而影响功耗的最大因素是内存访问代价（$MAC$）。根据 Shufflenetv2 的推导证明，可知卷积层的输入输出通道数相等时，即 $C{in} = C{out}$ 时， $MAC$ 最小。 4，最小化卷积输入/输出(CIO) CIO 是一个可以测量 DRAM IO 状态的指标。表 5 列出了 OSA、CSP 和我们设计的 CSPOSANet 的 CIO。当 $kg > \\frac {b}{2}$ 时，CSPOSANet 可以获得最佳的 CIO。 3.3，为高端设备缩放的Large模型 feature pyramid network (FPN)的架构告诉我们，更高的 stage 更适合预测大的物体。表 7 说明了感受野与几个参数之间的关系。 从表 7 可以看出，宽度缩放可以独立操作。当输入图像尺寸增大时，要想对大对象有更好的预测效果，就必须增大网络的 depth 或 stage （一般每个 stage 都会降低特征图分辨率的一半）的数量。在表 7 中列出的参数中，$\\left { size^{input}, #stage \\right }$ 的组合效果最好。因此，当执行缩放时，我们首先在 $\\left { size^{input} \\right }$，#stage 上执行复合缩放，然后根据实时的环境，我们再分别进一步缩放深度 depth 和宽度 width。 4，Scaled-YOLOv4 4.1，CSP-ized YOLOv4 YOLOv4 是为通用 GPU 上的实时目标检测而设计的。 1，Backbone 为了获得更好的速度/精度权衡，我们将第一个 CSP 阶段转换为原始的 DarkNet 的残差层。 没能理解这段内容。 2，Neck 3，SPP 4.2，YOLOv4-tiny YOLOv4-tiny 是为低端 GPU 设备设计的，设计将遵循 3.2 节中提到的原则。 我们将使用 PCB（partial in computational block） 架构的 CSPOSANet 作为 YOLOv4 backbone。我们设 $g = b/2$ 为增长率，最终使其增长到 $b/2 + kg = 2b$。通过计算，我们得到 $k = 3$。YOLOv4 的卷积块（computational block）结构如图 3 所示。对于每个阶段的通道数量和 neck 网络结构，我们采用 YOLOv3-tiny 一样的设计。 4.3，YOLOv4-large 专门为云端 GPU 设计的，主要目的就是为实现高精度的目标检测。我们设计了一个完全 CSP 化的模型 YOLOv4-P5，并将其扩展到 YOLOv4-P6 和 YOLOv4-P7。Sacled-YOLOv4 large 版本的模型结构图，如下图所示。 我们通过设计 $size^{input}$, #stage 来对 backbone 执行复合缩放。我们把每个 stage 的深度设置为 $2^{d{s{i}}}$。$d_s$ 范围为 $[1, 3, 15, 15, 7, 7, 7]$。与之前的 ResNet 的卷积划分为 4 个 stage 不同，这里最多划分为 7 个 stage（YOLOv4-P7）。 5，实验 与其他实时目标检测检测器进行比较，对比实验结果如表 11 所示。 总结 通篇论文看下来，感觉这篇论文最主要的贡献在于通过简单的理论分析和对比实验，验证了模型缩放的原则，进一步拓展了 CSPNet 方法，并基于此设计了一个全新的 Scaled-YOLOv4。个人感觉就是针对不同的 GPU 平台，可以根据作者分析出来的模型缩放理论且符合其他一些原则的情况下，通过选择不同的模型宽度和深度参数，让模型更深更宽。 anchor-free 的方法，如 centernet 是不需要复杂的后处理，如 NMS。Backbone 模型的宽度、深度、模块的瓶颈比（bottleneck）、输入图像分辨率等参数的关系。 Reference [43] Sergey Zagoruyko and Nikos Komodakis. Wide residualnet works. arXiv preprint arXiv:1605.07146, 2016. 参考资料 Scaled-YOLOv4: Scaling Cross Stage Partial Network Scaled-YOLOv4: Scaling Cross Stage Partial Network 论文翻译 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/3D视觉算法/3D视觉算法初学概述.html":{"url":"6-computer_vision/3D视觉算法/3D视觉算法初学概述.html","title":"3D视觉算法初学概述","keywords":"","body":" 背景知识 RGB-D相机 一，基于3DMM的三维人脸重建技术概述 1.1，3D 人脸重建概述 1.2，初版 3DMM 二，视觉SLAM算法基础概述 2.1，视觉里程计 2.2，后端优化 2.3，回环检测 2.4，建图 三，三维点云语义分割和实例分割综述 3.1，三维数据的表示方法 3.1.1，点云定义 3.1.2，点云的属性： 3.1.3，点云获取 3.1.4，点云存储格式 3.1.5，三维点云的多种表示方法 3.2，基于点云的分类和检测 3.3，基于点云的语义分割 3.3.1，PointNet 网络 四，参考资料 3D 视觉算法包括很多内容，此文仅当作入门了解些概念和知识概括。 背景知识 3D图像描述有多种方法，常见的如下： 点云 网格（meshes） 基于视图的描述 深度图像（depth images） RGB-D相机 一般普通的相机拍出来的图像，其每个像素坐标（x, y）可以获得三种颜色属性（R, G, B）。但在 RGB-D 图像中，每个（x, y）坐标将对应于四个属性（深度 D，R，G，B）。 一，基于3DMM的三维人脸重建技术概述 1.1，3D 人脸重建概述 3D 人脸重建定义：从一张或多张2D图像中重建出人脸的3D模型。数学表达式： $M = (S,T)$ 其中 S 表示人脸 3D 坐标形状向量（shape-vector），T 表示对应点的纹理信息向量（texture-vector）。 3D 人脸重建算法分类： 1.2，初版 3DMM 1999 年论文 《A Morphable Model For The Synthesis Of 3D Faces》提出三维形变模型（3DMM），三维形变模型建立在三维人脸数据库的基础上，以人脸形状和人脸纹理统计为约束，同时考虑到了人脸的姿态和光照因素的影响，因而生成的三维人脸模型精度高。每一个人脸模型都由相应的形状向量 $S_i$ 和 $T_i$组成，其定义如下： $S{newModel} = \\bar{S} + \\sum{i=1}^{m-1} \\alpha{i} s{i}$ $T{newModel} = \\bar{T} + \\sum{i=1}^{m-1} bata{i} t{i}$ 其中 $\\bar{S}$ 表示平均脸部形状模型，$s_i$表示 shape 的 PCA 部分（按照特征值降序排列的协方差阵的特征向量），$\\alpha_i$表示对应形状系数；纹理模型符号定义类似。通过调整形状、纹理系数系数可生成不同的人脸 3D 模型。 二，视觉SLAM算法基础概述  SLAM问题的本质:对运动主体自身和周围环境空间不确定性的估计。为了解决SLAM问题，我们需要状 态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，估计状态的均值和不确定性(方差)。 SLAM 是Simultaneous Localization and Mapping的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器（单目、双目、RGB-D相机、Lidar）的主体，在没有环境先验信息的情况下，在运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉SLAM”；如果传感器位激光，则为激光 SLAM，两者对比如下： SLAM 主要解决定位和地图构建两个问题。视觉 SLAM 流程图如下： 整个视觉 SLAM 流程包括以下几个步骤： 传感器信息读取。视觉 SLAM 中主要指摄像头图像数据读取与预处理。 视觉里程计（Visual Odometry, VO）。视觉里程计的任务是估算相邻图像间相机的运动，以及局部地图的样子。VO 又称为前端 (Front End)。 后端优化（Optimization）。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图。由于接在 VO 之后，又称为后端(Back End)。 回环检测 （Loop Closing）。回环检测判断机器人是否到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。 建图（Mapping）。它根据估计的轨迹，建立与任务要求对应的地图。 2.1，视觉里程计 视觉里程计 VO 目的是通过相邻帧间的图像估计相机运动，并恢复场景的空间结构。其中为了定量地估计相机运动，必须先了解相机与空间点的几何关系。同时，仅通过视觉里程计来估计轨迹，将不可避免地出现累积漂移 (Accumulating Drift)，即每次估计都有误差的情况下，先前时刻的误差将会传递到下一个时刻，导致经过一段时间累积之后，估计的轨迹将不再准确，如下图所示。 2.2，后端优化 概述性的说，后端优化主要目是为了处理 SLAM 过程中噪声的问题。后端优化要考虑的问题，就是如何从这些带有噪声的数据中估计整 个系统的状态，以及这个状态估计的不确定性有多大—这称为最大后 验概率估计(Maximum-a-Posteriori，MAP)。这里的状态既包括机器 人自身的轨迹，也包含地图。 前端与后端的关系：前端给后端提供待优化的数据，以及这些数据的初始值。后端只关心数据的优化过程，不关系这些数据来源于什么传感器。因此在视觉 SLAM 中，前端和计算机视觉研究领域更为相关，比如图像的特征提取与匹配等，后端则主要是滤波与非线性优化算法。 2.3，回环检测 回环检测（又称闭环检测 Loop Closure Detection），主要目的是为了解决位置估计随时间漂移的问题。 可以通过图像相似性来完成回环检测。在检测到回环之后，我们会把“A与B是同一个点”这样的信息告诉 后端优化算法。然后，后端根据这些新的信息，把轨迹和地图调整到符合回环检测结果的样子。这样，如果我们有充分而且正确的回环检测， 就可以消除累积误差，得到全局一致的轨迹和地图。 2.4，建图 建图(Mapping)是指构建地图的过程。这里的地图是对环境的描述，但这个描述并不是固定的，需要视SLAM 的应用而定。 建图技术，根据用途的不同，可以分为稀疏重建和稠密重建，稀疏重建通常是重建一些图像特征点的三维坐标，稀疏重建主要用于定位。稠密建图又称三维重建，是对整个图像或图像中绝大部分像素进行重建，在导航、避障等方面起着举足轻重的作用。 三，三维点云语义分割和实例分割综述 3.1，三维数据的表示方法 三维图像 = 普通的 RGB 三通道彩色图像 + Depth Map。 三维数据有四种表示方法，分别是 point cloud（点云），Mesh（网格），Voxel（体素）以及 Multi-View（多角度图片）。由此也衍生出了对应的三维数据语义和示例分割的算法，但主要是针对 point cloud 的算法越来越多。三维数据集有 ShapeNet、S3DIS、ModelNet40 等。 3.1.1，点云定义 点云简单来说就是一堆三维点的集合，必须包括各个点的三维坐标信息，其他信息比如各个点的法向量、颜色、分类值、强度值、时间等均是可选。 点云在组成特点上分为两种，一种是有序点云，一种是无序点云。 有序点云：一般由深度图还原的点云，有序点云按照图方阵一行一行的，从左上角到右下角排列，当然其中有一些无效点因为。有序点云按顺序排列，可以很容易的找到它的相邻点信息。有序点云在某些处理的时候还是很便利的，但是很多情况下是无法获取有序点云的。 无序点云：无序点云就是其中的点的集合，点排列之间没有任何顺序，点的顺序交换后没有任何影响。是比较普遍的点云形式，有序点云也可看做无序点云来处理。 3.1.2，点云的属性： 空间分辨率、点位精度、表面法向量等。 点云可以表达物体的空间轮廓和具体位置，我们能看到街道、房屋的形状，物体距离摄像机的距离也是可知的；其次，点云本身和视角无关，可以任意旋转，从不同角度和方向观察一个点云，而且不同的点云只要在同一个坐标系下就可以直接融合。 3.1.3，点云获取 点云一般需要通过三维成像传感器获得，比如双目相机、RGB-D相机和 LiDAR激光传感器。 根据激光测量原理得到的点云，包括三维坐标（XYZ）和激光反射强度（Intensity），强度信息与目标的表面材质、粗糙度、入射角方向以及仪器的发射能量、激光波长有关。根据摄影测量原理得到的点云，包括三维坐标（XYZ）和颜色信息（RGB）。结合激光测量和摄影测量原理得到点云，包括三维坐标（XYZ）、激光反射强度（Intensity）和颜色信息（RGB）。 3.1.4，点云存储格式 点云的文件格式可以有很多种，包括 .xyz，npy，ply，obj，off 等（mesh 可以通过泊松采样等方式转化成点云）。对于单个点云，如果你使用np.loadtxt得到的实际上就是一个维度为 的张量，num_channels一般为 3，表示点云的三维坐标。 pts 点云文件格式是最简便的点云格式，直接按 XYZ 顺序存储点云数据， 可以是整型或者浮点型。 LAS 是激光雷达数据（LiDAR），存储格式比 pts 复杂，旨在提供一种开放的格式标准，允许不同的硬件和软件提供商输出可互操作的统一格式。LAS 格式点云截图，其中 C：class(所属类)，F：flight(航线号)，T：time(GPS 时间)，I：intensity(回波强度)，R：return(第几次回波)，N：number of return(回波次数)，A：scan angle(扫描角)，RGB：red green blue(RGB 颜色值)。 .xyz 一种文本格式，前面 3 个数字表示点坐标，后面 3 个数字是点的法向量，数字间以空格分隔。 .pcap 是一种通用的数据流格式，现在流行的 Velodyne 公司出品的激光雷达默认采集数据文件格式。它是一种二进制文件 3.1.5，三维点云的多种表示方法 三维点云除了原始点云表示还要网格 (Mesh) 表示和体素表示，如下图所示： 3.2，基于点云的分类和检测 背景：相比于图像数据，点云不直接包含空间结构，因此点云的深度模型必须解决三个主要问题: 如何从稀疏的点云找到高信息密度的表示。 如何构建一个网络满足必要的限制如 size-variance 和 permutation-invariance。 如何以较低的时间和计算资源消耗处理大量数据。 对点云的分类通常称为三维形状分类。与图像分类模型相似，三维形状分类模型通常是先通过聚合编码器生成全局嵌入，然后将嵌入通过几个完全连通的层来获得最终结果。基于点云聚合方法，分类模型大致可分为两类: 基于投影的方法和基于点的方法。 3.3，基于点云的语义分割 基于点云的语义分割方法大致可分为基于投影的方法和基于点的方法。 3.3.1，PointNet 网络 PointNet 是第一个可以直接处理原始三维点云的深度神经网络，简单来说 PointNet 所作的事情就是对点云做特征学习，并将学习到的特征去做不同的应用：分类（shape-wise feature）、分割（point-wise feature）等。 无论是分类还是分割，本质上都还是分类任务，只是粒度不同罢了。因此损失函数 loss 一定有有监督分类任务中常用的交叉熵 loss，另外 loss 还有之前 alignment network（用于实现网络对于仿射变换、刚体变换等变换的无关性）的约束 loss，也就是上面的 mat_diff_loss 。 PointNet 网络结构如下所示： 其大致的运算流程如下（来自【3D视觉】PointNet和PointNet++）： 输入为一帧的全部点云数据的集合，表示为一个 nx3 的 2d tensor，其中 n 代表点云数量，3 对应 xyz 坐标。 输入数据先通过和一个 T-Net学习到的转换矩阵相乘来对齐，保证了模型的对特定空间转换的不变性。 通过多次 mlp 对各点云数据进行特征提取后，再用一个 T-Net 对特征进行对齐。 在特征的各个维度上执行 maxpooling 操作来得到最终的全局特征。 对分类任务，将全局特征通过 mlp 来预测最后的分类分数。 对分割任务，将全局特征和之前学习到的各点云的局部特征进行串联，再通过 mlp 得到每个数据点的分类结果。 分割任务针对于每一个点做分类，在下面的图中，把全局的特征复制成 n 份然后与之前的 64 维特征进行拼接，然后接着做一个 mlp，最后的输出 nxm 就是每一个点的分类结果。 四，参考资料 细嚼慢咽读论文：PointNet论文及代码详细解析 3D点云基础知识 【3D视觉】PointNet和PointNet++ 点云+深度学习的开山之作–Pointnet Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/工业视觉/Halcon快速入门.html":{"url":"6-computer_vision/工业视觉/Halcon快速入门.html","title":"Halcon快速入门","keywords":"","body":" 前言 一，HALCON 概述 1.1，HALCON 安装 二，HALCON 架构 2.1，算子 2.1.1，参数和数据结构 2.2，拓展包 2.3，接口 2.3.1，HALCON-Python 接口 2.3.2，HALCON-C 接口 2.3.3，HALCON-C++ 接口 2.3.4，HALCON-.NET 接口 2.4，图像获取接口 2.5，I/O 接口 三，如何开发应用 3.1，HDevelop 3.2，示例程序 四，更多参考资料 前言 工业智慧视觉应用主要涉及四个场景：识别、测量、定位、检测。 识别：识别物体的物理特征，包括形状、颜色、字符和条码等，常见的应用场景是 OCR，读取零部件上的字母、数字、字符等用于溯源。 测量：把获取到的图像像素信息标定成常用的度量衡单位，再通过精确计算出目标的几何尺寸。 定位：获取目标的二维/三维位置信息，常用语元件定位，用以辅助机器臂进行后续的抓取等动作。 检测：一般特指缺陷检测，判断产品是否存在缺陷，如零部件缺陷检测等。 一，HALCON 概述 HALCON 是德国 MVtec 公司开发的一款综合性的机器视觉标准软件，拥有全球通用的集成开发环境（HDevelop）。它节约了产品成本，缩短了软件开发周期——HALCON 灵活的架构便于机器视觉，医学图像和图像分析应用的快速开发。在欧洲以及日本的工业界已经是公认具有最佳效能的机器视觉（Machine Vision）软件。 MVTec 提供了 5 种软件：HALCON、MERLIC、深度学习工具、接口、嵌入式视觉，其中 HALCON 是最核心和应用最广的。 HALCON 主要提供的技术有：条形码和二维码读取、BLOB 分析、物图像分类、计算光学成像、过滤技术、缺陷检查、匹配、1D/2D/3D 测量、形态学处理、OCR 和 OCV、基于样本的识别（SBI）、亚像素边缘检测和线条提取技术、深度学习和 3D 视觉技术。 所谓 Blob 分析，即是从连通像素中提取具有相同逻辑状态的特征 (Blob)。 更多技术的描述请参阅官网资料。 1.1，HALCON 安装 注意：HALCON 目前不支持 arm 处理器版的 M1 Pro 机器，而且目前主流是在 Windows 开发居多。 注意，需要先在官网注册账号，然后才能下载对应软件，MVTec HALCON 提供两个不同的软件版本：HALCON 订阅版 (HALCON Progress) 和 HALCON 永久版 (HALCON Steady)。两个版本是完全独立的。 需要许可证，这意味着没有可能从一个版本 \"切换\" 到另一个版本。 HALCON 下载安装步骤如下所示： 进入 HALCON 官网，选择产品版本、操作系统以及架构后就会下载对应版本软件直接点击下载好的安装包即可安装。 安装的详细步骤截图如下所示，试用版不用安装 license 文件，跳过即可。 二，HALCON 架构 HALCON 架构如下图 2.1 所示。HALCON 机器视觉软件的主要部分就是图像处理库，其由超过 2000 多个算子组成，当然我们也可以通过拓展包的形式开发自定义算子，并在程序中使用。 HALCON 提供了通用的图像采集接口来支持不同的图像采集设备（3D相机、相机等），包好特定设备的实现库会在程序运行时动态加载。 2.1，算子 Operators。 我们使用 HALCON 库中任何功能实际上都是通过算子（Operators）完成的，每个功能都有多种实现方法，其可以通过算子参数选择。完整的算子列表在 HALCON Operator Reference，其提供了 HDevelop, .NET, Python, C++, 和 C syntax 接口。HALCON库提供的算子的重要特征如下: 算子之间没有层次结构，所有算子都是一个级别的。 存在逻辑算子。 很多算子可以使用并行加速技术。 算子的输入输出参数的排序是有标准化规则: 输入图标(iconic)、输出图标、输入控制和输出控制。 2.1.1，参数和数据结构  Quick Guide to HALCON Parameters and Data Structures HALCON 算子的参数有两种基本类型：图标数据和控制数据（iconic data and control data）。图像、区域（regions）和 XLD（拓展线描述） 属于标志性数据。 Images（图像）即包含像素值的矩阵，由多个通道组成，其详细定义可以参考《数字图像处理》书籍。这里感兴趣区域 ROI 指的是输入图像的哪一部分区域会被处理，ROI 可以灵活定义，从简单的矩形到一组未连接到像素点都支持。 Regions 由一系列像素组成。区域之中的像素可以不互相连接，任意像素集合都可以作为单个区域处理。 XLDS 包括所有基于轮廓和多边形的数据。像 edges_sub_pix 这样的亚像素精度算子将轮廓作为 XLD 数据返回。 轮廓是一系列 2D 控制点，由线连接。 通常，控制点之间的距离约为 1 个像素。 除了控制点之外，XLD 对象还包含所谓的局部和全局属性。 这些的典型示例是，例如，控制点的边缘幅度或轮廓段的回归参数。 除了提取 XLD 对象外，HALCON 还支持进一步处理。 这方面的示例是基于给定特征范围的轮廓选择，用于将轮廓分割成线、弧、多边形或平行线。 控制数据（control data）包括句柄和基本数据类型，如整数、实数、字符串。 句柄是对复杂数据结构的引用，例如，与图像采集接口或基于形状匹配的模型的连接。 出于效率和数据安全的原因，在操作符之间传递的不是整个结构而是只有句柄。 句柄是不能更改的神奇值(magic values)，并且可能因执行和版本而异。 一旦所有引用被覆盖，它们就会自动清除。 使用句柄的示例有图形窗口、文件、套接字、图像采集接口、OCR、OCV、测量和匹配。 2.2，拓展包 为了支持特殊硬件或实现新的算法，HALCON 支持以 C 语言实现的自定义算子。拓展包接口包含几个预定义的例程和宏，用于在 C 中轻松处理图像数据和内存对象。成功集成新算子后，它可以像任何其他 HALCON 算子一样使用。 2.3，接口 HALCON 支持Python、C、C++ 和 .NET 语言接口，对于·不同编程语言接口，其数据类型、类和算子的命名会有所不同。 2.3.1，HALCON-Python 接口 读取图像并计算连接区域(connected regions)数量的示例代码如下。 img = ha.read_image('pcb') region = ha.threshold(img, 0, 122) num_regions = ha.count_obj(ha.connection(region)) print(f'Number of Regions: {num_regions}') 2.3.2，HALCON-C 接口 C 接口是 HALCON 支持的最简单的接口，每个算子由 1 或 2 个全局函数表示，其中算子的名称和参数序列和 HDevelop 语言相同。 因为 HALCON 算子的本身就是由 C 语言实现的，所以 C 是原生接口，支持也是最好。 以下示例代码也是实现读取图像并计算连接区域(connected regions)数量。 Hobject img; read_image(&img, \"pcb\"); Hobject region; threshold(img, &region, 0, 122); Hobject connected_regions; connection(region, &connected_regions); Hlong num_regions = 0; count_obj(connected_regions, &num_regions); printf(\"Number of Regions: %\" PRIdPTR \"\\n\", num_regions); 2.3.3，HALCON-C++ 接口 C++ 接口比 C 接口复杂得多，应用了 C++ 面向对象编程的优点，包括自动类型转换、构造和析构函数等。另外和 C 接口一样，也为每个 HALCON 算子提供了全局函数，来实现程序化的编程风格（a procedural style of programming）。 读取图像并计算连接区域(connected regions)数量的 C++ 接口实现代码如下。 HImage img{\"pcb\"}; HRegion region = img.Threshold(0, 122); Hlong numRegions = region.Connection().CountObj(); std::cout 2.3.4，HALCON-.NET 接口 略 2.4，图像获取接口 HALCON 通过动态库（Windows: 动态加载库 DLLs, Linux: 共享库 shared libraries）的形式为 50 多个图像采集卡和数百个工业相机提供采集图像的接口。库名称以前缀 hAcq 开头；HALCON XL 使用以 xl 结尾的库。 HALCON 图像采集接口的更新会比 HALCON 库本身更新更为频繁。 成功安装好图像采集设备后，通过 open_framegrabber 算子（需配置设备的名称和其他信息）访问设备，通过 grab_image 算子获取图像。 2.5，I/O 接口 HALCON 对不同 I/O 设备使用同一类算子实现统一访问。安装好 I/O 设备后，使用 open_io_device 算子建立连接，指定 I/O 设备接口的名称；建立连接后，通过调用 open_io_channel 来打开传输通道，然后使用 read_io_channel 和 write_io_channel 算子读取和写入值。 三，如何开发应用 官方推荐使用 HDevelop（HALCON 机器视觉库的交互式开发环境） 进行快速原型设计。在开发好 HDevelop 程序后需要将其转换为最终环境，方法有以下三种： Start from Scratch: 从头(scratch)开始编写程序意味着手动将 HDevelop 代码翻译成目标编程语言（C++、Python...）。 导出 HDevelop 代码: 使用 HDevelop 的代码导出功能将您的 HDevelop 代码自动翻译成目标编程语言。 导出库项目：：HDevelop 的库导出会生成一个即用型项目文件夹，包括目标语言的包装代码和用于构建项目的 CMake 文件。 HDevelop 的库导出使用 HDevEngine，一个充当解释器的库。 3.1，HDevelop 默认情况下，HDevelop 窗口入下图 3.1 所示，窗口主要分为 3 类： 图形窗口: 显示（中间）结果。即显示图像、区域和 XLD 等标志性数据。 程序窗口: 即输入和编辑代码的地方。 变量窗口: 显示所有变量。即显示图标变量（iconic variables）和控制变量。图标变量包含图标数据，控制变量包含控制数据。 3.2，示例程序 推荐观看视频教程: Integrate HDevelop code into a C++ application using the Library Project Export。 分步说明的描述可以参考 《quick_guide》 文档的3.2 节内容。 四，更多参考资料 HALCON 相关文档描述及下载链接汇总如下表所示。 REFERENCE MANUAL 参考手册 下载链接 文件大小 HALCON Operator Reference（HALCON 算子参考资料） 下载 PDF在线阅读 （需要 Javascript） 24.8 MB BASICS 基础知识 下载链接 文件大小 Quick Guide（快速指南） 下载 PDF 2.6 MB Installation Guide（安装指南） 下载 PDF 0.4 MB HDevelop Users' Guide（HDevelop 用户指南） 下载 PDF 6.2 MB Solution Guide I - Basics（解决方案指南 I - 基础知识） 下载 PDF 6.7 MB Solution Guide II - A - Image Acquisition（解决方案指南 II-B - 图像采集） 下载 PDF 0.7 MB Solution Guide II - B - Matching（解决方案指南 II-B - 匹配） 下载 PDF 3.4 MB Solution Guide II - C - 2D Data Codes（解决方案指南 II-C - 二维码） 下载 PDF 4.6 MB Solution Guide II - D - Classification（解决方案指南 II-D - 分类） 下载 PDF 4.3 MB Solution Guide III - A - 1D Measuring（解决方案指南 III-A - 1D 测量） 下载 PDF 1.2 MB Solution Guide III - B - 2D Measuring（解决方案指南 III-B - 2D 测量） 下载 PDF 2.5 MB Solution Guide III - C - 3D Vision（解决方案指南 III-C - 3D 视觉） 下载 PDF 14.2 MB Technical Updates（技术更新） 下载 PDF 0.2 MB Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"6-computer_vision/项目实践/GitHub车牌检测识别项目调研.html":{"url":"6-computer_vision/项目实践/GitHub车牌检测识别项目调研.html","title":"GitHub车牌检测识别项目调研","keywords":"","body":" 一，EasyOCR 1.1，仓库介绍 1.2，使用记录 二，HyperLPR 2.1，HyperLPR 概述 2.3，使用记录 2.3，使用建议 三，simple-car-plate-recognition-2 3.1，仓库介绍 3.2，使用记录 3.3，使用建议 四，车牌检测-License-Plate-Detector 4.1，仓库介绍 4.2，建议 五，MMOCR 5.1，仓库介绍 5.2，使用记录 5.3，使用建议 六，推荐 YOLOv5-LPRNet-Licence-Recognition 6.1，仓库介绍 6.2，使用记录 一，EasyOCR 1.1，仓库介绍 EasyOCR 是一个用于从图像中提取文本的 python 库, 它是一种通用的 OCR，既可以读取自然场景文本，也可以读取文档中的密集文本。目前支持 80 多种语言和所有流行的书写脚本，包括：拉丁文、中文、阿拉伯文、梵文、西里尔文等。 EasyOCR 仓库 截止到 2022-11-8日，star 数为 16.2k，其文件目录和作者给出的一些示例效果如下。 ├── custom_model.md ├── Dockerfile ├── easyocr ├── easyocr.egg-info ├── examples ├── LICENSE ├── MANIFEST.in ├── README.md ├── releasenotes.md ├── requirements.txt ├── scripts ├── setup.cfg ├── setup.py ├── trainer └── unit_test 1.2，使用记录 1，安装较为麻烦 在自行安装了 cuda 库和 pytorch 的基础上，可通过 pip install easyocr 命令安装 easyocr 库，但是注意卸载掉之前安装的 opencv-python 库（如果有）。 2，代码自动下载模型速度很慢 下载的仓库里面默认是不提供任何模型的，因此第一次运行快速推理脚本会自动下载对应的 ocr 模型，但是！如果网络不稳定，其下载速度非常慢，试了 n 次，基本不可能下载成功。 所以一般必须通过 Model hub 页面借助浏览器手动点击下载对应中英文 ocr 识别模型，然后手动把模型文件移动到 ~/.EasyOCR/model 文件夹下。 EasyOCR 仓库主要是通过 download_and_unzip 接口下载对应模型文件的，其也是通过调用 urllib 模块提供的 urlretrieve() 函数来实现文件的下载，其定义如下: def download_and_unzip(url, filename, model_storage_directory, verbose=True): zip_path = os.path.join(model_storage_directory, 'temp.zip') reporthook = printProgressBar(prefix='Progress:', suffix='Complete', length=50) if verbose else None # url 下载链接，zip_path 文件保存的本地路径, reporthook 利用这个回调函数来显示当前的下载进度 urlretrieve(url, zip_path, reporthook=reporthook) with ZipFile(zip_path, 'r') as zipObj: zipObj.extract(filename, model_storage_directory) # 解压到指定目录 os.remove(zip_path) # 移除下载的压缩包文件 3，车牌场景识别准确率非常低 经过我的大量测试，其在中国车牌场景下识别率几乎为 0，我猜测是因为作者提供的训练模型所用的训练数据没有车牌场景的，而 ocr 效果又非常依赖场景数据，所以导致汽车车牌识别率几乎为 0 ，具体实践效果如下。 二，HyperLPR 2.1，HyperLPR 概述 HyperLPR 框架是 github 作者 szad670401 开源的基于深度学习高性能中文车牌识别框架，支持多平台，提供了 Window、Linux、Android、IOS、ROS 平台的支持。 Python 依赖于 Keras (>2.0.0) 和 Theano(>0.9) or Tensorflow(>1.1.x) 机器学习库。项目的 C++ 实现和 Python 实现无任何关联，均为单独实现。 作者提供的测试用例效果如下： 2.3，使用记录 仓库 README 文件描述说 HyperLPR 框架对 python 包支持一键安装: pip install hyperlpr 。但是经过我实际测试发现，pip install hyperlpr 命令只能成功安装 hyperlpr 库. 1，快速上手的 py 代码运行会出错： 2，我把 demo 代码移动到 hyperlpr_py3 目录下运行，不再报上图的错误，但是又报了 opencv 函数版本的问题。 hyperlpr) root@crowd-max:/framework/HyperLPR/hyperlpr_py3# python test.py (1, 3, 150, 400) 40 22 335 123 Traceback (most recent call last): File \"test.py\", line 7, in print(HyperLPR_plate_recognition(image)) File \"/opt/miniconda3/envs/hyperlpr/lib/python3.8/site-packages/hyperlpr/__init__.py\", line 8, in HyperLPR_plate_recognition return PR.plate_recognition(Input_BGR,minSize,charSelectionDeskew) File \"/opt/miniconda3/envs/hyperlpr/lib/python3.8/site-packages/hyperlpr/hyperlpr.py\", line 311, in plate_recognition cropped_finetuned = self.finetune(cropped) File \"/opt/miniconda3/envs/hyperlpr/lib/python3.8/site-packages/hyperlpr/hyperlpr.py\", line 263, in finetune g = self.to_refine(image_, pts) File \"/opt/miniconda3/envs/hyperlpr/lib/python3.8/site-packages/hyperlpr/hyperlpr.py\", line 231, in to_refine mat_ = cv2.estimateRigidTransform(org_pts, target_pts, True) AttributeError: module 'cv2' has no attribute 'estimateRigidTransform' 3，ubuntu16.04+python3.8+cuda11.0 环境下，pip install -r requirements.txt 命令安装依赖包依然会出错。 2.3，使用建议 个人建议直接使用 C++ 版本，截止到 2022-11-8 日为止，纯 Python 版本还是有各种问题。 三，simple-car-plate-recognition-2 3.1，仓库介绍 simple-car-plate-recognition-2仓库 简称：简易车牌字符识别 2-Inception/CTC 。 作者使用的字符识别模型是参考 HyperLPR 里面的一个叫 SegmenationFree-Inception 的模型结构，并改用 pytorch 框架实现，然后训练模型，最后测试用整张车牌图片进行字符识别。 作者所用的车牌训练集，是利用 generateCarPlate 这个车牌生成工具生成的。 3.2，使用记录 直接用车牌做识别，实际测试下来，不管用作者给的模型，还是自己训练的模型，效果都很差。 3.3，使用建议 虽然代码简单，模型结构容易看懂，但是不建议使用，效果不稳定和太差。 四，车牌检测-License-Plate-Detector 4.1，仓库介绍 License-Plate-Detector 仓库 作者利用 Yolov5 模型进行了车牌检测，训练集使用 CCPD 数据集，测试效果如下： 4.2，建议 不建议使用，代码写的不够整洁，使用不够方便，使用 yolov5** 用作车牌检测的模型**的方法还是可以参考下。 五，MMOCR 5.1，仓库介绍 mmocr 是商汤 + openmmlab 实验室开发的 OCR 框架。MMOCR 是基于 PyTorch 和 mmdetection 的开源工具箱，专注于文本检测，文本识别以及相应的下游任务，如关键信息提取。 它是 OpenMMLab 项目的一部分。 主分支目前支持 PyTorch 1.6 以上的版本。mmocr 库的安装，可参考我之前的文章-ubuntu16.04安装mmdetection库。 5.2，使用记录 1，官方提供中文字符识别模型只有一个，其使用步骤如下： 创建 mmocr/data/chineseocr/labels 目录; 为了模型推理成功，下载中文字典，并放置到 labels 目录; wget -c https://download.openmmlab.com/mmocr/textrecog/sar/dict_printed_chinese_english_digits.txt mv dict_printed_chinese_english_digits.txt mmocr/data/chineseocr/labels 运行推理脚本。 python mmocr/utils/ocr.py --det DB_r18 --recog SAR_CN demo/car1.jpeg --output='./' 车牌识别效果不好，测试结果如下： 2，官方提供的测试用例的推理效果如下: 5.3，使用建议 官方提供的不管是中文还是英文文本识别模型，在车牌场景下识别效果都不好，不推荐在车牌识别场景下使用，更适合通用场景。 六，推荐 YOLOv5-LPRNet-Licence-Recognition 6.1，仓库介绍 YOLOv5-LPRNet-Licence-Recognition 项目是使用 YOLOv5s 和 LPRNet 对中国车牌进行检测和识别，车牌数据集是使用 CCPD。 车牌字符识别的准确率如下: model 数据集 epochs acc size LPRNet val 100 94.33% 1.7M LPRNet test 100 94.30% 1.7M 总体模型速度：（YOLOv5 + LPRNet）速度：47.6 FPS（970 GPU）。 6.2，使用记录 作者提供的模型实际测试下来效果还不错，部分示例如下： Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"7-model_compression/":{"url":"7-model_compression/","title":"7. 模型压缩","keywords":"","body":"前言 本目录内容旨在分享轻量级模型总结以及论文详解、神经网络剪枝与量化等模型压缩知识和笔记。 卷积网络压缩方法总结 轻量级网络 MobileNetv1论文详解 ShuffleNetv2论文详解 RepVGG论文详解 CSPNet论文详解 VoVNet论文解读 轻量级模型设计总结 神经网络量化 模型量化基础 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"7-model_compression/轻量级网络论文解析/CSPNet论文详解.html":{"url":"7-model_compression/轻量级网络论文解析/CSPNet论文详解.html","title":"CSPNet论文详解","keywords":"","body":" 摘要 1，介绍 2，相关工作 3，改进方法 3.1，Cross Stage Partial Network 3.2，Exact Fusion Model 4，实验 4.1，实验细节 4.2，消融实验 4.3，实验总结 5，结论 6，代码解读 参考资料 文章同步发于 github、博客园 和 知乎。最新版以 github 为主。如果看完文章有所收获，一定要先点赞后收藏。毕竟，赠人玫瑰，手有余香。 摘要 CSPNet 是作者 Chien-Yao Wang 于 2019 发表的论文 CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN。也是对 DenseNet 网络推理效率低的改进版本。 作者认为网络推理成本过高的问题是由于网络优化中的梯度信息重复导致的。CSPNet 通过将梯度的变化从头到尾地集成到特征图中，在减少了计算量的同时可以保证准确率。CSP（Cross Stage Partial Network，简称 CSPNet） 方法可以减少模型计算量和提高运行速度的同时，还不降低模型的精度，是一种更高效的网络设计方法，同时还能和Resnet、Densenet、Darknet 等 backbone 结合在一起。 1，介绍 虽然已经出现了 MobileNetv1/v2/v3 和 ShuffleNetv1/v2 这种为移动端（CPU）设计的轻量级网络，但是它们所采用的基础技术-深度可分离卷积技术并不适用于 NPU 芯片（基于专用集成电路 (ASIC) 的边缘计算系统）。 CSPNet 和不同 backbone 结合后的效果如下图所示。 和目标检测网络结合后的效果如下图所示。 CSPNet 提出主要是为了解决三个问题： 增强 CNN 的学习能力，能够在轻量化的同时保持准确性。 降低计算瓶颈和 DenseNet 的梯度信息重复。 降低内存成本。 2，相关工作 CNN 架构的设计。 实时目标检测器。 3，改进方法 原论文命名为 Method，但我觉得叫改进方法更能体现章节内容。 3.1，Cross Stage Partial Network 1，DenseNet 其中 $f$ 为权值更新函数，$g_i$ 为传播到第 $i$ 个密集层的梯度。从公式 (2) 可以发现，大量的度信息被重用来更新不同密集层的权值，这将导致无差异的密集层反复学习复制的梯度信息。 2，Cross Stage Partial DenseNet. 作者提出的 CSPDenseNet 的单阶段的架构如图 2(b) 所示。CSPDenseNet 的一个阶段是由局部密集块和局部过渡层组成（a partial dense block and a partial transition layer）。 总的来说，作者提出的 CSPDenseNet 保留了 DenseNet 重用特征特性的优点，但同时通过截断梯度流防止了过多的重复梯度信息。该思想通过设计一种分层的特征融合策略来实现，并应用于局部过渡层（partial transition layer）。 3，Partial Dense Block. 设计局部密集块（partial dense block）的目的是为了 增加梯度路径:通过分块归并策略，可以使梯度路径的数量增加一倍。由于采用了跨阶段策略，可以减轻使用显式特征图 copy 进行拼接所带来的弊端; 每一层的平衡计算:通常，DenseNet 基层的通道数远大于生长速率。由于在局部稠密块中，参与密集层操作的基础层通道仅占原始数据的一半，可以有效解决近一半的计算瓶颈; 减少内存流量: 假设 DenseNet 中一个密集块的基本特征图大小为 $w\\times h\\times c$，增长率为 $d$，共有 $m$ 个密集块。则该密集块的 CIO为 $(c\\times m) + ((m^2+m)\\times d)/2$，而局部密集块（partial dense block）的 CIO为 $((c\\times m) + (m^2+m)\\times d)/2$。虽然 $m$ 和 $d$ 通常比 $c$ 小得多，但是一个局部密集的块最多可以节省网络一半的内存流量。 4，Partial Transition Layer. 设计局部过渡层的目的是使梯度组合的差异最大。局部过渡层是一种层次化的特征融合机制，它利用梯度流的聚合策略来防止不同的层学习重复的梯度信息。在这里，我们设计了两个 CSPDenseNet 变体来展示这种梯度流截断是如何影响网络的学习能力的。 Transition layer 的含义和 DenseNet 类似，是一个 1x1 的卷积层（没有再使用 average pool）。上图中 transition layer 的位置决定了梯度的结构方式，并且各有优势： (c) 图 Fusion First 方式，先将两个部分进行 concatenate，然后再进行输入到Transion layer 中，采用这种做法会是的大量特梯度信息被重用，有利于网络学习； (d) 图 Fusion Last 的方式，先将部分特征输入 Transition layer，然后再进行concatenate，这样梯度信息将被截断，损失了部分的梯度重用，但是由于 Transition 的输入维度比（c）图少，大大减少了计算复杂度。 (b) 图中的结构是论文 CSPNet 所采用的，其结合了 (c)、(d) 的特点，提升了学习能力的同时也提高了一些计算复杂度。 作者在论文中给出其使用不同 Partial Transition Layer 的实验结果，如下图所示。具体使用哪种结构，我们可以根据条件和使用场景进行调整。 5，Apply CSPNet to Other Architectures. 将 CSP 应用到 ResNeXt 或者 ResNet 的残差单元后的结构图如下所示： 3.2，Exact Fusion Model Aggregate Feature Pyramid. 提出了 EFM 结构能够更好地聚集初始特征金字塔。 4，实验 4.1，实验细节 略 4.2，消融实验 EFM 在 COCO 数据集上的消融实验结果。 4.3，实验总结 从实验结果来看，分类问题中，使用 CSPNet 可以降低计算量，但是准确率提升很小；在目标检测问题中，使用 CSPNet 作为Backbone 带来的精度提升比较大，可以有效增强 CNN 的学习能力，同时也降低了计算量。 5，结论 CSPNet 是能够用于移动 gpu 或 cpu 的轻量级网络架构。 作者认为论文最主要的贡献是认识到冗余梯度信息问题，及其导致的低效优化和昂贵的推理计算。同时也提出了利用跨阶段特征融合策略和截断梯度流来增强不同层间学习特征的可变性。 此外，还提出了一种 EFM 结构，它结合了 Maxout 操作来压缩从特征金字塔生成的特征映射，这大大降低了所需的内存带宽，因此推理的效率足以与边缘计算设备兼容。 实验结果表明，本文提出的基于 EFM 的 CSPNet 在移动GPU 和 CPU 的实时目标检测任务的准确性和推理率方面明显优于竞争对手。 6，代码解读 1，Partial Dense Block 的实现，代码可以直接在 Dense Block 代码的基础上稍加修改即可，代码参考 这里。简单的 Dense Block 代码如下： class conv2d_bn_relu(nn.Module): \"\"\" BN_RELU_CONV, \"\"\" def __init__(self, in_channels: object, out_channels: object, kernel_size: object, stride: object, padding: object, dilation=1, groups=1, bias=False) -> object: super(BN_Conv2d, self).__init__() layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias), nn.BatchNorm2d(in_channels), nn.ReLU(inplace=False)] self.seq = nn.Sequential(*layers) def forward(self, x): return self.seq(x) class bn_relu_conv2d(nn.Module): \"\"\" BN_RELU_CONV, \"\"\" def __init__(self, in_channels: object, out_channels: object, kernel_size: object, stride: object, padding: object, dilation=1, groups=1, bias=False) -> object: super(BN_Conv2d, self).__init__() layers = [nn.BatchNorm2d(in_channels), nn.ReLU(inplace=False), nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)] self.seq = nn.Sequential(*layers) def forward(self, x): return self.seq(x) class DenseBlock(nn.Module): def __init__(self, input_channels, num_layers, growth_rate): super(DenseBlock, self).__init__() self.num_layers = num_layers self.k0 = input_channels self.k = growth_rate self.layers = self.__make_layers() def __make_layers(self): layer_list = [] for i in range(self.num_layers): layer_list.append(nn.Sequential( bn_relu_conv2d(self.k0 + i * self.k, 4 * self.k, 1, 1, 0), bn_relu_conv2d(4 * self.k, self.k, 3, 1, 1) )) return layer_list def forward(self, x): feature = self.layers[0](x \"0\") out = torch.cat((x, feature), 1) for i in range(1, len(self.layers)): feature = self.layers[i](out \"i\") out = torch.cat((feature, out), 1) return out # Partial Dense Block 的实现： class CSP_DenseBlock(nn.Module): def __init__(self, in_channels, num_layers, k, part_ratio=0.5): super(CSP_DenseBlock, self).__init__() self.part1_chnls = int(in_channels * part_ratio) self.part2_chnls = in_channels - self.part1_chnls self.dense = DenseBlock(self.part2_chnls, num_layers, k) trans_chnls = self.part2_chnls + k * num_layers self.transtion = conv2d_bn_relu(trans_chnls, trans_chnls, 1, 1, 0) def forward(self, x): part1 = x[:, :self.part1_chnls, :, :] part2 = x[:, self.part1_chnls:, :, :] part2 = self.dense(part2) # 也可以是残差块单元 part2 = self.transtion(part2) # Fusion lirst out = torch.cat((part1, part2), 1) return out 参考资料 增强CNN学习能力的Backbone:CSPNet CSPNet——PyTorch实现CSPDenseNet和CSPResNeXt Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"7-model_compression/轻量级网络论文解析/MobileNetv1论文详解.html":{"url":"7-model_compression/轻量级网络论文解析/MobileNetv1论文详解.html","title":"MobileNetv1论文详解","keywords":"","body":" 1、相关工作 标准卷积 分组卷积 从 Inception module 到 depthwise separable convolutions 2、MobileNets 结构 2.1，深度可分离卷积 Depthwise 卷积 Pointwise 卷积 2.2、网络结构 2.3、宽度乘系数-更小的模型 2.4、分辨率乘系数-减少表示 2.5、模型结构总结 3、实验 4、结论 5、基准模型代码 个人思考 后续改进-MobileDets 参考资料 文章同步发于 github、博客园 和 知乎。最新版以 github 为主。如果看完文章有所收获，一定要先点赞后收藏。毕竟，赠人玫瑰，手有余香。 MobileNet 论文的主要贡献在于提出了一种深度可分离卷积架构（DW+PW 卷积），先通过理论证明这种架构比常规的卷积计算成本（Mult-Adds）更小，然后通过分类、检测等多种实验证明模型的有效性。 1、相关工作 标准卷积 一个大小为 $h_1\\times w_1$ 过滤器（2 维卷积核），沿着 feature map 的左上角移动到右下角，过滤器每移动一次，将过滤器参数矩阵和对应特征图 $h_1 \\times w_1 \\times c_1$ 大小的区域内的像素点相乘后累加得到一个值，又因为 feature map 的数量（通道数）为 $c_1$，所以我们需要一个 shape 为 $ (c_1, h_1, w_1)$ 的滤波器（ 3 维卷积核），将每个输入 featue map 对应输出像素点位置计算和的值相加，即得到输出 feature map 对应像素点的值。又因为输出 feature map 的数量为 $c_2$ 个，所以需要 $c_2$ 个滤波器。标准卷积抽象过程如下图所示。 2D 卷积计算过程动态图如下，通过这张图能够更直观理解卷积核如何执行滑窗操作，又如何相加并输出 $c_2$ 个 feature map ，动态图来源 这里。 分组卷积 Group Convolution 分组卷积，最早见于 AlexNet。常规卷积与分组卷积的输入 feature map 与输出 feature map 的连接方式如下图所示，图片来自CondenseNet。 分组卷积的定义：对输入 feature map 进行分组，然后分组分别进行卷积。假设输入 feature map 的尺寸为 $H \\times W \\times c{1}$，输出 feature map 数量为 $c_2$ 个，如果将输入 feature map 按通道分为 $g$ 组，则每组特征图的尺寸为 $H \\times W \\times \\frac{c_1}{g}$，每组对应的滤波器（卷积核）的 尺寸 为 $h{1} \\times w{1} \\times \\frac{c{1}}{g}$，每组的滤波器数量为 $\\frac{c{2}}{g}$ 个，滤波器总数依然为 $c_2$ 个，即分组卷积的卷积核 shape 为 $(c_2,\\frac{c_1}{g}, h_1,w_1)$。每组的滤波器只与其同组的输入 map 进行卷积，每组输出特征图尺寸为 $H \\times W \\times \\frac{c{2}}{g}$，将 $g$ 组卷积后的结果进行拼接 (concatenate) 得到最终的得到最终尺寸为 $H \\times W \\times c_2$ 的输出特征图，其分组卷积过程如下图所示： 分组卷积的意义：分组卷积是现在网络结构设计的核心，它通过通道之间的稀疏连接（也就是只和同一个组内的特征连接）来降低计算复杂度。一方面，它允许我们使用更多的通道数来增加网络容量进而提升准确率，但另一方面随着通道数的增多也对带来更多的 $MAC$。针对 $1 \\times 1$ 的分组卷积，$MAC$ 和 $FLOPs$ 计算如下： $$ \\begin{align} & MACC = H \\times W \\times 1 \\times 1 \\times \\frac{c{1}}{g}\\frac{c{2}}{g} \\times g = \\frac{hwc{1}c{2}}{g} \\\\ & FLOPs = 2 \\times MACC \\\\ & Params = g \\times \\frac{c2}{g}\\times\\frac{c_1}{g} \\times 1\\times 1 + c_2 = \\frac{c{1}c{2}}{g} \\\\ & MAC = HW(c_1 + c_2) + \\frac{c{1}c_{2}}{g} \\\\ \\end{align}$$ 从以上公式可以得出分组卷积的参数量和计算量是标准卷积的 $\\frac{1}{g}$ 的结论 ，但其实对分组卷积过程进行深入理解之后也可以直接得出以上结论。 分组卷积的深入理解：对于 $1\\times 1$ 卷积，常规卷积输出的特征图上，每一个像素点是由输入特征图的 $c_1$ 个点计算得到，而分组卷积输出的特征图上，每一个像素点是由输入特征图的 $ \\frac{c_1}{g}$个点得到（参考常规卷积计算过程）。卷积运算过程是线性的，自然，分组卷积的参数量和计算量是标准卷积的 $\\frac{1}{g}$ 了。 当分组卷积的分组数量 = 输入 feature map 数量 = 输出 feature map 数量，即 $g=c1=c_2$，有 $c_1$ 个滤波器，且每个滤波器尺寸为 $1 \\times K \\times K$ 时，Group Convolution 就成了 Depthwise Convolution（DW 卷积），DW 卷积的卷积核权重尺寸为 $(c{1}, 1, K, K)$。 常规卷积的卷积核权重 shape 都为（C_out, C_in, kernel_height, kernel_width），分组卷积的卷积核权重 shape 为（C_out, C_in/g, kernel_height, kernel_width），DW 卷积的卷积核权重 shape 为（C_in, 1, kernel_height, kernel_width）。 从 Inception module 到 depthwise separable convolutions 深度可分离卷积（depthwise separable convolutions）的提出最早来源于 Xception 论文，Xception 的论文中提到，对于卷积来说，卷积核可以看做一个三维的滤波器：通道维+空间维（Feature Map 的宽和高），常规的卷积操作其实就是实现通道相关性和空间相关性的联合映射。Inception 模块的背后存在这样的一种假设：卷积层通道间的相关性和空间相关性是可以退耦合（完全可分）的，将它们分开映射，能达到更好的效果（the fundamental hypothesis behind Inception is that cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly.）。 引入深度可分离卷积的 Inception，称之为 Xception，其作为 Inception v3 的改进版，在 ImageNet 和 JFT 数据集上有一定的性能提升，但是参数量和速度并没有太大的变化，因为 Xception 的目的也不在于模型的压缩。深度可分离卷积的 Inception 模块如图 Figure 4 所示。 Figure 4 中的“极限” Inception 模块与本文的主角-深度可分离卷积模块相似，区别在于：深度可分离卷积先进行 channel-wise 的空间卷积，再进行 $1 \\times 1$ 的通道卷积，Figure 4 的 Inception 则相反； 2、MobileNets 结构 2.1，深度可分离卷积 MobileNets 是谷歌 2017 年提出的一种高效的移动端轻量化网络，其核心是深度可分离卷积（depthwise separable convolutions），深度可分离卷积的核心思想是将一个完整的卷积运算分解为两步进行，分别为 Depthwise Convolution（DW 卷积） 与 Pointwise Convolution（PW 卷积）。深度可分离卷积的计算步骤和滤波器尺寸如下所示。 Depthwise 卷积 注意本文 DW 和 PW 卷积计算量的计算与论文有所区别，本文的输出 Feature map 大小是 $D_G \\times D_G$， 论文公式是$D_F \\times D_F$。 不同于常规卷积操作， Depthwise Convolution 的一个卷积核只负责一个通道，一个通道只能被一个卷积核卷积（不同的通道采用不同的卷积核卷积），也就是输入通道、输出通道和分组数相同的特殊分组卷积，因此 Depthwise（DW）卷积不会改变输入特征图的通道数目。深度可分离卷积的 DW卷积步骤如下图： DW 卷积的计算量 $MACC = M \\times D{G}^{2} \\times D{K}^{2}$ Pointwise 卷积 上述 Depthwise 卷积的问题在于它让每个卷积核单独对一个通道进行计算，但是各个通道的信息没有达到交换，从而在网络后续信息流动中会损失通道之间的信息，因此论文中就加入了 Pointwise 卷积操作，来进一步融合通道之间的信息。PW 卷积是一种特殊的常规卷积，卷积核的尺寸为 $1 \\times 1$。PW 卷积的过程如下图： 假设输入特征图大小为 $D{G} \\times D{G} \\times M$，输出特征图大小为 $D{G} \\times D{G} \\times N$，则滤波器尺寸为 $1 \\times 1 \\times M$，且一共有 $N$ 个滤波器。因此可计算得到 PW 卷积的计算量 $MACC = N \\times M \\times D_{G}^{2}$。 综上：Depthwise 和 Pointwise 卷积这两部分的计算量相加为 $MACC1 = M \\times D{G}^{2} \\times D{K}^{2} + N \\times M \\times D{G}^{2}$，而标准卷积的计算量 $MACC2 = N \\times D{G}^{2} \\times D_{K}^{2} \\times M$。所以深度可分离卷积计算量于标准卷积计算量比值的计算公式如下。 $$ \\begin{align} \\frac{Depthwise \\ Separable \\ Conv}{Standard \\ Conv} &= \\frac{M \\times D{G}^{2}(D{K}^{2} + N)}{N \\times D{G}^{2} \\times D{K}^{2} \\times M} \\\\ &= \\frac{D{K}^{2} + N}{D{K}^{2} \\times N} \\\\ &= \\frac{1}{N} + \\frac{1}{D_{K}^{2}} \\\\ \\end{align} $$ 可以看到 Depthwise + Pointwise 卷积的计算量相较于标准卷积近乎减少了 $N$ 倍，$N$ 为输出特征图的通道数目，同理参数量也会减少很多。在达到相同目的（即对相邻元素以及通道之间信息进行计算）下， 深度可分离卷积能极大减少卷积计算量，因此大量移动端网络的 backbone 都采用了这种卷积结构，再加上模型蒸馏，剪枝，能让移动端更高效的推理。 深度可分离卷积的详细计算过程可参考 Depthwise卷积与Pointwise卷积。 2.2、网络结构 $3 \\times 3$ 的深度可分离卷积 Block 结构如下图所示： 左边是带 bn 和 relu 的标准卷积层，右边是带 bn 和 relu 的深度可分离卷积层。 $3 \\times 3$ 的深度可分离卷积 Block 网络的 pytorch 代码如下： class MobilnetV1Block(nn.Module): \"\"\"Depthwise conv + Pointwise conv\"\"\" def __init__(self, in_channels, out_channels, stride=1): super(MobilnetV1Block, self).__init__() # dw conv kernel shape is (in_channels, 1, ksize, ksize) self.dw = nn.Conv2d(in_channels, in_channels, kernel_size=3,stride=stride,padding=1, groups=in_channels, bias=False) self.bn1 = nn.BatchNorm2d(in_channels) self.pw = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) def forward(self, x): out1 = F.relu(self.bn1(self.dw(x))) out2 = F.relu(self.bn2(self.pw(out1))) return out2 MobileNet v1 的 pytorch 模型导出为 onnx 模型后，深度可分离卷积 block 结构图如下图所示。 仅用 MobileNets 的 Mult-Adds（乘加操作）次数更少来定义高性能网络是不够的，确保这些操作能够有效实施也很重要。例如非结构化稀疏矩阵运算（unstructured sparse matrix operations）通常并不会比密集矩阵运算（dense matrix operations）快，除非是非常高的稀疏度。 这句话是不是和 shufflenet v2 的观点一致，即不能仅仅以 FLOPs 计算量来表现网络的运行速度，除非是同一种网络架构。 MobileNet 模型结构将几乎所有计算都放入密集的 1×1 卷积中（dense 1 × 1 convolutions），卷积计算可以通过高度优化的通用矩阵乘法（GEMM）函数来实现。 卷积通常由 GEMM 实现，但需要在内存中进行名为 im2col 的初始重新排序，然后才映射到 GEMM。 caffe 框架就是使用这种方法实现卷积计算。 1×1 卷积不需要在内存中进行重新排序，可以直接使用 GEMM（最优化的数值线性代数算法之一）来实现。 如表 2 所示，MobileNet 模型的 1x1 卷积占据了 95% 的计算量和 75% 的参数，剩下的参数几乎都在全连接层中， 3x3 的 DW 卷积核常规卷积占据了很少的计算量（Mult-Adds）和参数。 2.3、宽度乘系数-更小的模型 尽管基本的 MobileNet 体系结构已经很小且网络延迟 latency 很低，但很多情况下特定用例或应用可能要求模型变得更小，更快。为了构建这些更小且计算成本更低的模型，我们引入了一个非常简单的参数 $\\alpha$，称为 width 乘数。宽度乘数 $\\alpha$ 的作用是使每一层的网络均匀变薄。对于给定的层和宽度乘数 $\\alpha$，输入通道的数量变为 $\\alpha M$，而输出通道的数量 $N$ 变为 $\\alpha N$。具有宽度乘数 $\\alpha$ 的深度可分离卷积（其它参数和上文一致）的计算成本为： $$\\alpha M \\times D{G}^{2} \\times D{K}^{2} + \\alpha N \\times \\alpha M \\times D_{G}^{2}$$ 其中 $\\alpha \\in (0,1]$，典型值设置为 1、0.75、0.5 和 0.25。$\\alpha = 1$ 是基准 MobileNet 模型，$\\alpha MobileNets。宽度乘数的作用是将计算量和参数数量大约减少 $\\alpha^2$ 倍，从而降低了网络计算成本（ computational cost of a neural network）。 宽度乘数可以应用于任何模型结构，以定义新的较小模型，且具有合理的准确性、网络延迟 latency 和模型大小之间的权衡。 它用于定义新的精简结构，需要从头开始进行训练模型。基准 MobileNet 模型的整体结构定义如表 1 所示。 2.4、分辨率乘系数-减少表示 减少模型计算成本的的第二个超参数（hyper-parameter）是分辨率因子 $\\rho$，论文将其应用于输入图像，则网络的每一层 feature map 大小也要乘以 $\\rho$。实际上，论文通过设置输入分辨率来隐式设置 $\\rho$。 将网络核心层的计算成本表示为具有宽度乘数 $\\alpha$ 和分辨率乘数 $\\rho$ 的深度可分离卷积的公式如下： $$\\alpha M \\times \\rho D{G}^{2} \\times D{K}^{2} + \\alpha N \\times \\alpha M \\times \\rho D_{G}^{2}$$ 其中 $\\rho \\in (0,1]$，通常是隐式设置的，因此网络的输入分辨率为 224、192、160 或 128。$\\rho = 1$ 时是基准(baseline) MobilNet，$\\rho MobileNets。分辨率乘数的作用是将计算量减少 $\\rho^2$。 2.5、模型结构总结 整个网络不算平均池化层与 softmax 层，且将 DW 卷积和 PW 卷积计为单独的一层，则 MobileNet 有 28 层网络。+ 在整个网络结构中步长为2的卷积较有特点，卷积的同时充当下采样的功能； 第一层之后的 26 层都为深度可分离卷积的重复卷积操作，分为 4 个卷积 stage； 每一个卷积层（含常规卷积、深度卷积、逐点卷积）之后都紧跟着批规范化和 ReLU 激活函数； 最后一层全连接层不使用激活函数。 3、实验 作者分别进行了 Stanford Dogs dataset 数据集上的细粒度识别、大规模地理分类、人脸属性分类、COCO 数据集上目标检测的实验，来证明与 Inception V3、GoogleNet、VGG16 等 backbone 相比，MobilNets 模型可以在计算量（Mult-Adds）数 10 被下降的情况下，但是精度却几乎不变。 4、结论 论文提出了一种基于深度可分离卷积的新模型架构，称为 MobileNets。 在相关工作章节中，作者首先调查了一些让模型更有效的重要设计原则，然后，演示了如何通过宽度乘数和分辨率乘数来构建更小，更快的 MobileNet，通过权衡合理的精度以减少模型大小和延迟。 然后，我们将不同的 MobileNets 与流行的模型进行了比较，这些模型展示了出色的尺寸，速度和准确性特性。 最后，论文演示了 MobileNet 在应用于各种任务时的有效性。 5、基准模型代码 自己复现的基准 MobileNet v1 代模型 pytorch 代码如下： import torch import torch.nn as nn import torch.nn.functional as F import torchvision.models as models from torch import flatten class MobilnetV1Block(nn.Module): \"\"\"Depthwise conv + Pointwise conv\"\"\" def __init__(self, in_channels, out_channels, stride=1): super(MobilnetV1Block, self).__init__() # dw conv kernel shape is (in_channels, 1, ksize, ksize) self.dw = nn.Sequential( nn.Conv2d(in_channels, 64, kernel_size=3, stride=stride, padding=1, groups=4, bias=False), nn.BatchNorm2d(in_channels), nn.ReLU(inplace=True) ) # print(self.dw[0].weight.shape) # print dw conv kernel shape self.pw = nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True) ) def forward(self, x): x = self.dw(x) x = self.pw(x) return x def convbn_relu(in_channels, out_channels, stride=2): return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)) class MobileNetV1(nn.Module): # (32, 64, 1) means MobilnetV1Block in_channnels is 32, out_channels is 64, no change in map size. stage_cfg = [(32, 64, 1), (64, 128, 2), (128, 128, 1), # stage1 conv (128, 256, 2), (256, 256, 1), # stage2 conv (256, 512, 2), (512, 512, 1), (512, 512, 1), (512, 512, 1), (512, 512, 1), (512, 512, 1), # stage3 conv (512, 1024, 2), (1024, 1024, 1) # stage4 conv ] def __init__(self, num_classes=1000): super(MobileNetV1, self).__init__() self.first_conv = convbn_relu(3, 32, 2) # Input image size reduced by half self.stage_layers = self._make_layers(in_channels=32) self.linear = nn.Linear(1024, num_classes) # 全连接层 def _make_layers(self, in_channels): layers = [] for x in self.stage_cfg: in_channels = x[0] out_channels = x[1] stride = x[2] layers.append(MobilnetV1Block(in_channels, out_channels, stride)) in_channels = out_channels return nn.Sequential(*layers) def forward(self, x): \"\"\"Feature map shape(h、w) is 224 -> 112 -> 56 -> 28 -> 14 -> 7 -> 1\"\"\" x = self.first_conv(x) x = self.stage_layers(x) x = F.avg_pool2d(x, 7) # x shape is 7*7 x = flatten(x, 1) # x = x.view(x.size(0), -1) x = self.linear(x) return x if __name__ == \"__main__\": model = MobileNetV1() model.eval() # set the model to inference mode input_data = torch.rand(1, 3, 224, 224) outputs = model(input_data) print(\"Model output size is\", outputs.size()) 程序运行结果如下： Model output size is torch.Size([1, 1000]) 个人思考 在降低 FLOPs 计算量上，MobileNet 的网络架构设计确实很好，但是 MobileNet 模型在 GPU、DSP 和 TPU 硬件上却不一定性能好，原因是不同硬件进行运算时的行为不同，从而导致了 FLOPs少不等于 latency 低的问题。 如果要实际解释 TPU 与 DSP 的运作原理，可能有点麻烦，可以参考下图，从结果直观地理解他们行为上的差异。考虑一个简单的 convolution，在 CPU 上 latency 随着 input 与 output 的channel 上升正相关的增加。然而在 DSP 上却是阶梯型，甚至在更高的 channel 数下存在特别低latency 的甜蜜点。 在一定的程度上，网络越深越宽，性能越好。宽度，即通道(channel)的数量，网络深度，即 layer 的层数，如 resnet18 有 18 个卷积层。注意我们这里说的和宽度学习一类的模型没有关系，而是特指深度卷积神经网络的(通道)宽度。 网络深度的意义：CNN 的网络层能够对输入图像数据进行逐层抽象，比如第一层学习到了图像边缘特征，第二层学习到了简单形状特征，第三层学习到了目标形状的特征，网络深度增加也提高了模型的抽象能力。 网络宽度的意义：网络的宽度（通道数）代表了滤波器（3 维）的数量，滤波器越多，对目标特征的提取能力越强，即让每一层网络学习到更加丰富的特征，比如不同方向、不同频率的纹理特征等。 后续改进-MobileDets FLOPs 低不等于 latency 低，尤其是在有加速功能的硬体 (GPU、DSP与 TPU )上不成立。 MobileNet conv block (depthwise separable convolution)在有加速功能的硬件（专用硬件设计-NPU 芯片）上比较没有效率。 低 channel 数的情况下 (如网路的前几层)，在有加速功能的硬件使用普通 convolution 通常会比separable convolution 有效率。 在大多数的硬件上，channel 数为 8 的倍数比较有利计算。 DSP 与 TPU 上，一般我们需要运算为 uint8 形式，quantization（低精度量化）是常见的技巧。 DSP 与 TPU 上，h-Swish 与 squeeze-and-excitation 效果不明显 (因为硬体设计与 uint8 压缩的关系)。 DSP 与 TPU 上，5x5 convolution 比较没效率。 参考资料 Group Convolution分组卷积，以及Depthwise Convolution和Global Depthwise Convolution 理解分组卷积和深度可分离卷积如何降低参数量 深度可分离卷积（Xception 与 MobileNet 的点滴） MobileNetV1代码实现 Depthwise卷积与Pointwise卷积 【CNN结构设计】深入理解深度可分离卷积 FLOPs与模型推理速度 MobileDets: FLOPs不等于Latency，考量不同硬体的高效架构 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"7-model_compression/轻量级网络论文解析/RepVGG论文详解.html":{"url":"7-model_compression/轻量级网络论文解析/RepVGG论文详解.html","title":"RepVGG论文详解","keywords":"","body":" 背景知识 VGG 和 ResNet 回顾 MAC 计算 卷积运算与矩阵乘积 点积 ACNet 理解 ACBlock 的 Pytorch 代码实现 摘要 RepVGG 模型定义 RepVGG Block 结构 RepVGG Block 的结构重参数化 结论 RepVGG 的问题 参考资料 文章同步发于 github、博客园 和 知乎。最新版以 github 为主。如果看完文章有所收获，一定要先点赞后收藏。毕竟，赠人玫瑰，手有余香。 RepVGG 是截止到 2021.2.9 日为止最新的一个轻量级网络架构。在我的测试中，其在安霸 CV22 上的加速效果不如 ShuffleNet v2。根据作者的描述，RepVGG 是为 GPU 和专用硬件设计的高效模型，追求高速度、省内存，较少关注参数量和理论计算量。在低算力设备上，可能不如 MobileNet 和 ShuffleNet 系列适用。 背景知识 VGG 和 ResNet 回顾 1，VGGNet 系列网络有 5 个卷积阶段 ，以 VGG16 为例，每一个卷积阶段内有 2~3 个卷积层，同时每段尾部会连接一个最大池化层来缩小 Feature map 尺寸。每个阶段内的卷积核数量一样，越靠后的卷积核数量越多，分别是: 64-128-256-512-512。VGG16 每段卷积对应的卷积层数量为 2-2-3-3-3，5 段卷积的总层数为 $2+2+3+3+3 = 13$，再加上最后的三个全连接分类层，总共是 16 层网络，所以命令为 VGG16。5 个卷积阶段的卷积核数量依次呈 2 倍递增关系: 64-128-256-512-512。VGG 系列网络结构参数表如下图所示。 2，ResNet18 也拥有 5 个卷积阶段 ，由 1 个单独的 $7 \\times 7$ 卷积层和工程代码中 make_layer 函数产生的四个 layer（四个卷积阶段）组成，每个 layer 的基础残差模块（basic block）数量（即 units 数量）为 2，因为 basic block 中只包含了 2 层卷积，故所有残差模块的总层数为 $(2+2+2+2)*2=16$，再加上第一层的卷积和最后一层的分类，总共是 18 层，所以命名为 ResNet18。5 段卷积的卷积核数量也依次呈 2 倍递增关系: 64-64-128-256-512。ResNet18 网络具体参数表如下所示。 总结：小卷积核代替大卷积核，分段卷积，卷积核数量逐段呈 2 倍递增，Feature Map 尺寸逐段呈 1/2 倍递减。 MAC 计算 MAC(memory access cost) 内存访问次数也叫内存使用量，CNN 网络中每个网络层 MAC 的计算分为读输入 feature map 大小、权重大小（DDR 读）和写输出 feature map 大小（DDR 写）三部分。 以卷积层为例计算 MAC，可假设某个卷积层输入 feature map 大小是 (Cin, Hin, Win)，输出 feature map 大小是 (Hout, Wout, Cout)，卷积核是 (Cout, Cin, K, K)，理论 MAC（理论 MAC 一般小于 实际 MAC）计算公式如下： feature map 大小一般表示为 （N, C, H, W），MAC 指标一般用在端侧模型推理中，端侧模型推理模式一般都是单帧图像进行推理，即 N = 1(batch_size = 1)，不同于模型训练时的 batch_size 大小一般大于 1。 input = Hin x Win x Cin # 输入 feature map 大小 output = Hout x Wout x Cout # 输出 feature map 大小 weights = K x K x Cin x Cout + bias # bias 是卷积层偏置 ddr_read = input + weights ddr_write = output MAC = ddr_read + ddr_write 卷积运算与矩阵乘积 点积 在数学中，点积（英语：Dot Product）又称数量积或标量积（英语：Scalar Product），是一种接受两个等长的数字序列（通常是坐标向量）、返回单个数字的代数运算。 在欧几里得几何中，两个笛卡尔坐标向量的点积常称为内积（英语：Inner Product），见内积空间。 卷积运算，可以看作是一串内积运算，等效于矩阵相乘，因此卷积满足交换、结合等定律。 矩阵乘积的常用性质： 分配律： $A(B+C) = AB+BC$ 结合律： $A(BC) = (AB)C$ 交换律：绝大多数情况不满足交换律，即大多数情况下 $AB \\neq BA$。 ACNet 理解 学习 ACNet 之前，首先得理解卷积计算的恒等式，它是“结构重参数化思想”的理论基础，卷积计算的恒等式的示意图如下图 2 所示。 概念结构重参数化（structural re-parameterization）指的是首先构造一系列结构（一般用于训练），并将其参数等价转换为另一组参数（一般用于推理），从而将这一系列结构等价转换为另一系列结构。 下面等式表达的意思就是对于输入特征图 $I$，先进行 $K^{(1)}$ 和 $I$ 卷积、$K^{(2)}$ 和 $I$ 卷积，再对结果进行相加，与先进行 $K^{(1)}$ 和 $K^{(2)}$ 的逐点相加后再和 $I$ 进行卷积得到的结果是一致的（可通过矩阵乘积的分配律来理解）。这是 ACNet 在推理阶段不增加任何计算量的理论基础，但同时训练阶段计算量增加，训练时间更长，需要的显存更大。 $$I \\ast K^{(1)} + I \\ast K^{(2)} = I \\ast (K^{(1)} \\oplus K^{(2)})$$ ACNet 的创新分为训练和推理阶段： 训练阶段：将现有网络中的每一个 $3 \\times 3$ 卷积层换成 $3 \\times 1$ 卷积 + $1 \\times 3$卷积 + $3 \\times 3$ 卷积共三个卷积层，并将三个卷积层的计算结果进行相加得到最终卷积层的输出。因为这个过程引入的 $1 \\times 3$ 卷积和 $3 \\times 1$ 卷积是非对称的，所以将其命名为 Asymmetric Convolution。论文中有实验证（见论文 Table 4）明引入 $1 \\times 3$ 这样的水平卷积核可以提升模型对图像上下翻转的鲁棒性，竖直方向的 $3 \\times 1$ 卷积核同理。 推理阶段：主要是对三个卷积核进行融合，这部分在实现过程中就是使用融合后的卷积核参数来初始化现有的网络。 推理阶段的卷积融合操作是和 BN 层一起的，融合操作发生在 BN 之后，论文实验证明融合在 BN 之后效果更好些。推理阶段卷积层融合操作示意图如下所示（BN 操作省略了 $\\varepsilon$）： ACBlock 的训练阶段权重参数转化为推理阶段的权重参数的代码如下所示。 def get_equivalent_kernel_bias(self): hor_k, hor_b = self._fuse_bn_tensor(self.hor_conv, self.hor_bn) ver_k, ver_b = self._fuse_bn_tensor(self.ver_conv, self.ver_bn) square_k, square_b = self._fuse_bn_tensor(self.square_conv, self.square_bn) self._add_to_square_kernel(square_k, hor_k) self._add_to_square_kernel(square_k, ver_k) return square_k, hor_b + ver_b + square_b def _fuse_bn_tensor(self, conv, bn): std = (bn.running_var + bn.eps).sqrt() t = (bn.weight / std).reshape(-1, 1, 1, 1) return conv.weight * t, bn.bias - bn.running_mean * bn.weight / std def _add_to_square_kernel(self, square_kernel, asym_kernel): asym_h = asym_kernel.size(2) asym_w = asym_kernel.size(3) square_h = square_kernel.size(2) square_w = square_kernel.size(3) # 水平卷积和竖直卷积分别在对应位置和 square con 的权重相加 square_kernel[:, :, square_h // 2 - asym_h // 2: square_h // 2 - asym_h // 2 + asym_h, square_w // 2 - asym_w // 2: square_w // 2 - asym_w // 2 + asym_w] += asym_kernel ACBlock 的 Pytorch 代码实现 作者开源了代码，将原始 $3\\times 3$ 卷积替换成 $3 \\times 3 + 3 \\times 1 + 1 \\times3$ 卷积的训练阶段基础结构 ACBlock 代码如下： import torch.nn as nn class CropLayer(nn.Module): \"\"\"# 去掉因为 3x3 卷积的 padding 多出来的行或者列 \"\"\" # E.g., (-1, 0) means this layer should crop the first and last rows of the feature map. And (0, -1) crops the first and last columns def __init__(self, crop_set): super(CropLayer, self).__init__() self.rows_to_crop = - crop_set[0] self.cols_to_crop = - crop_set[1] assert self.rows_to_crop >= 0 assert self.cols_to_crop >= 0 def forward(self, input): return input[:, :, self.rows_to_crop:-self.rows_to_crop, self.cols_to_crop:-self.cols_to_crop] class ACBlock(nn.Module): \"\"\"# ACNet 论文提出的 3x3+1x3+3x1 卷积结构 \"\"\" def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False): super(ACBlock, self).__init__() self.deploy = deploy if deploy: self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size,kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode) else: self.square_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=False, padding_mode=padding_mode) self.square_bn = nn.BatchNorm2d(num_features=out_channels) center_offset_from_origin_border = padding - kernel_size // 2 ver_pad_or_crop = (center_offset_from_origin_border + 1, center_offset_from_origin_border) hor_pad_or_crop = (center_offset_from_origin_border, center_offset_from_origin_border + 1) if center_offset_from_origin_border >= 0: self.ver_conv_crop_layer = nn.Identity() ver_conv_padding = ver_pad_or_crop self.hor_conv_crop_layer = nn.Identity() hor_conv_padding = hor_pad_or_crop else: self.ver_conv_crop_layer = CropLayer(crop_set=ver_pad_or_crop) ver_conv_padding = (0, 0) self.hor_conv_crop_layer = CropLayer(crop_set=hor_pad_or_crop) hor_conv_padding = (0, 0) self.ver_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 1), stride=stride, padding=ver_conv_padding, dilation=dilation, groups=groups, bias=False, padding_mode=padding_mode) self.hor_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1, 3), stride=stride, padding=hor_conv_padding, dilation=dilation, groups=groups, bias=False, padding_mode=padding_mode) self.ver_bn = nn.BatchNorm2d(num_features=out_channels) self.hor_bn = nn.BatchNorm2d(num_features=out_channels) def forward(self, input): if self.deploy: return self.fused_conv(input) else: square_outputs = self.square_conv(input) # 3x3 convolution square_outputs = self.square_bn(square_outputs) vertical_outputs = self.ver_conv_crop_layer(input) vertical_outputs = self.ver_conv(vertical_outputs) # 3x1 convolution vertical_outputs = self.ver_bn(vertical_outputs) horizontal_outputs = self.hor_conv_crop_layer(input) horizontal_outputs = self.hor_conv(horizontal_outputs) # 1x3 convolution horizontal_outputs = self.hor_bn(horizontal_outputs) # 3x3 卷积、1x3 卷积、3x1 卷积输出结果直接相加(+) return square_outputs + vertical_outputs + horizontal_outputs 摘要 论文的主要贡献在于： 提出了一种简单而强有力的 CNN 架构 RepVGG，相比 EfficientNet、RegNet 等架构，RepVGG 具有更佳的精度-速度均衡； 提出采用重参数化技术对 plain 架构进行训练-推理解耦； 在图像分类、语义分割等任务上验证了 RepVGG 的有效性。 RepVGG 模型定义 我们说的 VGG 式网络结构通常是指： 没有任何分支结构，即通常所说的 plain 或 feed-forward 架构。 仅使用 $3 \\times 3$ 类型的卷积。 仅使用 ReLU 作为激活函数。 VGG 式极简网络结构的五大优势： 3x3 卷积非常快。在 GPU 上，3x3 卷积的计算密度（理论运算量除以所用时间）可达 1x1 和 5x5 卷积的四倍。 单路架构非常快，因为并行度高。同样的计算量，“大而整”的运算效率远超“小而碎”的运算。已有研究表明：并行度高的模型要比并行度低的模型推理速度更快。 单路架构省内存。例如，ResNet 的 shortcut 虽然不占计算量，却增加了一倍的显存占用。 单路架构灵活性更好，容易改变各层的宽度（如剪枝）。 RepVGG 主体部分只有一种算子：3x3 卷积接 ReLU。在设计专用芯片时，给定芯片尺寸或造价，可以集成海量的 3x3 卷积-ReLU 计算单元来达到很高的效率，同时单路架构省内存的特性也可以帮我们少做存储单元。 RepVGG模型的基本架构简单来说就是：将 20 多层 $3 \\times 3$ 卷积层堆叠起来，分成 5 个 stage，每个 stage 的第一层是 stride=2 的降采样，每个卷积层用 ReLU 作为激活函数。 RepVGG Block 结构 模型结构的创新。 相比于多分支结构（如 ResNet、Inception、DenseNet等），近年来 Plain 式架构模型（VGG）鲜有关注，主要原因是因为性能差。有研究[1]认为 ResNet 性能好的一种解释是 ResNet 的分支结构（shortcut）产生了一个大量子模型的隐式 ensemble（因为每遇到一次分支，总的路径就变成两倍），单路直连架构显然不具备这种特点。 RepVGG 的设计是受 ResNet 启发得到，尽管多分支结构以对于推理不友好，但对于训练友好，本文作者提出一种新思想：训练一个多分支模型，推理时将多分支模型等价转换为单路模型。参考 ResNet 的 identity 与 $1 \\times 1$ 分支，设计了如下卷积模块： $$y = x + g(x) + f(x)$$ 其中，$x$, $g(x)$, $f(x)$ 分别对应恒等映射，$1 \\times 1$ 卷积，$3 \\times 3$ 卷积，即在训练时，为每一个 3x3 卷积层添加平行的 1x1 卷积分支和恒等映射分支，构成一个 RepVGG Block。这种设计是借鉴 ResNet 的做法，区别在于 ResNet 是每隔两层或三层加一分支，RepVGG 模型是每层都加两个分支（训练阶段）。 训练阶段，通过简单的堆叠上述 RepVGG Block 构建 RepVGG 模型；而在推理阶段，上述模块转换成 $y=h(x)$ 形式， $h(x)$ 的参数可以通过线性组合方式从训练好的模型中转换得到。 RepVGG Block 的结构重参数化 训练时使用多分支卷积结构，推理时将多分支结构进行融合转换成单路 $3 \\times 3$ 卷积层，由卷积的线性（具体说就是可加性）原理，每个 RepVGG Block 的三个分支可以合并为一个 $3 \\times 3$ 卷积层（等价转换），Figure 4 详细描绘了这一转换过程。 论文中使用 $W^{3} \\in \\mathbb{R}^{C2 \\times C_1 \\times 3 \\times 3}$ 表示卷积核 shape 为 $(C_2, C_1, 3, 3)$的卷积层，$W^{1} \\in \\mathbb {R}^{C{2} \\times C_{1}}$ 表示输入输出通道数为 $C_2$、$C_1$，卷积核为 $1 \\times 1$ 的卷积分支，采用 $\\mu^{(3)}, \\sigma^{(3)}, \\gamma^{(3)}, \\beta^{(3)}$ 表示 $3 \\times 3$ 卷积后的 BatchNorm 参数（平均值、标准差、比例因子、偏差），采用 $\\mu^{(1)}, \\sigma^{(1)}, \\gamma^{(1)}, \\beta^{(1)}$ 表示 $1 \\times 1$ 卷积分支后的 BatchNorm 参数，采用 $\\mu^{(0)}, \\sigma^{(0)}, \\gamma^{(0)}, \\beta^{(0)}$ 表示 identity 分支后的 BatchNorm 参数。假设 $M^{(1)} \\in \\mathbb{R}^{N \\times C_1 \\times H_1 \\times W_1}$, $M^{(2)} \\in \\mathbb{R}^{N \\times C_2 \\times H_2 \\times W_2}$ 分别表示输入输出矩阵，$\\ast $ 是卷积算子。当 $C_2 = C_1, H_1 = H_2, W_1 = W_2$ 时，有 $$ \\begin{split} M^{(2)} &= bn(M^{(1)} \\ast W^{(3)}, \\mu^{(3)}, \\sigma^{(3)}, \\gamma^{(3)}, \\beta^{(3)}) \\ &+ bn(M^{(1)} \\ast W^{(1)}, \\mu^{(1)}, \\sigma^{(1)}, \\gamma^{(1)}, \\beta^{(1)}) \\ &+ bn(M^{(1)}, \\mu^{(0)}, \\sigma^{(0)}, \\gamma^{(0)}, \\beta^{(0)}). \\end{split}\\tag{1} $$ 如果不考虑 identity 的分支，上述等式只有前面两部分。这里 bn 表示推理时 BN 计算函数，$1 \\leq i \\leq C_2$。bn 函数公式如下： $$ \\begin{split} bn(M, \\mu, \\sigma, \\gamma, \\beta) = (M_{:,i,:,:} - \\mu_i) \\frac{\\gamma_i}{\\sigma_i} + \\beta. \\end{split}\\tag{2} $$ 首先将每一个 BN 及其前面的卷积层转换成一个带有偏置向量的卷积（吸 BN），设 ${w^{'}, b^{'}}$ 表示 吸 BN 之后卷积层的卷积核和偏置向量参数，卷积层和 BN 合并后的卷积有如下公式： 推理时的卷积层和其后的 BN 层可以等价转换为一个带 bias 的卷积层（也就是通常所谓的“吸BN”），其原理参考深度学习推理时融合BN，轻松获得约5%的提速。 $$ \\begin{split} W{i,:,:,:}^{'} = \\frac{\\gamma_i}{\\sigma_i} W{i,:,:,:}, \\quad b{i}^{'} = -\\frac{\\mu{i} \\gammai}{\\sigma_i} + \\beta{i}. \\end{split}\\tag{3} $$ 很容易证明当 $1 \\leq i \\leq C_2$： $$ \\begin{split} bn(M \\ast W,\\mu,\\sigma,\\gamma,\\beta){:,i,:,:} = (M \\ast W^{'}){:,i,:,:} + b_{i}^{'}. \\end{split}\\tag{4} $$ 公式（4）同样适用于identity 分支，因为 identity 可以视作 kernel 为单位矩阵 $1\\times 1$ 卷积。至此，三个分支的卷积层和 BN 合并原理和公式已经叙述完毕，可以等效于下图 Figure 4 的第二步（吸收 BN 在前）。 最后一步是三个分支的的合并，也就是三个分支卷积层的融合，每个 RepVGG Block转换前后的输出是完全相同的，其原理参见作者的上一篇 ACNet 论文。通过前面的变换，可以知道 RepVGG Block 模块有一个 $3 \\times 3$ 卷积核，两个 $1 \\times 1$ 卷积核以及三个 bias 向量参数。通过简单的 add 方式合并三个 bias 向量可以得到融合后新卷积层的 bias。将 $1 \\times 1$ 卷积核用 0 填充 (pad) 成 $3 \\times 3$ 卷积核，然后和 $3 \\times 3$ 卷积核相加（elemen twise-add），得到融合后卷积层的 $3 \\times 3$ 卷积核。 至此三个分支的卷积层合并过程讲解完毕，可以等效于 Figure 4 的第三步。 卷积核细节：注意 $3 \\times 3$ 和 $1 \\times 1$ 卷积核拥有相同的 stride，后者的 padding 值比前者小于 1。 从上述这一转换过程中，可以理解结构重参数化的实质：训练时的结构对应一组参数，推理时我们想要的结构对应另一组参数；只要能把前者的参数等价转换为后者，就可以将前者的结构等价转换为后者。 结论 最后需要注明的是，RepVGG 是为 GPU 和专用硬件设计的高效模型，追求高速度、省内存，较少关注参数量和理论计算量。在低算力设备上，可能不如 MobileNet 和 ShuffleNet 系列适用。 RepVGG 的问题 RepVGG 的推理模型很难使用后量化方法 (Post-Training Quantization, PTQ)，比如，使用简单的 INT8 PTQ，ImageNet 上的 RepVGG 模型的准确性会降低到 54.55%。 RepOpt 对重参数化结构量化困难的问题进行了研究，发现重参数结构的分支融合和吸 BN 操作，显著放大了权重参数分布的标准差。而异常的权重分布又会产生了过大的网络激活层数值分布，从而进一步导致该层量化损失过大，因此模型精度损失严重。 参考美团结束团队文章-通用目标检测开源框架YOLOv6在美团的量化部署实战。 后续改进论文 RepOptimizer 中，直接量化 RepOpt-VGG 模型，ImageNet 上准确率仅会下降2.5%。 RepOptimizer：重参数化你的优化器：VGG 型架构 + 特定的优化器 = 快速模型训练 + 强悍性能。 参考资料 Residual Networks Behave Like Ensembles of Relatively Shallow Networks ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks RepVGG: Making VGG-style ConvNets Great Again https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%99%A3%E4%B9%98%E6%B3%95 RepVGG：极简架构，SOTA性能，让VGG式模型再次伟大 深度学习推理时融合BN，轻松获得约5%的提速 【CNN结构设计】无痛的涨点技巧：ACNet Markdown下LaTeX公式、编号、对齐 重参数化你的优化器：VGG 型架构 + 特定的优化器 = 快速模型训练 + 强悍性能 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"7-model_compression/轻量级网络论文解析/ShuffleNetv2论文详解.html":{"url":"7-model_compression/轻量级网络论文解析/ShuffleNetv2论文详解.html","title":"ShuffleNetv2论文详解","keywords":"","body":" 摘要 1、介绍 2、高效网络设计的实用指导思想 G1-同样大小的通道数可以最小化 MAC G2-分组数太多的卷积会增加 MAC G3-网络碎片化会降低并行度 G4-逐元素的操作不可忽视 3、ShuffleNet V2：一个高效的架构 4、实验 5、结论 6，个人思考 参考资料 近期在研究轻量级 backbone 网络，我们所熟悉和工业界能部署的网络有 MobileNet V2、ShuffleNet V2、RepVGG 等，本篇博客是对 ShuffleNet v2 论文的个人理解分析。本文的参考资料是自己对网络上资料进行查找和筛选出来的，质量相对较高、且对本文有参考意义的文章。ShuffleNet v2 论文最大的贡献在于看到了 GPU 访存带宽（内存访问代价 MAC）对于模型推理时间的影响，而不仅仅是模型复杂度，也就是 FLOPs 和参数量 Params 对于推理时间的影响，并由此提出了 4 个轻量级网络设计的原则和一个新颖的 卷积 block 架构-ShuffleNet v2。 摘要 当前，神经网络结构的设计基本由间接的计算复杂度主导，例如 FLOPs，但是直接的度量如速度，还取决于其他因素，例如内存的获取损耗和平台特性。因此，我们将使用直接的标准衡量，而不仅仅是 FLOPs。因此本文建议直接在目标平台上用直接度量进行测试。基于一系列控制条件实验，作者提出了设计高效网络结构的一些实用指导思想，并据此提出了一个称之为 ShuffleNet V2 的新结构。综合的对比实验证明了作者的模型在速度和准确性上取得了最佳的平衡（state-of-the-art）。 1、介绍 为了衡量计算复杂度，一个广泛采用的度量方式是浮点运算的次数 FLOPs，但是，它是一个间接的度量，是对我们真正关心的直接度量比如速度或者时延的一种近似估计。在以前的工作中，这种不一致已经被学者们所发现，比如 MobileNet v2 要比 NASNET-A 快很多，但是它们两者具有差不多的 FLOPs。 图 1 中在 GPU 和 ARM 两个平台上，具有相同 FLOPs 的模型运行速度也会相差很多。因此只用 FLOPs 来衡量计算复杂度是不充分的，也会导致得不到最优的网络设计。 导致这种不一致的主要有两个原因：一是影响速度的几个重要因素只通过 FLOPs 是考虑不到的，比如 MAC（Memory Access Cost）和并行度；二是具有相同 FLOPs 的模型在不同的平台上可能运行速度不一样。 因此，作者提出了设计有效网络结构的两个原则。一是用直接度量来衡量模型的性能，二是直接在目标平台上进行测试。 2、高效网络设计的实用指导思想 首先，作者分析了两个经典结构 ShuffleNet v1 和 MobileNet v2 的运行时间。ARM 和 GPU 平台的具体运行环境如下图所示。 从图 2 可以看出，虽然以 FLOPs 度量的卷积占据了大部分的时间，但其余操作也消耗了很多运行时间，比如数据输入输出、通道打乱和逐元素的一些操作（张量相加、激活函数）。因此，FLOPs 不是实际运行时间的一个准确估计。 G1：同样大小的通道数可以最小化 MAC。 G2：太多的分组卷积会增加 MAC。 G3：网络碎片化会减少并行度。 G4：逐元素的操作不可忽视。 G1-同样大小的通道数可以最小化 MAC 现代的网络如 Xception [12], MobileNet [13], MobileNet V2 [14], ShuffleNet [15] 都采用了深度可分离卷积，它的点卷积（即 $1\\times 1$ 卷积）占据了大部分的计算复杂度（ShuffleNet 有分析）。假设输入特征图大小为 $hwc1$，那么卷积核 shape 为 $(c_2, c_1, 1, 1)$，输出特征图长宽不变，那么 $1 \\times 1$ 卷积的 FLOPs 为 $B = hwc{1}c_{2}$。 论文中 FLOPs 的计算是把乘加当作一次浮点运算的，所以其实等效于我们通常理解的 MACs 计算公式。 简单起见，我们假设计算设备的缓冲足够大能够存放下整个特征图和参数。那么 $1 \\times 1$ 卷积层的内存访问代价（内存访问次数）为 $MAC = hwc1 + hwc_2 + c{1}c{2} = hw(c{1} + c{2}) + c{1}c_{2}$，等式的三项分别代表输入特征图、输出特征图和权重参数的代价。由均值不等式，我们有： $$ \\begin{split} MAC &= hw(c{1} + c{2}) + c{1}c{2} \\\\ &= \\sqrt{(hw)^{2}(c{1} + c{2})^{2}} + \\frac{B}{hw} \\\\ &\\geq \\sqrt{(hw)^{2}(4c{1}c_{2})}+ \\frac{B}{hw} \\\\ &\\geq 2\\sqrt{hwB} + \\frac{B}{hw} \\\\ \\end{split}$$ 由均值不等式，可知当 $c1 = c_2$ 时，$(c{1} + c{2})^{2} = 4c{1}c{2}$，即式子 $(c{1} + c{2})^{2}$ 取下限。即当且仅当 $c{1}=c{2}$ （$1 \\times 1$ 卷积输入输出通道数相等）时，MAC 取得最小值。但是这个结论只是理论上成立的，实际中缓存容量可能不够大，缓存策略也因平台各异。所以我们进一步设计了一个对比试验来验证，实验的基准的网络由 10 个卷积 block 组成，每个块有两层卷积，第一个卷积层输入通道数为 $c{1}$ 输出通道数为$c{2}$，第二层与第一层相反，然后固定总的 FLOPs 调整$c{1}:c_{2}$的值测试实际的运行速度，结果如表 1 所示： 可以看到，当比值接近 1:1 的时候，网络的 MAC 更小，测试速度也最快。 G2-分组数太多的卷积会增加 MAC 分组卷积是现在轻量级网络结构（ShuffleNet/MobileNet/Xception/ResNeXt）设计的核心，它通过通道之间的稀疏连接（也就是只和同一个组内的特征连接）来降低计算复杂度。一方面，它允许我们使用更多的通道数来增加网络容量进而提升准确率，但另一方面随着通道数的增多也对带来更多的 MAC。 针对 $1 \\times 1$ 的分组卷积，我们有： 分组卷积 FLOPs 的计算公式，我写的 MobileNet v1 论文详解 有给出推导。 $$ \\begin{split} B = h \\ast w \\ast 1 \\ast 1 \\ast \\frac{c1}{g} \\ast \\frac{c_2}{g} \\ast g = \\frac{hwc{1}c_{2}}{g} \\end{split} $$ $$ \\begin{split} MAC = hw(c{1} + c{2}) + \\frac{c{1}c{2}}{g} = hwc_{1} + \\frac{Bg}{c_1}+\\frac{B}{hw}\\end{split} $$ 固定 $\\frac{c2}{g}$ 的比值，又因为输入特征图 $c{1} \\times h \\times w$ 固定，从而也就固定了计算代价 $B$，所以可得 上式中 $MAC$ 与 $g$ 成正比的关系。 其中 $B$ 是卷积层的浮点运算次数（FLOPs），$g$ 是分组卷积的组数，可以看到，如果给定输入特征图尺寸（shape）$c_{1} \\times h \\times w$ 和计算代价 $B$，则 $MAC$ 与组数 $g$ 成正比。本文通过叠加 10 个分组点卷积层设计了实验，在保证计算代价（FLOPs）相同的情况下采用不同的分组组数测试模型的运行时间，结果如下表 2 所示。 很明显使用分组数多的网络速度更慢，比如 分为 8 个组要比 1 个组慢得多，主要原因在于 MAC 的增加 。因此，本文建议要根据硬件平台和目标任务谨慎地选择分组卷积的组数，不能简单地因为可以提升准确率就选择很大的组数，而忽视了内存访问代价（MAC）的增加。 G3-网络碎片化会降低并行度 在 GoogLeNet 系列和自动搜索得到的网络架构中，每个网络的 block 都采用多分支（multi-path）结构，在这种结构中多采用小的算子（fragmented operators 支路算子/碎片算子）而不是大的算子，block 中的每一个卷积或者池化操作称之为一个 fragmented operator。如 NASNET-A[9]网络的碎片算子的数量（即一个 building block 的单个卷积或池化操作的总数）为 13。相反，在 ResNet[4] 这样的标准网络中，碎片算子的数量为 2 或者 3。 Residual Block 有两种，basic block 和 bottleneck block 的残差结构。fragment，翻译过来就是分裂的意思，可以简单理解为网络的单元或者支路数量。 尽管过去的论文已经表明，这种 fragmented structure（碎片化/支路结构）能够提升模型的准确性，但是其会降低效率，因为这种结构 GPU 对并行性强的设备不友好。而且它还引入了额外的开销，如内核启动和同步。 kernel launching and synchronization. synchronization：同步支路结构分支之间的同步。network fragmentation 我翻译为网络碎片化。 为了量化网络碎片化（network fragmentation）如何影响效率，我们评估了一系列具有不同碎片化程度(degree of fragmentation)的网络块（network blocks）。具体来说，对比实验实验的每个构建块由 1 到 4 个 顺序或并行结构的1x1 卷积层组成。The block structure 如附录图1所示。 每个 block 重复堆叠 10 次。表 3 的结果表明，碎片化会降低 GPU 的速度，例如 4-fragment 比 1-fragment 结构慢约 3 倍。但是在 ARM 上，fragmentation 对速度的影响速会比 GPU 相对较小些。 G4-逐元素的操作不可忽视 如图 2 所示，例如 MobileNet v2 和 ShuffleNet v1 这样的轻量级模型中，按元素操作（element-wise）会占用大量时间的，尤其是在 GPU 平台上。 在我们的论文中，逐元素算子包括 ReLU、AddTensor、AddBias 等，它们的 FLOPs 相对较小，但是 MAC 较大。特别地，我们把 depthwise convolution 当作一个 逐元素算子（element-wise operator），因为它的 MAC/FLOPs 的比值也较高。 shortcut 操作并不能当作 element-wise 算子。 论文使用 ResNet 的 \"bottleneck\" 单元进行实验，其是由 $1 \\times 1$ 卷积、然后是$3 \\times 3$ 卷积，最后又是 $1 \\times 1$ 卷积组成，并带有 ReLU 和 shortcut 连接，其结构图如下图所示。在论文的实验中，删除 ReLU 和 shortcut 操作，表 4 报告了不同变体 \"bottleneck\" 的运行时间。我们观察到，在删除 ReLU 和 shortcut 后，在 GPU 和 CPU 平台都取得了 20% 的加速。 结论和讨论。根据上诉 4 个指导原则和经验研究，我们得出高效的网络结构应该满足： 使用平衡的卷积，也就是通道数一样； 合理使用分组卷积； 减少碎片度； 减少逐元素操作。 以上 4 个理想的属性的发挥效果取决于平台特性（例如内存操作和代码优化），这超出了论文理论上的范围，但在实际网络设计中我们应尽量遵守这些原则。 之前轻量级神经网络体系结构的进展主要是基于 FLOPs 的度量标准，并没有考虑上述 4 个属性。比如 ShuffleNet v1 严重依赖分组卷积，这违反了 G2；MobileNet v2 利用了反转瓶颈结构，这违反了 G1，而且在通道数较多的扩展层使用 ReLU 和深度卷积，违反了 G4，NAS 网络生成的结构碎片化很严重，这违反了 G3。 3、ShuffleNet V2：一个高效的架构 重新审查 ShuffleNet v1。ShuffleNet 是一个 state-of-the-art 网络，被广泛应用于低端设备中（如手机）。它启发了我们论文中的工作，因此，它首先被审查和分析。 根据 ShuffleNet v1，轻量级神经网络的主要挑战在于，在给定预算（FLOPs）的情况下，特征图的通道数也是受限制的。为了在不显著增加 FLOPs 计算量的情况下提高通道数，ShuffleNet v1 论文采用了两种技术：逐点组卷积和类瓶颈结构（pointwise group convolutions and bottleneck-like structures.）；然后引入“channel shuﬄe” 操作，令不同组的通道之间能够进行信息交流，提高精度。其构建模块如图 3(a)(b) 所示。 从本文 Section 2 的讨论，可以知道逐点组卷积和瓶颈结构都增加了 MAC( G2 和 G1 )。这个成本不可忽视，特别是对于轻量级模型。另外，使用太多分组也违背了 G3。shortcut connection 中的逐元素加法（element-wise \"Add\"）操作也不可取 (G4)。因此，为了实现较高的模型容量和效率，关键问题是如何保持大量且同样宽的通道，同时没有密集卷积也没有太多的分组。 如何保持大量且同样宽的通道，同时没有密集卷积也没有太多的分组，这句话比较难理解。我的理解：1，卷积 block 里面的卷积层通道多且同样宽的通道的，意味着两个连接的卷积层的通道数要多且相等。2，这里密集卷积是指 $1\\times 1$ 卷积。3，使用分组卷积时，分组数 group 不宜过多，那就意味着 DW 卷积的输入通道数要较小。 ShuffleNet v2 的通道拆分。在 ShuffleNet v1 block的基础上，ShuffleNet v2 block 引入通道分割（Channel Split）这个简单的算子来实现上述目的，如图 3(c) 所示。在每个单元 (block) 的开始，我们将输入特征图的 $c$ 个通道切分成 (split) 两个分支 (branches)：$c-c^{'}$ 个通道和 $c^{'}$ 个通道。根据 G3 网络碎片尽可能少，其中一个分支保持不变（shortcut connection），另外一个分支包含三个通道数一样的卷积来满足 G1。和 v1 不同，v2 block 的两个 $1 \\times 1$ 卷积不再使用分组卷积，一部分原因是为了满足 G2，另外一部分原因是一开始的通道切分 （split）操作已经完成了分组效果。 最后，对两个分支的结果进行拼接（concatnate），这样对于卷积 block 来说，输入输出通道数是一样的，符合 G1 原则。和 ShuffleNet v1 一样都使用通道打乱（channel shuffle）操作来保证两个分支的信息进行交互。 ResNet 的 basic block 和 bottleneck block 也是这样设计的，符合 G1 原则。 通道打乱之后的输出，就是下一个单元的输入。ShuffleNet v1 的 “Add” 操作不再使用，逐元素操作算子如：ReLU 和 DW 卷积 只存在于在右边的分支。与此同时，我们将三个连续的逐元素操作算子：拼接（“Concat”）、通道打乱（“Channel Shuffle”）和通道拆分（“Channel Split”）合并成一个逐元素算子。根据 G4原则，这些改变是有利的。 针对需要进行空间下采样的 block，卷积单元（block）进行了修改，通道切分算子被移除，然后 block 的输出通道数变为两倍，详细信息如图 3(d) 所示。 图 3(c)(d) 显示的卷积 block叠加起来即组成了最后的 ShuffleNet v2 模型，简单起见，设置 $c^{'} = c/2$，这样堆叠后的网络是类似 ShuffleNet v1 模型的，网络结构详细信息如表 5 所示。v1 和 v2 block 的区别在于， v2 在全局平均池化层（global averaged pooling）之前添加了一个 $1 \\times 1$ 卷积来混合特征（mix up features），而 v1 没有。和 v1 一样，v2 的 block 的通道数是按照 0.5x 1x 等比例进行缩放，以生成不同复杂度的 ShuffleNet v2 网络，并标记为 ShuffleNet v2 0.5×、ShuffleNet v2 1× 等模型。 注意：表 5 的通道数设计是为了控制 FLOPs，需要调整通道数将 FLOPs 与之前工作对齐从而使得对比实验公平，没有使用 2^n 通道数是因为其与精度无关。 根据前文的分析，我们可以得出此架构遵循所有原则，因此非常高效。 网络精度的分析。ShuffleNet v2 不仅高效而且精度也高。有两个主要理由：一是高效的卷积 block 结构允许我们使用更多的特征通道数，网络容量较大。二是当 $c^{'} = c/2$时，一半的特征图直接经过当前卷积 block 并进入下一个卷积 block，这类似于 DenseNet 和 CondenseNet 的特征重复利用。 DenseNet 是一种具有密集连接的卷积神经网络。在该网络中，任何两层之间都有直接的连接，也就是说，网络每一层的输入都是前面所有层输出的并集，而该层所学习的特征图也会被直接传给其后面所有层作为输入。 在 DenseNet 论文中，作者通过画不同权重的 L1 范数值来分析特征重复利用的模式，如图 4(a)所示。可以看到，相邻层之间的关联性是远远大于其它层的，这也就是说所有层之间的密集连接可能是多余的，最近的论文 CondenseNet 也支持这个观点。 在 ShuffleNet V2 中，很容易证明，第 $i$ 层和第 $i+j$ 层之间直接相连的特征图通道数为 $r^{j}c$，其中 $r=(1−c^{'})/c$。换句话说，两个 blocks 之间特征复用的数量是随着两个块之间的距离变大而呈指数级衰减的。相距远的 blocks，特征重用会变得很微弱。 图 4 的两个 blocks 之间关联性的理解有些难。 因此，和 DenseNet 一样，Shufflenet v2 的结构通过设计实现了特征重用模式，从而得到高精度，并具有更高的效率，在实验中已经证明了这点，实验结果如表 8 所示 。 4、实验 精度与 FLOPs 的关系。很明显，我们提出的 ShuffleNet v2 模型很大程度上优于其他网络，特别是在小的计算量预算情况下。此外，我们也注意到 MobileNet v2 模型在 图像尺寸为$224 \\times 224$ 和模型计算量为 40 MFLOPs 量级时表现不佳，这可能是因为通道数太少的原因。相比之下，我们的设计的高效模型可以使用更多的通道数，所以并不具备此缺点。另外，如 Section 3 讨论的那样，虽然我们的模型和 DenseNet 都具有特征重用功能，但是我们的模型效率更高。 推理速度和 FLOPs/Accuracy 的关系。本文比较了 ShuffleNet v2、MobileNet v2、ShuffleNet v1 和 Xception 四种模型的实际推理速度和 FLOPs的关系，如图 1(c)(d) 所示，在不同分辨率条件下的更多结果在附录表 1 中提供。 尽管 MobileNet v1 的精度表现不佳，但是其速度快过了 SHuffleNet v2等网络。我们认为是因为 MobileNet v1 符合本文建议的原则（比如 G3 原则， MobileNet v1 的碎片化程度少于 ShuffleNet v2）。 与其他方法的结合。ShuffleNet v2 与其他方法结合可以进一步提高性能。当使用 SE 模块时，模型会损失一定的速度，但分类的精度会提升 0.5%。卷积 block 的结构如附录 2(b)所示，对比实验结果在表 8 中。 Sequeeze-and-Excitation(SE) block 不是一个完整的网络结构，而是一个子结构（卷积 block），通用性较强、即插即用，可以嵌到其他分类或检测模型中，和 ResNext、ShuffleNet v2 等模型结合。SENet 主要是学习了 channel 之间的相关性，筛选出了针对通道的注意力，稍微增加了一点计算量，但是效果比较好。SE 其 block 结构图如下图所示。 大型模型通用化（Generation to Large Models）。虽然本文的消融（ablation）实验主要是针对轻量级网络，但是 ShuffleNet v2 在大型模型($FLOPs \\geq 2G$)的表现上也丝毫不逊色。表6 比较了50 层的 ShuffleNet v2、ShuffleNet v1 和 ResNet50 在 ImageNet 分类实验上的精度，可以看出同等 FLOPs=2.3G 条件下 ShuffleNet v2 比 v1 的精度更高，同时和 ResNet50 相比 FLOPs 减少 40%，但是精度表现更好。实验用的网络细节参考附录表2。 对于很深的 ShuffleNet v2 模型（例如超过 100 层），我们通过添加一个 residual path 来轻微的修改基本的 block 结构，来使得模型的训练收敛更快。表 6 提供了 带 SE 模块的 164 层的 ShuffleNet v2 模型，其精度比当前最高精度的 state-of-the-art 模型 SENet 精度更高，同时 FLOPs 更少。 目标检测任务评估。为了评估模型的泛化性能，我们使用 Light-Head RCNN 作为目标检测的框架，在 COCO 数据集上做了对比实验。表 7 的实验结果表明模型在 4 种不同复杂度条件下， ShuffleNet v2 做 backbone 的模型精度比其他网络更高、速度更快，全面超越其他网络。 Table 7: Performance on COCO object detection. The input image size is 800 1200. FLOPs row lists the complexity levels at 224 224 input size. For GPU speed evaluation, the batch size is 4. We do not test ARM because the PSRoI Pooling operation needed in [34] is unavailable on ARM currently. 比较检测任务的结果（表 7），发现精度上 ShuffleNet v2 > Xception ≥ ShuffleNet v1 ≥ MobileNet v2。 比较分类任务的结果（表 8），精度等级上 ShuffleNet v2 ≥ MobileNet v2 > ShuffeNet v1 > Xception。 5、结论 我们建议对于轻量级网络设计应该考虑直接 metric（例如速度 speed），而不是间接 metric（例如 FLOPs）。本文提出了实用的原则和一个新的网络架构-ShuffleNet v2。综合实验证了我们模型的有效性。我们希望本文的工作可以启发未来的网络架构设计可以更重视平台特性和实用性。 这里的直接 metric，可以是inference time or latency，也可以是模型推理速度 speed，其意义都是一样的。 6，个人思考 分析模型的推理性能得结合具体的推理平台（常见如：英伟达 GPU、移动端 ARM CPU、端侧 NPU 芯片等），目前已知影响推理性能的因素包括: 算子计算量 FLOPs（参数量 Params）、算子内存访问代价（访存带宽）。但相同硬件平台、相同网络架构条件下， FLOPs 加速比与推理时间加速比成正比。 举例：对于 GPU 平台，Depthwise 卷积算子实际上是使用了大量的低 FLOPs、高数据读写量的操作。这些具有高数据读写量的操作，加上 GPU 的访存带宽限制，使得模型把大量的时间浪费在了从显存中读写数据上，导致 GPU 的算力没有得到“充分利用”。结论来源知乎文章-FLOPs与模型推理速度。 最后，目前 AI 训练系统已经有了一个公认的评价标准和平台-MLPerf，但是 AI 推理系统的评估，目前还没有一个公认的评价指标。Training 系统的性能可以使用“达到特定精度的时间”这个简单的标准来衡量。但 Inference 系统却很难找到一个简单的指标，Latency，Throughput，Power，Cost 等等，哪个指标合适呢？目前没有统一的标准。 参考资料 Group Convolution分组卷积，以及Depthwise Convolution和Global Depthwise Convolution 分组卷积和深度可分离卷积 理解分组卷积和深度可分离卷积如何降低参数量 深度可分离卷积（Xception 与 MobileNet 的点滴） MobileNetV1代码实现 轻量级神经网络：ShuffleNetV2解读 ShufflenetV2_高效网络的4条实用准则 ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices MobileNetV2: Inverted Residuals and Linear Bottlenecks MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications Squeeze-and-Excitation Networks Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"7-model_compression/轻量级网络论文解析/VoVNet论文解读.html":{"url":"7-model_compression/轻量级网络论文解析/VoVNet论文解读.html","title":"VoVNet论文解读","keywords":"","body":" 摘要 1，介绍 2，高效网络设计的影响因素 2.1，内存访问代价 2.2，GPU计算效率 3，建议的方法 3.1，重新思考密集连接 3.2，One-Shot Aggregation 3.3，构建 VoVNet 网络 4，实验 5，代码解读 参考资料 文章同步发于 github、博客园 和 知乎。最新版以 github 为主。如果看完文章有所收获，一定要先点赞后收藏。毕竟，赠人玫瑰，手有余香。 摘要 Youngwan Lee* 作者于 2019 年发表的论文 An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection. 是对 DenseNet 网络推理效率低的改进版本。 因为 DenseNet 通过用密集连接，来聚合具有不同感受野大小的中间特征，因此它在对象检测任务上表现出良好的性能。虽然特征重用（feature reuse）的使用，让 DenseNet 以少量模型参数和 FLOPs，也能输出有力的特征，但是使用 DenseNet 作为 backbone 的目标检测器却表现出了运行速度慢和效率低下的弊端。作者认为是密集连接(dense connection)带来的输入通道线性增长，从而导高内存访问成本和能耗。为了提高 DenseNet 的效率，作者提出一个新的更高效的网络 VoVet，由 OSA（One-Shot Aggregation，一次聚合）组成。OSA 仅在模块的最后一层聚合前面所有层的特征，这种结构不仅继承了 DenseNet 的多感受野表示多种特征的优点，也解决了密集连接效率低下的问题。基于 VoVNet 的检测器不仅速度比 DenseNet 快 2 倍，能耗也降低了 1.5-4.1 倍。另外，VoVNet 网络的速度和效率还优于 ResNet，并且其对于小目标检测的性能有了显著提高。 1，介绍 随着 CNN 模型：VGG、ResNet 和 DensNet 的巨大进步，它们开始被广泛用作目标检测器的 backbone，用来提取图像特征。 ResNet 和 DenseNet 主要的区别在于它们聚合特征的方式，ResNet 是通过逐元素相加（element-wise add）和前面特征聚合，DenseNet 则是通过拼接（concatenation）的方式。Zhu 等人在论文32 中认为前面的特征图携带的信息将在与其他特征图相加时被清除。换句话说，通过 concatenation 的方式，早期的特征才能传递下去，因为它保留了特征的原始形式（没有改变特征本身）。 最近的一些工作 [25, 17, 13] 表明具有多个感受野的抽象特征可以捕捉各种尺度的视觉信息。因为检测任务比分类更加需要多样化尺度去识别对象，因此保留来自各个层的信息对于检测尤为重要，因为网络每一层都有不同的感受野。因此，在目标检测任务上，DenseNet 比 ResNet 有更好更多样化的特征表示。 这是不是说明对于，多标签分类问题，用 VoVNet 作为 backbone，效果要比 ResNet 要好。因为前者可以实现多感受野表示特征。 尽管使用 DenseNet 的检测器的参数量和 FLOPs 都比 ResNet 小，但是前者的能耗能耗和速度却更慢。这是因为，还有其他因素 FLOPs 和模型尺寸（参数量）影响能耗。 首先，内存访问代价 MAC 是影响能耗的关键因素。如图 1(a) 所示，因为 DenseNet 中的所有特征图都被密集连接用作后续层的输入，因此内存访问成本与网络深度成二次方增加，从而导致计算开销和更多的能耗。 从图 (a) 中可以看出，DenseBlock 中的每一层的输入都是前面所有层 feature map 的叠加。而图 (b)只有最后一层的输入是前面所有层 feature map 的叠加。 其次，关于 GPU 的并行计算，DenseNet 有计算瓶颈的限制。一般来说，当操作的张量更大时，GPU 的并行利用率会更高[19,29,13]。 然而，由于为了线性增加输入通道，需要 DenseNet 采用 1×1 卷积 bottleneck 架构来减少输入维度和 FLOPs，这导致使用较小的操作数张量增加层数。作为结果就是 GPU 计算变得低效。总结就是，bottleneck 结构中的 $1\\times 1$ 卷积会导致 GPU 并行利用率。 本文的目的在于将 DenseNet 改进的更高效，同时，还保留对目标检测有益的连接聚合（concatenative aggregation）操作。 作者认为 DenseNet 网络 DenseBlock 中间层的密集连接（dense connections）会导致网络效率低下，并假设相应的密集连接是多余的。 作者使用 OSA 模块构建了 VoVNet 网络，为了验证有效性，将其作为 DSOD、RefineDet 和 Mask R-CNN 的 backbone 来做对比实验。实验结果表明，基于 VoVNet 的检测器优于 DenseNet 和 ResNet，速度和能耗都更优。 2，高效网络设计的影响因素 作者认为，MobileNet v1 [8], MobileNet v2 [21], ShuffleNet v1 [31], ShuffleNet v2 [18], and Pelee 模型主要是通过使用 DW 卷积和 带 $1\\times 1$ 卷积的 bottleneck 结构来减少 FLOPs 和模型尺寸（参数量）。 这里我觉得作者表达不严谨，因为 shufflenetv2 在论文中已经声明过，FLOPs 和模型参数量不是模型运行速度的唯一决定因素。 实际上，减少 FLOPs 和模型大小并不总能保证减少 GPU 推理时间和实际能耗，典型的例子就是 DenseNet 和 ResNet 的对比，还有就是在 GPU 平台上， Shufflenetv2 在同等参数条件下，运行速度比 MobileNetv2 更快。这些现象告诉我们，FLOPs 和 模型尺寸（参数）是衡量模型实用性（practicality）的间接指标。为了设计更高效的网络，我们需要使用直接指标 FPS，除了上面说的 FLOPs 和模型参数量会影响模型的运行速度（FPS），还有以下几个因素。 2.1，内存访问代价 这个 Shufflenetv2 作者已经解释得很清楚了，本文的作者的描述基本和 Shufflenetv2 一致。我这里直接给结论： MAC 对能耗的影响超过了计算量 FLOPs [28]。 卷积层输入输出通道数相等时，MAC 取得最小值。 即使模型参数量一致，只要 MAC 不同，那么模型的运行时间也是不一致的(ShuffleNetv2 有实验证明)。 论文 [28] Designing energy-efficient convolutional neural networks using energyaware pruning. 2.2，GPU计算效率 其实这个内容和 shufflenetv2 论文中的 G3 原则（网络碎片化会降低 GPU 并行度）基本一致。 为提高速度而降低 FLOPs 的网络架构基于这样一种理念，即设备中的每个浮点运算都以相同的速度进行处理。但是，当模型部署在 GPU 上时，不是这样的，因为 GPU 是并行处理机制能同时处理多个浮点运算进程。我们用 GPU 计算效率来表示 GPU 的运算能力。 通过减少 FLOPs 是来加速的前提是，设备中的每个浮点运算都以相同的速度进行处理； GPU 特性： 擅长 parallel computation，tensor 越大，GPU 使用效率越高。 把大的卷积操作拆分成碎片的小操作将不利于 GPU 计算。 因此，设计 layer 数量少的网络是更好的选择。MobileNet使用额外的 1x1 卷积来减少计算量，不过这不利于 GPU 计算。 为了衡量 GPU 利用率，引入有一个新指标：$FLOP/s = \\frac{FLOPs}{GPU\\ inference\\ time}$（每秒完成的计算量 FLOPs per Second），FLOP/s 高，则 GPU 利用率率也高。 3，建议的方法 3.1，重新思考密集连接 1，DenseNet 的优点： 在计算第 $l$ 层的输出时，要用到之前所有层的输出的 concat 的结果。这种密集的连接使得各个层的各个尺度的特征都能被提取，供后面的网络使用。这也是它能得到比较高的精度的原因，而且密集的连接更有利于梯度的回传（ResNet shorcut 操作的加强版）。 2，DenseNet 缺点（导致了能耗和推理效率低的）： 密集连接会增加输入通道大小，但输出通道大小保持不变，导致的输入和输出通道数都不相等。因此，DenseNet 具有具有较高的 MAC。 DenseNet 采用了 bottleneck 结构，这种结构将一个 $3\\times 3$ 卷积分成了两个计算（1x1+3x3 卷积），这带来了更多的序列计算（sequential computations），导致会降低推理速度。 密集连接会导致计算量增加，所以不得不采用 $1\\times 1$ 卷积的 bottleneck 结构。 图 7 的第 1 行是 DenseNet 各个卷积层之间的相互关系的大小。第 $(s,l)$ 块代表第 $s$ 层和第 $l$ 层之间这个卷积权值的平均 $L_1$ 范数（按特征图数量归一化后的 L1 范数）的大小，也就相当于是表征 $X_s$ 和 $X_l$ 之间的关系。 图 2. 训练后的 DenseNet(顶部) 和 VoVNet(中间和底部) 中卷积层的滤波器权重的绝对值的平均值。像素块的颜色表示的是相互连接的网络层(i, j)的权重的平均 $L_1$ 范数（按特征图数量归一化后的 L1 范数）的值。OSA Module (x/y) 指的是 OSA 模块由 $x$ 层和 $y$ 个通道组成。 如图 2 顶部图所示， Hu 等人[9]通过评估每层输入权重归一化后的 L1 范数来说明密集连接的连通性（connectivity），这些值显示了前面所有层对相应层的归一化影响，1 表示影响最大，0 表示没有影响（两个层之间的权重没有关系）。 这里重点解释下连通性的理解。两层之间的输入权重的绝对值相差越大，即 L1 越大，那么说明卷积核的权重越不一样，前面层对后面层影响越大（connectivity），即连通性越好（大）。从实用性角度讲，我们肯定希望相互连接的网络层的连通性越大越好（归一化后是 0~1 范围），这样我的密集连接才起作用了嘛。不然，耗费了计算量、牺牲了效率，但是连通性结果又差，那我还有必要设计成密集连接（dense connection）。作者通过图 2 后面的两张图也证明了DenseBlock 模块中各个层之间的联系大部分都是没用，只有少部分是有用的，即密集连接中大部分网络层的连接是无效的。 在 Dense Block3 中，对角线附近的红色框表示中间层（intermediate layers）上的聚合处于活动状态，但是分类层（classification layer）只使用了一小部分中间特征。 相比之下，在 Dense Block1 中，过渡层（transition layer）很好地聚合了其大部分输入特征，而中间层则没有。 Dense Block3 的分类层和 Dense Block1 的过渡层都是模块的最后一层。 通过前面的观察，我们先假设中间层的聚集强度和最后一层的聚集强度之间存在负相关（中间层特征层的聚合能力越好，那么最后层的聚合能力就越弱）。如果中间层之间的密集连接导致了每一层的特征之间存在相关性，则密集连接会使后面的中间层产生更好的特征的同时与前一层的特征相似，则假设成立。在这种情况下，因为这两种特征代表冗余信息，所以最后一层不需要学习聚合它们，从而前中间层对最终层的影响变小。 因为最后一层的特征都是通过聚集（aggregated）所有中间层的特征而产生的，所以，我们当然希望中间层的这些特征能够互补或者相关性越低越好。因此，进一步提出假设，相比于造成的损耗，中间特征层的 dense connection 产生的作用有限。为了验证假设，我们重新设计了一个新的模块 OSA，该模块仅在最后一层聚合块中其他层的特征（intermediate features），把中间的密集连接都去掉。 3.2，One-Shot Aggregation 为了验证我们的假设，中间层的聚合强度和最后一层的聚合强度之间存在负相关，并且密集连接是多余的，我们与 Hu 等人进行了相同的实验，实验结果是图 2 中间和底部位置的两张图。 从图 2（中间）可以观察到，随着中间层上的密集连接被剪掉，最终层中的聚合变得更加强烈。同时，蓝色的部分 (联系大部分不紧密的部分) 明显减少了很多，也就是说 OSA 模块的每个连接都是相对有用的。 从图 2（底部）的可以观察到，OSA 模块的过渡层的权重显示出与 DenseNet 不同的模式：来自浅层的特征更多地聚集在过渡层上。由于来自深层的特征对过渡层的影响不大，我们可以在没有显着影响的情况下减少 OSA 模块的层数，得到。令人惊讶的是，使用此模块（5 层网络），我们实现了 5.44% 的错误率，与 DenseNet-40 （模块里有 12 层网络）的错误率（5.24%）相似。这意味着通过密集连接构建深度中间特征的效果不如预期（This implies that building deep intermediate feature via dense connection is less effective than expected）。 One-Shot Aggregation（只聚集一次）是指 OSA 模块的 concat 操作只进行一次，即只有最后一层的输入是前面所有层 feature map 的 concat（叠加）。OSA 模块的结构图如图 1(b) 所示。 在 OSA 模块中，每一层产生两种连接，一种是通过 conv 和下一层连接，产生 receptive field 更大的 feature map，另一种是和最后的输出层相连，以聚合足够好的特征。 为了验证 OSA 模块的有效性，作者使用 dense block 和 OSA 模块构成 DenseNet-40网络，使两种模型参数量一致，做对比实验。OSA 模板版本在 CIFAR-10 数据集上的精度达到了 93.6，和 dense block 版本相比，只下降了 1.2%。再根据 MAC 的公式，可知 MAC 从 3.7M 减少为 2.5M。MAC 的降低是因为 OSA 中的中间层具有相同大小的输入输出通道数，从而使得 MAC 可以取最小值（lower boundary）。 因为 OSA 模块中间层的输入输出通道数一致，所以没必要使用 bottleneck 结构，这又进一步提高了 GPU 利用率。 3.3，构建 VoVNet 网络 因为 OSA 模块的多样化特征表示和效率，所以可以通过仅堆叠几个模块来构建精度高、速度快的 VoVNet 网络。基于图 2 中浅层深度更容易聚合的认识，作者认为可以配置比 DenseNet 具有更大通道数的但更少卷积层的 OSA 模块。 如下图所示，分别构建了 VoVNet-27-slim，VoVNet-39， VoVNet-57。注意，其中downsampling 层是通过 3x3 stride=2 的 max pooling 实现的，conv 表示的是 Conv-BN-ReLU 的顺序连接。 VOVNet 由 5 个阶段组成，各个阶段的输出特征大小依次降为原来的一半。VOVNet-27 前 2 个 stage 的连接图如下所示。 4，实验 GPU 的能耗计算公式如下： 实验1： VoVNet vs. DenseNet. 对比不同 backbone 下的目标检测模型性能(PASCALVOC) 对比指标： Flops：模型需要的计算量 FPS：模型推断速度img/s Params：参数数量 Memory footprint：内存占用 Enegry Efficiency：能耗 Computation Efficiency：GPU 计算效率（GFlops/s） mAP（目标检测性能评价指标） 现象与总结： 现象 1：相比于 DenseNet-67，PeleeNet 减少了 Flops，但是推断速度没有提升，与之相反，VoVNet-27-slim 稍微增加了Flops，而推断速度提升了一倍。同时，VoVNet-27-sli m的精度比其他模型都高。 现象 2：VoVNet-27-slim 的内存占用、能耗、GPU 利用率都是最高的。 结论 1：相比其他模型，VoVNet做到了准确率和效率的均衡，提升了目标检测的整体性能。 实验2：Ablation study on 1×1 conv bottleneck. 结论 2：可以看出，1x1 bottleneck 增加了 GPU Inference 时间，降低了 mAP，尽管它减少了参数数量和计算量。 因为 1x1 bottleneck 增加了网路的总层数，需要更多的激活层，从而增加了内存占用。 实验3： GPU-Computation Efficiency. 图3(a) VoVNet 兼顾准确率和 Inference 速度 图3(b) VoVNet 兼顾准确率和 GPU 使用率 图3(c) VoVNet 兼顾准确率和能耗 图3(d) VoVNet 兼顾能耗和 GPU 使用率 实验室4：基于RefineDet架构比较VoVNet、ResNet和DenseNet。 结论 4：从 COCO 数据集测试结果看，相比于 ResNet，VoVnet在 Inference 速度，内存占用，能耗，GPU 使用率和准确率上都占据优势。尽管很多时候，VoVNet 需要更多的计算量以及参数量。 对比 DenseNet161(k=48) 和 DenseNet201(k=32)可以发现，深且”瘦“的网络，GPU 使用率更低。 另外，作者发现相比于 ResNet，VoVNet 在小目标上的表现更好。 实验 5：Mask R-CNN from scratch. 通过替换 Mask R-CNN 的 backbone，也发现 VoVNet 在Inference 速度和准确率上优于 ResNet。 5，代码解读 虽然 VoVNet 在 CenterMask 论文 中衍生出了升级版本 VoVNetv2，但是本文的代码解读还是针对原本的 VoVNet，代码来源这里。 1，定义不同类型的卷积函数 def conv3x3(in_channels, out_channels, module_name, postfix, stride=1, groups=1, kernel_size=3, padding=1): \"\"\"3x3 convolution with padding. conv3x3, bn, relu的顺序组合 \"\"\" return [ ('{}_{}/conv'.format(module_name, postfix), nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False)), ('{}_{}/norm'.format(module_name, postfix), nn.BatchNorm2d(out_channels)), ('{}_{}/relu'.format(module_name, postfix), nn.ReLU(inplace=True)), ] def conv1x1(in_channels, out_channels, module_name, postfix, stride=1, groups=1, kernel_size=1, padding=0): \"\"\"1x1 convolution\"\"\" return [ ('{}_{}/conv'.format(module_name, postfix), nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False)), ('{}_{}/norm'.format(module_name, postfix), nn.BatchNorm2d(out_channels)), ('{}_{}/relu'.format(module_name, postfix), nn.ReLU(inplace=True)), ] 2，其中 OSA 模块结构的代码如下。 class _OSA_module(nn.Module): def __init__(self, in_ch, stage_ch, concat_ch, layer_per_block, module_name, identity=False): super(_OSA_module, self).__init__() self.identity = identity # 默认不使用恒等映射 self.layers = nn.ModuleList() in_channel = in_ch # stage_ch: 每个 stage 内部的 channel 数 for i in range(layer_per_block): self.layers.append(nn.Sequential( OrderedDict(conv3x3(in_channel, stage_ch, module_name, i)))) in_channel = stage_ch # feature aggregation in_channel = in_ch + layer_per_block * stage_ch # concat_ch: 1×1 卷积输出的 channel 数 # 也从 stage2 开始，每个 stage 最开始的输入 channnel 数 self.concat = nn.Sequential( OrderedDict(conv1x1(in_channel, concat_ch, module_name, 'concat'))) def forward(self, x): identity_feat = x output = [] output.append(x) for layer in self.layers: # 中间所有层的顺序连接 x = layer(x) output.append(x) # 最后一层的输出要和前面所有层的 feature map 做 concat x = torch.cat(output, dim=1) xt = self.concat(x) if self.identity: xt = xt + identity_feat return xt 3，定义 _OSA_stage，每个 stage 有多少个 OSA 模块，由 _vovnet 函数的 block_per_stage 参数指定。 class _OSA_stage(nn.Sequential): \"\"\" in_ch: 每个 stage 阶段最开始的输入通道数（feature map 数量） \"\"\" def __init__(self, in_ch, stage_ch, concat_ch, block_per_stage, layer_per_block, stage_num): super(_OSA_stage, self).__init__() if not stage_num == 2: self.add_module('Pooling', nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)) module_name = f'OSA{stage_num}_1' self.add_module(module_name, _OSA_module(in_ch, stage_ch, concat_ch, layer_per_block, module_name)) for i in range(block_per_stage-1): module_name = f'OSA{stage_num}_{i+2}' self.add_module(module_name, _OSA_module(concat_ch, stage_ch, concat_ch, layer_per_block, module_name, identity=True)) 4，定义 VOVNet， class VoVNet(nn.Module): def __init__(self, config_stage_ch, config_concat_ch, block_per_stage, layer_per_block, num_classes=1000): super(VoVNet, self).__init__() # Stem module --> stage1 stem = conv3x3(3, 64, 'stem', '1', 2) stem += conv3x3(64, 64, 'stem', '2', 1) stem += conv3x3(64, 128, 'stem', '3', 2) self.add_module('stem', nn.Sequential(OrderedDict(stem))) stem_out_ch = [128] # vovnet-57，in_ch_list 结果是 [128, 256, 512, 768] in_ch_list = stem_out_ch + config_concat_ch[:-1] self.stage_names = [] for i in range(4): #num_stages name = 'stage%d' % (i+2) self.stage_names.append(name) self.add_module(name, _OSA_stage(in_ch_list[i], config_stage_ch[i], config_concat_ch[i], block_per_stage[i], layer_per_block, i+2)) self.classifier = nn.Linear(config_concat_ch[-1], num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight) elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.constant_(m.bias, 0) def forward(self, x): x = self.stem(x) for name in self.stage_names: x = getattr(self, name)(x) x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1) x = self.classifier(x) return x 5，VoVNet 各个版本的实现。vovnet57 中有 4 个 stage，每个 stage 的 OSP 模块数目依次是 [1,1,4,3]，每个 个 stage 内部对应的通道数都是一样的，分别是 [128, 160, 192, 224]。每个 stage 最后的输出通道数分别是 [256, 512, 768, 1024]，由 concat_ch 参数指定。 所有版本的 vovnet 的 OSA 模块中的卷积层数都是 5。 def _vovnet(arch, config_stage_ch, config_concat_ch, block_per_stage, layer_per_block, pretrained, progress, **kwargs): model = VoVNet(config_stage_ch, config_concat_ch, block_per_stage, layer_per_block, **kwargs) if pretrained: state_dict = load_state_dict_from_url(model_urls[arch], progress=progress) model.load_state_dict(state_dict) return model def vovnet57(pretrained=False, progress=True, **kwargs): r\"\"\"Constructs a VoVNet-57 model as described in `\"An Energy and GPU-Computation Efficient Backbone Networks\" `_. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr \"\"\" return _vovnet('vovnet57', [128, 160, 192, 224], [256, 512, 768, 1024], [1,1,4,3], 5, pretrained, progress, **kwargs) def vovnet39(pretrained=False, progress=True, **kwargs): r\"\"\"Constructs a VoVNet-39 model as described in `\"An Energy and GPU-Computation Efficient Backbone Networks\" `_. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr \"\"\" return _vovnet('vovnet39', [128, 160, 192, 224], [256, 512, 768, 1024], [1,1,2,2], 5, pretrained, progress, **kwargs) def vovnet27_slim(pretrained=False, progress=True, **kwargs): r\"\"\"Constructs a VoVNet-39 model as described in `\"An Energy and GPU-Computation Efficient Backbone Networks\" `_. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr \"\"\" return _vovnet('vovnet27_slim', [64, 80, 96, 112], [128, 256, 384, 512], [1,1,1,1], 5, pretrained, progress, **kwargs) 参考资料 论文笔记VovNet（专注GPU计算、能耗高效的网络结构） An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection 实时目标检测的新backbone网络：VOVNet Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"7-model_compression/轻量级网络论文解析/轻量级模型设计与部署总结.html":{"url":"7-model_compression/轻量级网络论文解析/轻量级模型设计与部署总结.html","title":"轻量级模型设计与部署总结","keywords":"","body":" 前言 一些关键字理解 计算量 FLOPs 内存访问代价 MAC GPU 内存带宽 Latency and Throughput Volatile GPU Util 英伟达 GPU 架构 CNN 架构的理解 手动设计高效 CNN 架构建议 一些结论 一些建议 轻量级网络模型部署总结 轻量级网络论文解析文章汇总 参考资料 文章同步发于 github 仓库 和 csdn 博客，最新板以 github 为主。 本人水平有限，文章如有问题，欢迎及时指出。如果看完文章有所收获，一定要先点赞后收藏。毕竟，赠人玫瑰，手有余香。 前言 关于如何手动设计轻量级网络的研究，目前还没有广泛通用的准则，只有一些指导思想，和针对不同芯片平台（不同芯片架构）的一些设计总结，建议大家从经典论文中吸取指导思想和建议，然后自己实际做各个硬件平台的部署和模型性能测试。 一些关键字理解 计算量 FLOPs FLOPs：floating point operations 指的是浮点运算次数，理解为计算量，可以用来衡量算法/模型时间的复杂度。 FLOPS：（全部大写），Floating-point Operations Per Second，每秒所执行的浮点运算次数，理解为计算速度, 是一个衡量硬件性能/模型速度的指标，即一个芯片的算力。 MACCs：multiply-accumulate operations，乘-加操作次数，MACCs 大约是 FLOPs 的一半。将 w[0]∗x[0]+... 视为一个乘法累加或 1 个 MACC。 内存访问代价 MAC MAC: Memory Access Cost 内存访问代价。指的是输入单个样本（一张图像），模型/卷积层完成一次前向传播所发生的内存交换总量，即模型的空间复杂度，单位是 Byte。 FLOPs 和 MAC 的计算方式，请参考我之前写的文章 神经网络模型复杂度分析。 GPU 内存带宽 GPU 的内存带宽决定了它将数据从内存 (vRAM) 移动到计算核心的速度，是比 GPU 内存速度更具代表性的指标。 GPU 的内存带宽的值取决于内存和计算核心之间的数据传输速度，以及这两个部分之间总线中单独并行链路的数量。 NVIDIA RTX A4000 建立在 NVIDIA Ampere 架构之上，其芯片规格如下所示: A4000 芯片配备 16 GB 的 GDDR6 显存、256 位显存接口（GPU 和 VRAM 之间总线上的独立链路数量），因为这些与显存相关的特性，所以 A4000 内存带宽可以达到 448 GB/s。 Latency and Throughput 参考英伟达-Ashu RegeDirector of Developer Technology 的 ppt 文档 An Introduction to Modern GPU Architecture。 深度学习领域延迟 Latency 和吞吐量 Throughput的一般解释： 延迟 (Latency): 人和机器做决策或采取行动时都需要反应时间。延迟是指提出请求与收到反应之间经过的时间。大部分人性化软件系统（不只是 AI 系统），延迟都是以毫秒来计量的。 吞吐量 (Throughput): 在给定创建或部署的深度学习网络规模的情况下，可以传递多少推断结果。简单理解就是在一个时间单元（如：一秒）内网络能处理的最大输入样例数。 CPU 是低延迟低吞吐量处理器；GPU 是高延迟高吞吐量处理器。 Volatile GPU Util 一般，很多人通过 nvidia-smi 命令查看 Volatile GPU Util 数据来得出 GPU 利用率，但是！关于这个利用率(GPU Util)，容易产生两个误区： 误区一: GPU 的利用率 = GPU 内计算单元干活的比例。利用率越高，算力就必然发挥得越充分。 误区二: 同条件下，利用率越高，耗时一定越短。 但实际上，GPU Util 的本质只是反应了，在采样时间段内，一个或多个内核（kernel）在 GPU 上执行的时间百分比，采样时间段取值 1/6s~1s。 原文为 Percent of time over the past sample period during which one or more kernels was executing on the GPU. The sample period may be between 1 second and 1/6 second depending on the product. 来源文档 nvidia-smi.txt 通俗来讲，就是，在一段时间范围内， GPU 内核运行的时间占总时间的比例。比如 GPU Util 是 69%，时间段是 1s，那么在过去的 1s 中内，GPU 内核运行的时间是 0.69s。如果 GPU Util 是 0%，则说明 GPU 没有被使用，处于空闲中。 也就是说它并没有告诉我们使用了多少个 SM 做计算，或者程序有多“忙”，或者内存使用方式是什么样的，简而言之即不能体现出算力的发挥情况。 GPU Util 的本质参考知乎文章-教你如何继续压榨GPU的算力 和 stackoverflow 问答。 英伟达 GPU 架构 GPU 设计了更多的晶体管（transistors）用于数据处理（data process）而不是数据缓冲（data caching）和流控（flow control），因此 GPU 很适合做高度并行计算（highly parallel computations）。同时，GPU 提供比 CPU 更高的指令吞吐量和内存带宽（instruction throughput and memory bandwidth）。 CPU 和 GPU 的直观对比图如下所示 图片来源 CUDA C++ Programming Guide 最后简单总结下英伟达 GPU 架构的一些特点: SIMT (Single Instruction Multiple Threads) 模式，即多个 Core 同一时刻只能执行同样的指令。虽然看起来与现代 CPU 的 SIMD（单指令多数据）有些相似，但实际上有着根本差别。 更适合计算密集与数据并行的程序，原因是缺少 Cache 和 Control。 2008-2020 英伟达 GPU 架构进化史如下图所示: 另外，英伟达 GPU 架构从 2010 年开始到 2020 年这十年间的架构演进历史概述，可以参考知乎的文章-英伟达GPU架构演进近十年，从费米到安培。 GPU 架构的深入理解可以参考博客园的文章-深入GPU硬件架构及运行机制。 CNN 架构的理解 在一定的程度上，网络越深越宽，性能越好。宽度，即通道(channel)的数量，网络深度，及 layer 的层数，如 resnet18 有 18 层网络。注意我们这里说的和宽度学习一类的模型没有关系，而是特指深度卷积神经网络的（通道）宽度。 网络深度的意义：CNN 的网络层能够对输入图像数据进行逐层抽象，比如第一层学习到了图像边缘特征，第二层学习到了简单形状特征，第三层学习到了目标形状的特征，网络深度增加也提高了模型的抽象能力。 网络宽度的意义：网络的宽度（通道数）代表了滤波器（3 维）的数量，滤波器越多，对目标特征的提取能力越强，即让每一层网络学习到更加丰富的特征，比如不同方向、不同频率的纹理特征等。 手动设计高效 CNN 架构建议 一些结论 分析模型的推理性能得结合具体的推理平台（常见如：英伟达 GPU、移动端 ARM CPU、端侧 NPU 芯片等）；目前已知影响 CNN 模型推理性能的因素包括: 算子计算量 FLOPs（参数量 Params）、卷积 block 的内存访问代价（访存带宽）、网络并行度等。但相同硬件平台、相同网络架构条件下， FLOPs 加速比与推理时间加速比成正比。 建议对于轻量级网络设计应该考虑直接 metric（例如速度 speed），而不是间接 metric（例如 FLOPs）。 FLOPs 低不等于 latency 低，尤其是在有加速功能的硬体 (GPU、DSP 与 TPU)上不成立，得结合具硬件架构具体分析。 不同网络架构的 CNN 模型，即使是 FLOPs 相同，但其 MAC 也可能差异巨大。 Depthwise 卷积操作对于流水线型 CPU、ARM 等移动设备更友好，对于并行计算能力强的 GPU 和具有加速功能的硬件（专用硬件设计-NPU 芯片）上比较没有效率。Depthwise 卷积算子实际上是使用了大量的低 FLOPs、高数据读写量的操作。因为这些具有高数据读写量的操作，再加上多数时候 GPU 芯片算力的瓶颈在于访存带宽，使得模型把大量的时间浪费在了从显存中读写数据上，从而导致 GPU 的算力没有得到“充分利用”。结论来源知乎文章-FLOPs与模型推理速度和论文 G-GhostNet。 一些建议 在大多数的硬件上，channel 数为 16 的倍数比较有利高效计算。如海思 351x 系列芯片，当输入通道为 4 倍数和输出通道数为 16 倍数时，时间加速比会近似等于 FLOPs 加速比，有利于提供 NNIE 硬件计算利用率。(来源海思 351X 芯片文档和 MobileDets 论文) 低 channel 数的情况下 (如网路的前几层)，在有加速功能的硬件使用普通 convolution 通常会比 separable convolution 有效率。（来源 MobileDets 论文） shufflenetv2 论文 提出的四个高效网络设计的实用指导思想: G1同样大小的通道数可以最小化 MAC、G2-分组数太多的卷积会增加 MAC、G3-网络碎片化会降低并行度、G4-逐元素的操作不可忽视。 GPU 芯片上 $3\\times 3$ 卷积非常快，其计算密度（理论运算量除以所用时间）可达 $1\\times 1$ 和 $5\\times 5$ 卷积的四倍。（来源 RepVGG 论文） 从解决梯度信息冗余问题入手，提高模型推理效率。比如 CSPNet 网络。 从解决 DenseNet 的密集连接带来的高内存访问成本和能耗问题入手，如 VoVNet 网络，其由 OSA（One-Shot Aggregation，一次聚合）模块组成。 轻量级网络模型部署总结 在阅读和理解经典的轻量级网络 mobilenet 系列、MobileDets、shufflenet 系列、cspnet、vovnet、repvgg 等论文的基础上，做了以下总结： 低算力设备-手机移动端 cpu 硬件，考虑 mobilenetv1(深度可分离卷机架构-低 FLOPs)、低 FLOPs 和 低MAC的shuffletnetv2（channel_shuffle 算子在推理框架上可能不支持） 专用 asic 硬件设备-npu 芯片（地平线 x3/x4 等、海思 3519、安霸cv22 等），分类、目标检测问题考虑 cspnet 网络(减少重复梯度信息)、repvgg2（即 RepOptimizer: vgg 型直连架构、部署简单） 英伟达 gpu 硬件-t4 芯片，考虑 repvgg 网络（类 vgg 卷积架构-高并行度有利于发挥 gpu 算力、单路架构省显存/内存，问题: INT8 PTQ 掉点严重） MobileNet block (深度可分离卷积 block, depthwise separable convolution block)在有加速功能的硬件（专用硬件设计-NPU 芯片）上比较没有效率。 这个结论在 CSPNet 和 MobileDets 论文中都有提到。 除非芯片厂商做了定制优化来提高深度可分离卷积 block 的计算效率，比如地平线机器人 x3 芯片对深度可分离卷积 block 做了定制优化。 下表是 MobileNetv2 和 ResNet50 在一些常见 NPU 芯片平台上做的性能测试结果。 以上，均是看了轻量级网络论文总结出来的一些不同硬件平台部署轻量级模型的经验，实际结果还需要自己手动运行测试。 轻量级网络论文解析文章汇总 MobileNetv1论文详解 ShuffleNetv2论文详解 RepVGG论文详解 CSPNet论文详解 VoVNet论文解读 参考资料 An Introduction to Modern GPU Architecture 轻量级网络论文解析合集 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"7-model_compression/神经网络量化/神经网络量化基础.html":{"url":"7-model_compression/神经网络量化/神经网络量化基础.html","title":"神经网络量化","keywords":"","body":" 1，模型量化概述 1.1，模型量化优点 1.2，模型量化的方案 1.2.1，PTQ 理解 1.3，量化的分类 1.3.1，线性量化概述 2，量化算术 2.1，定点和浮点 2.2，量化浮点 2.2，量化算术 3，量化方法的改进 3.1，浮点数动态范围选择 3.2，最大最小值（MinMax） 3.3，滑动平均最大最小值(MovingAverageMinMax) 3.4，KL 距离采样方法(Kullback–Leibler divergence) 3.5，总结 4，量化实战经验 参考资料 本文为对目前线性量化优点、原理、方法和实战内容的总结，主要参考 神经网络量化简介 并加以自己的理解和总结，适合初学者阅读和自身复习用。 1，模型量化概述 1.1，模型量化优点 模型量化是指将神经网络的浮点算法转换为定点。量化有一些相似的术语，低精度（Low precision）可能是常见的。 低精度模型表示模型权重数值格式为 FP16（半精度浮点）或者 INT8（8位的定点整数），但是目前低精度往往就指代 INT8。 常规精度模型则一般表示模型权重数值格式为 FP32（32位浮点，单精度）。 混合精度（Mixed precision）则在模型中同时使用 FP32 和 FP16 的权重数值格式。 FP16 减少了一半的内存大小，但有些参数或操作符必须采用 FP32 格式才能保持准确度。 模型量化有以下好处： 参考 TensorFlow 模型优化：模型量化-张益新 减小模型大小：如 int8 量化可减少 75% 的模型大小，int8 量化模型大小一般为 32 位浮点模型大小的 1/4： 减少存储空间：在端侧存储空间不足时更具备意义。 减少内存占用：更小的模型当然就意味着不需要更多的内存空间。 减少设备功耗：内存耗用少了推理速度快了自然减少了设备功耗； 加快推理速度，访问一次 32 位浮点型可以访问四次 int8 整型，整型运算比浮点型运算更快；CPU 用 int8 计算的速度更快 某些硬件加速器如 DSP/NPU 只支持 int8。比如有些微处理器属于 8 位的，低功耗运行浮点运算速度慢，需要进行 8bit 量化。 总结：模型量化主要意义就是加快模型端侧的推理速度，并降低设备功耗和减少存储空间， 工业界一般只使用 INT8 量化模型，如 NCNN、TNN 等移动端模型推理框架都支持模型的 INT8 量化和量化模型的推理功能。 通常，可以根据 FP32 和 INT8 的转换机制对量化模型推理方案进行分类。一些框架简单地引入了 Quantize 和 Dequantize 层，当从卷积或全链接层送入或取出时，它将 FP32 转换为 INT8 或相反。在这种情况下，如下图的上半部分所示，模型本身和输入/输出采用 FP32 格式。深度学习推理框架加载模型时，重写网络以插入 Quantize 和 Dequantize 层，并将权重转换为 INT8 格式。 注意，之所以要插入反量化层（Dequantize），是因为量化技术的早期，只有卷积算子支持量化，但实际网络中还包含其他算子，而其他算子又只支持 FP32 计算，因此需要把 INT8 转换成 FP32。但随着技术的迭代，后期估计会逐步改善乃至消除 Dequantize 操作，达成全网络的量化运行，而不是部分算子量化运行。 图四：混合 FP32/INT8 和纯 INT8 推理。红色为 FP32，绿色为 INT8 或量化。 其他一些框架将网络整体转换为 INT8 格式，因此在推理期间没有格式转换，如上图的下半部分。该方法要求算子（Operator）都支持量化，因为运算符之间的数据流是 INT8。对于尚未支持的那些，它可能会回落到 Quantize/Dequantize 方案。 1.2，模型量化的方案 在实践中将浮点模型转为量化模型的方法有以下三种方法： data free：不使用校准集，传统的方法直接将浮点参数转化成量化数，使用上非常简单，但是一般会带来很大的精度损失，但是高通最新的论文 DFQ 不使用校准集也得到了很高的精度。 calibration：基于校准集方案，通过输入少量真实数据进行统计分析。很多芯片厂商都提供这样的功能，如 tensorRT、高通、海思、地平线、寒武纪 finetune：基于训练 finetune 的方案，将量化误差在训练时仿真建模，调整权重使其更适合量化。好处是能带来更大的精度提升，缺点是要修改模型训练代码，开发周期较长。 TensorFlow 框架按照量化阶段的不同，其模型量化功能分为以下两种： Post-training quantization PTQ（训练后量化、离线量化）； Quantization-aware training QAT（训练时量化，伪量化，在线量化）。 1.2.1，PTQ 理解 PTQ Post Training Quantization 是训练后量化，也叫做离线量化，根据量化零点 $x_{zero_point}$ 是否为 0，训练后量化分为对称量化和非对称量化；根据数据通道顺序 NHWC(TensorFlow) 这一维度区分，训练后量化又分为逐层量化和逐通道量化。目前 nvidia 的 TensorRT 框架中使用了逐层量化的方法，每一层采用同一个阈值来进行量化。逐通道量化就是对每一层每个通道都有各自的阈值，对精度可以有一个很好的提升。 1.3，量化的分类 目前已知的加快推理速度概率较大的量化方法主要有： 二值化，其可以用简单的位运算来同时计算大量的数。对比从 nvdia gpu 到 x86 平台，1bit 计算分别有 5 到128倍的理论性能提升。且其只会引入一个额外的量化操作，该操作可以享受到 SIMD（单指令多数据流）的加速收益。 线性量化(最常见)，又可细分为非对称，对称和 ristretto 几种。在 nvdia gpu，x86、arm 和 部分 AI 芯片平台上，均支持 8bit 的计算，效率提升从 1 倍到 16 倍不等，其中 tensor core 甚至支持 4bit计算，这也是非常有潜力的方向。线性量化引入的额外量化/反量化计算都是标准的向量操作，因此也可以使用 SIMD 进行加速，带来的额外计算耗时不大。 对数量化，一种比较特殊的量化方法。两个同底的幂指数进行相乘，那么等价于其指数相加，降低了计算强度。同时加法也被转变为索引计算。目前 nvdia gpu，x86、arm 三大平台上没有实现对数量化的加速库，但是目前已知海思 351X 系列芯片上使用了对数量化。 1.3.1，线性量化概述 与非线性量化不同，线性量化采用均匀分布的聚类中心，原始浮点数据和量化后的定点数据存在一个简单的线性变换关系，因为卷积、全连接等网络层本身只是简单的线性计算，因此线性量化中可以直接用量化后的数据进行直接计算。 2，量化算术 模型量化过程可以分为两部分：将模型从 FP32 转换为 INT8，以及使用 INT8 进行推理。本节说明这两部分背后的算术原理。如果不了解基础算术原理，在考虑量化细节时通常会感到困惑。 2.1，定点和浮点 定点和浮点都是数值的表示（representation），它们区别在于，将整数（integer）部分和小数（fractional）部分分开的点，点在哪里。定点保留特定位数整数和小数，而浮点保留特定位数的有效数字（significand）和指数（exponent）。 绝大多数现代的计算机系统采纳了浮点数表示方式，这种表达方式利用科学计数法来表达实数。即用一个尾数(Mantissa，尾数有时也称为有效数字，它实际上是有效数字的非正式说法)，一个基数(Base)，一个指数(Exponent)以及一个表示正负的符号来表达实数。具体组成如下： 第一部分为 sign 符号位 $s$，占 1 bit，用来表示正负号； 第二部分为 exponent 指数偏移值 $k$，占 8 bits，用来表示其是 2 的多少次幂； 第三部分是 fraction 分数值（有效数字） $M$，占 23 bits，用来表示该浮点数的数值大小。 基于上述表示，浮点数的值可以用以下公式计算： $$(-1)^s \\times M \\times 2^k$$ 值得注意是，上述公式隐藏了一些细节，如指数偏移值 $k$ 使用的时候需要加上一个固定的偏移值。 比如 123.45 用十进制科学计数法可以表示为 $1.2345\\times 10^2$，其中 1.2345 为尾数，10 为基数，2 为指数。 单精度浮点类型 float 占用 32bit，所以也称作 FP32；双精度浮点类型 double 占用 64bit。 图五：定点和浮点的格式和示例。 2.2，量化浮点 32-bit 浮点数和 8-bit 定点数的表示范围如下表所示： 数据类型 最小值 最大值 FP32 -3.4e38 3.4e38 int8 -128 128 uint8 0 255 神经网络的推理由浮点运算构成。FP32 和 INT8 的值域是 $[(2−2^{23})×2^{127},(2^{23}−2)\\times 2^{127}]$ 和 $[−128,127]$，而取值数量大约分别为 $2^{32}$ 和 $2^8$ 。FP32 取值范围非常广，因此，将网络从 FP32 转换为 INT8 并不像数据类型转换截断那样简单。但是，一般神经网络权重的值分布范围很窄，非常接近零。图八给出了 MobileNetV1 中十层（拥有最多值的层）的权重分布。 图八：十层 MobileNetV1 的权重分布。 根据偏移量 $Z$ 是否为 0，可以将浮点数的线性量化分为两类-对称量化和非对称量化。 当浮点值域落在 $(-1,1)$ 之间，权重浮点数据的量化运算可使用下式的方法将 FP32 映射到 INT8，这是对称量化。其中 $x{float}$ 表示 FP32 权重， $x{quantized}$ 表示量化的 INT8 权重，$x_{scale}$ 是缩放因子（映射因子、量化尺度（范围）/ float32 的缩放因子）。 $$x{float} = x{scale} \\times x_{quantized}$$ 对称量化的浮点值和 8 位定点值的映射关系如下图，从图中可以看出，对称量化就是将一个 tensor 中的 $[-max(|\\mathrm{x}|),max(|\\mathrm{x}|)]$ 内的 FP32 值分别映射到 8 bit 数据的 [-128, 127] 的范围内，中间值按照线性关系进行映射，称这种映射关系是对称量化。可以看出，对称量化的浮点值和量化值范围都是相对于零对称的。 因为对称量化的缩放方法可能会将 FP32 零映射到 INT8 零，但我们不希望这种情况出现，于是出现了数字信号处理中的均一量化，即非对称量化。数学表达式如下所示，其中 $x_{zero_point}$ 表示量化零点（量化偏移）。 $$x{float} = x{scale} \\times (x{quantized} - x{zero_point})$$ 大多数情况下量化是选用无符号整数，即 INT8 的值域就为 $[0,255]$ ，这种情况，显然要用非对称量化。非对称量化的浮点值和 8 位定点值的映射关系如下图： 总的来说，权重量化浮点值可以分为两个步骤： 通过在权重张量（Tensor）中找到 $min$ 和 $max$ 值从而确定 $x{scale}$ 和$x{zero_point}$。 将权重张量的每个值从 FP32 转换为 INT8 。 $$ \\begin{align} x{float} &\\in [x{float}^{min}, x{float}^{max}] \\ x{scale} &= \\frac{x{float}^{max} - x{float}^{min}}{x{quantized}^{max} - x{quantized}^{min}} \\ x{zero_point} &= x{quantized}^{max} - x{float}^{max} \\div x{scale} \\ x{quantized} &= x{float} \\div x{scale} + x{zero_point} \\end{align}$$ 注意，当浮点运算结果不等于整数时，需要额外的舍入步骤。例如将 FP32 值域 [−1,1] 映射到 INT8 值域 [0,255]，有 $x{scale}=\\frac{2}{255}$，而$x{zero_point}= 255−\\frac{255}{2}≈127$。 注意，量化过程中存在误差是不可避免的，就像数字信号处理中量化一样。非对称算法一般能够较好地处理数据分布不均匀的情况。 2.2，量化算术 量化的一个重要议题是用量化算术表示非量化算术，即量化神经网络中的 INT8 计算是描述常规神经网络的 FP32 计算，对应的就是反量化过程，也就是如何将 INT8 的定点数据反量化成 FP32 的浮点数据。 下面的等式 5-10 是反量化乘法 $x{float} \\cdot y{float}$ 的过程。对于给定神经网络，输入 $x$、权重 $y$ 和输出 $z$ 的缩放因子肯定是已知的，因此等式 14 的 $Multiplier{x,y,z} = \\frac{x{scale}y{scale}}{z{scale}}$ 也是已知的，在反量化过程之前可预先计算。因此，除了 $Multiplier{x,y,z}$ 和 $(x{quantized} - x{zero_point})\\cdot (y{quantized} - y_{zero_point})$ 之间的乘法外，等式 16 中的运算都是整数运算。 $$\\begin{align} z{float} & = x{float} \\cdot y{float} \\ z{scale} \\cdot (z{quantized} - z{zero_point}) & = (x{scale} \\cdot (x{quantized} - x{zero_point})) \\cdot (y{scale} \\cdot (y{quantized} - y{zero_point})) \\ z{quantized} - z{zero_point} &= \\frac{x{scale} \\cdot y{scale}}{z{scale}} \\cdot (x{quantized} - x{zero_point}) \\cdot (y{quantized} - y{zero_point}) \\ z{quantized} &= \\frac{x{scale} \\cdot y{scale}}{z{scale}} \\cdot (x{quantized} - x{zero_point}) \\cdot (y{quantized} - y{zero_point}) + z{zero_point} \\ Multiplier{x,y,z} &= \\frac{x{scale} \\cdot y{scale}}{z{scale}} \\ z{quantized} &= Multiplier{x,y,z} \\cdot (x{quantized} - x{zero_point}) \\cdot (y{quantized} - y{zero_point}) + z_{zero_point} \\ \\end{align}$$ 等式：反量化算术过程。 对于等式 10 可以应用的大多数情况，$quantized$ 和 $zero_point$ 变量 (x,y) 都是 INT8 类型，$scale$ 是 FP32。实际上两个 INT8 之间的算术运算会累加到 INT16 或 INT32，这时 INT8 的值域可能无法保存运算结果。例如，对于 $x{quantized}=20$、$x{zero_point} = 50$ 的情况，有 $(x{quantized} − x{zero_point}) = −30$ 超出 INT8 值范围 $[0,255]$。 数据类型转换可能将 $Multiplier{x,y,z} \\cdot (x{quantized} - x{zero_point}) \\cdot (y{quantized} - y{zero_point})$ 转换为 INT32 或 INT16，和 $z{zero_point}$ 一起确保计算结果几乎全部落入 INT8 值域 [0,255] 中。 对于以上情况，在工程中，比如对于卷积算子的计算，sum(x*y) 的结果需要用 INT32 保存，同时，b 值一般也是 INT32 格式的，之后再 requantize (重新量化)成 INT8。 3，量化方法的改进 量化浮点部分中描述权重浮点量化方法是非常简单的。在深度学习框架的早期开发中，这种简单的方法能快速跑通 INT8 推理功能，然而采用这种方法的网络的预测准确度通常会出现明显的下降。 虽然 FP32 权重的值域很窄，在这值域中数值点数量却很大。以上文的缩放为例，$[−1,1]$ 值域中 $2^{31}$（是的，基本上是总得可表示数值的一半）个 FP32 值被映射到 $256$ 个 INT8 值。 量化类型：（SYMMETRIC） 对称量化和 (NON-SYMMETRIC） 非对称量化； 量化算法：MINMAX、KL 散度、ADMM； 权重量化类型：per-channel per-layer； 采用普通量化方法时，靠近零的浮点值在量化时没有精确地用定点值表示。因此，与原始网络相比，量化网络一般会有明显的精度损失。对于线性（均匀）量化，这个问题是不可避免的。 同时值映射的精度是受由 $x{float}^{min}$ 和 $x{float}^{max}$ 得到的 $x{scale}$ 显著影响的。并且，如图十所示，权重中邻近 $x{float}^{min}$ 和 $x_{float}^{max}$ 附近的值通常是可忽略的，其实就等同于映射关系中浮点值的 min 和 max 值是可以通过算法选择的。 图十将浮点量化为定点时调整最小值-最大值。 上图展示了可以调整 min/max 来选择一个值域，使得值域的值更准确地量化，而范围外的值则直接映射到定点的 min/max。例如，当从原始值范围 $[−1,1]$ 中选定$x{min}^{float} = −0.9$ 和 $x{max}^{float} = 0.8$ ，$[−0.9,0.8]$ 中的值将能更准确地映射到 $[0,255]$ 中，而 $[−1,−0.9]$ 和 $[0.8,1]$ 中的值分别映射为 $0$ 和 $255$。 3.1，浮点数动态范围选择 参考干货：深度学习模型量化（低精度推理）大总结。 通过前文对量化算数的理解和上面两种量化算法的介绍我们不难发现，为了计算 scale 和 zero_point，我们需要知道 FP32 weight/activation 的实际动态范围。对于推理过程来说，weight 是一个常量张量，动态范围是固定的，activation 的动态范围是变化的，它的实际动态范围必须经过采样获取（一般把这个过程称为数据校准(calibration)）。 将浮点量化转为定点时调整最小值/最大值（值域调整），也就是浮点数动态范围的选择，动态范围的选取直接决定了量化数据的分布情况，处于动态范围之外的数据将被映射成量化数据的边界点，即值域的选择直接决定了量化的误差。 目前各大深度学习框架和三大平台的推理框架使用最多的有最大最小值（MinMax）、滑动平均最大最小值（MovingAverageMinMax）和 KL 距离（Kullback-Leibler divergence）三种方法，去确定浮点数的动态范围。如果量化过程中的每一个 FP32 数值都在这个实际动态范围内，我们一般称这种为不饱和状态；反之如果出现某些 FP32 数值不在这个实际动态范围之内我们称之为饱和状态。 3.2，最大最小值（MinMax） MinMax 是使用最简单也是较为常用的一种采样方法。基本思想是直接从 FP32 张量中选取最大值和最小值来确定实际的动态范围，如下公式所示。 $x{min} = \\left{\\begin{matrix}min(X) & if\\ x{min} = None \\ min(x_{min}, min(X)) & otherwise\\end{matrix}\\right.$ $x{max} = \\left{\\begin{matrix}max(X) & if\\ x{max} = None \\ max(x_{max}, max(X)) & otherwise\\end{matrix}\\right.$ 对 weights 而言，这种采样方法是不饱和的，但是对于 activation 而言，如果采样数据中出现离群点，则可能明显扩大实际的动态范围，比如实际计算时 99% 的数据都均匀分布在 [-100, 100] 之间，但是在采样时有一个离群点的数值为 10000，这时候采样获得的动态范围就变成 [-100, 10000]。 3.3，滑动平均最大最小值(MovingAverageMinMax) 与 MinMax 算法直接替换不同，MovingAverageMinMax 会采用一个超参数 c (Pytorch 默认值为0.01)逐步更新动态范围。 $x{min} = \\left{\\begin{matrix}min(X) & if x{min} = None \\ (1-c)x_{min}+c \\; min(X) & otherwise\\end{matrix}\\right.$ $x{max} = \\left{\\begin{matrix}max(X) & if x{max} = None \\ (1-c)x_{max}+c \\; max(X) & otherwise\\end{matrix}\\right.$ 这种方法获得的动态范围一般要小于实际的动态范围。对于 weights 而言，由于不存在采样的迭代，因此 MovingAverageMinMax 与 MinMax 的效果是一样的。 3.4，KL 距离采样方法(Kullback–Leibler divergence) 理解 KL 散度方法之前，我们先看下 TensorRT 关于值域范围阈值选择的一张图： 这张图展示的是不同网络结构的不同 layer 的激活值分布统计图，横坐标是激活值，纵坐标是统计数量的归一化表示，而不是绝对数值统计；图中有卷积层和池化层，它们之间分布很不相同，因此合理的量化方法应该是适用于不同的激活值分布，并且减小信息损失，因为从 FP32 到 INT8 其实也是一种信息再编码的过程。 简单的将一个 tensor 中的 -|max| 和 |max| FP32 value 映射为 -127 和 127 ，中间值按照线性关系进行映射，这种映射关系为不饱和的（No saturation），即对称的。对于这种简单的量化浮点方法，试验结果显示会导致比较大的精度损失。 通过上图可以分析出，线性量化中使用简单的量化浮点方法导致精度损失较大的原因是： 上图的激活值统计针对的是一批图片，不同图片输出的激活值不完全相同，所以图中是多条曲线而不是一条曲线，曲线中前面一部分数据重合在一起了（红色虚线），说明不同图片生成的大部分激活值其分布是相似的；但是在曲线的右边，激活值比较大时（红色实现圈起来的部分），曲线不重复了，一个激活值会对应多个不同的统计量，这时激活值分布是比较乱的。 曲线后面激活值分布比较乱的部分在整个网络层占是占少数的（比如 $10^-9$, $10^-7$, $10^-3$），因此曲线后面的激活值分布部分可以不考虑到映射关系中，只保留激活值分布的主方向。 一般认为量化之后的数据分布与量化前的数据分布越相似，量化对原始数据信息的损失也就越小，即量化算法精度越高。KL 距离(也叫 KL 散度)一般被用来度量两个分布之间的相似性。这里的数据分布都是离散形式的，其离散数据的 KL 散度公式如下： $$D{KL}(P | Q) = \\sum_i P(i)log{a} \\frac{P(i)}{Q(i)} = \\sum_i P(i)[logP(x) - log Q(x)]$$ 式中 P 和 Q 分布表示量化前 FP32 的数据分布和量化后的 INT8 数据分布。注意公式要求 P、Q 两个统计直方图长度一样（也就是 bins 的数量一样）。 TensorRT 使用 KL 散度算法进行量化校准的过程：首先在校准集上运行 FP32 推理，然后对于网络每一层执行以下步骤： 收集激活输出的直方图。 生成许多具有不同饱和度阈值的量化分布。 选择最小化 KL_divergence(ref_distr, quant_distr) 的阈值 T，并确定 Scale。 以上使用校准集的模型量化过程通常只需几分钟时间。 3.5，总结 对称的，不饱和的线性量化，会导致精度损失较大； 通过最小化 KL 散度来选择 饱和量化中的 阈值 |T|; 4，量化实战经验 参考【商汤泰坦公开课】模型量化了解一下？ 1，量化是一种已经获得了工业界认可和使用的方法，在训练 (Training) 中使用 FP32 精度，在推理 (Inference) 期间使用 INT8 精度的这套量化体系已经被包括 TensorFlow，TensorRT，PyTorch，MxNet 等众多深度学习框架和启用，地平线机器人、海思、安霸等众多 AI 芯片厂商也在深度学习工具链中提供了各自版本的模型量化功能。 2，量化是一个大部分硬件平台都会支持的，因此比较常用；知识蒸馏有利于获得小模型，还可以进一步提升量化模型的精度，所以这个技巧也会使用，尤其是在有已经训好比较强的大模型的基础上会非常有用。剪枝用的会相对较少，因为可以被网络结构搜索覆盖。 参考资料 NCNN Conv量化详解（一） 卷积神经网络优化算法 神经网络量化简介 QNNPACK 实现揭秘 《8-bit Inference with TensorRT》 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"8-high-performance_computing/":{"url":"8-high-performance_computing/","title":"8. 高性能计算","keywords":"","body":"一，GPU的Hello world: 矩阵相乘 英伟达GPU架构发展史如下所示: 矩阵相乘程序是GPU编程的 Hello World 程序。编写GPU程序，实质上是一种 CPU 和 GPU 之间的“互动”，即所谓的异构开发。 1.1，GPU编程的一些名词定义 GPU编程会涉及到一些名词概念: Host: 代表 CPU。 Device: 代表 GPU。 Host memory: RAM 内存。 Device memory: GPU上 的存储。 Kernal function: GPU 函数，执行在 device 上面，调用者是 host。 Device function: GPU 函数，执行在 device 上面，调用者是 kernal function 或者 device function。 1.2，GPU程序的执行流程 下图可视化了 GPU 程序的流程: 把数据从 host memory 拷贝到 device memory上。 配置 kernal function 的参数，参数有两种：一种用中括号[ ]，一种用小括号( )。中括号的参数是 threadperblock 和 blockpergrid；小括号就是那种普通函数的参数。 几千个线程同时调用同一个 kernal function(CUDA 的核函数)，在 GPU 里面进行计算。（kernal function 的编写，是一门技术。） 把 GPU 里面的运算结果，从 device memory 拷贝回 host memory。THE END。 参考资料 github-CUDA 基础 知乎专栏-CUDA编程入门 《CUDA C Programming Guide》(《CUDA C 编程指南》)导读 CSDN专栏-NVIDIA CUDA 并行编程 英伟达GPU架构演进近十年，从费米到安培 NVIDIA CUDA C++ Programming Guide Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"8-high-performance_computing/0-处理器基础知识.html":{"url":"8-high-performance_computing/0-处理器基础知识.html","title":"0-处理器基础知识","keywords":"","body":" 一，什么是处理器 二，指令集基础 什么是 ISA ISA 功能 三，CPU 设计与实现 整数范围 时钟频率 指令周期（Instruction cycle） 指令流水线（Instruction pipeline） 指令并行（Instruction-level parallelism） 数据并行（Data parallelism）： 并发与并行 线程级并行(Thread-Lever Parallelism) 性能 参考资料 本文的知识点比较零散，主要是关于处理器的一些基本知识，大部分内容来源于参考资料并给出了自己的理解和整理。 一，什么是处理器 先描述下一般处理器的概念，维基百科)的定义是 “In computing, a processor is an electronic circuit which performs operations on some external data source, usually memory or some other data stream”。最为常见的处理器有 CPU（可以运行任何程序）、GPU（图形图像处理）和 DSP(处理数字信号)，还有专门用来做 DNN 应用神经网络处理器。 处理器或处理单元是对外部数据源（通常是内存或其他数据流）执行操作的电子组件（数字电路）。 CPU 的主要运作原理，不论其外观，都是执行储存于被称为程序里的一系列指令。在此讨论的是遵循普遍的冯·诺伊曼结构（von Neumann architecture）设计的设备。程序以一系列数字储存在计算机存储器中。差不多所有的冯·诺伊曼 CPU 的运作原理可分为四个阶段：提取、解码、执行和写回。 而专用处理器就是针对特定应用或者领域的处理器，类似于是我们经常说的 Domain Specific Architecture 的概念。 二，指令集基础 什么是 ISA 指令集(Instruction Set Architecture, ISA)是计算机抽象模型的一部分，它定义了软件如何控制 CPU。ISA 充当硬件和软件的接口，指示了处理器能够实现什么功能以及如何实现。简单来说，ISA 就是传统上软件和硬件的分界线，是用户和硬件交互的唯一方式。 ISA 定义了硬件支持的数据类型、寄存器、硬件如何管理内存、关键特性（如虚拟内存）、微处理器可以执行哪些指令，以及多个 ISA 实现的输入输出模型。ISA 可以通过添加指令或其他功能或通过添加对更大地址和数据值的支持来扩展。 来源 What is Instruction Set Architecture (ISA)? - Arm ISA 功能 大多数 ISA（典型如 x86-Intel CPU 的指令集），将程序的行为描述成每条指令都是顺序执行的，一条指令结束后，下一条在开始。 ISA 提供的主要指令可以分为四大类功能： 执行运算或处理的功能，比如算术操作指令； 控制程序流，比如循环、判断分支和跳转指令； 实现数据搬移，如内存到寄存器，寄存器之间数据搬移等指令； 最后就是一些辅助指令，如 debug、中断和 cache 之类的指令。 三，CPU 设计与实现 整数范围 CPU 数字表示方法是一个设计上的选择，这个选择影响了设备的工作方式。一些早期的数字计算机内部使用电气模型来表示通用的十进制（基于 10进位）记数系统数字。还有一些罕见的计算机使用三进制表示数字。几乎所有的现代的 CPU 使用二进制系统来表示数字，这样数字可以用具有两个值的物理量来表示，例如高低电平等等。 时钟频率 主频＝外频×倍频。大部分的 CPU，甚至大部分的时序逻辑设备，本质上都是同步的，即它们被设计和使用的前题是假设都在同一个同步信号中工作。 指令周期（Instruction cycle） 指令周期是指 CPU 要执行一条机器指令经过的步骤，由若干机器周期组成。一般会经历“取指”，“译码”，“发射/执行”和“写回”这些操作。处理器执行程序的过程就是不断重复这几个操作。 指令流水线（Instruction pipeline） 在1978年的 Intel 8086 处理器都只能一次执行单指令。 Intel首次在486芯片中开始使用，原理是：当指令之间不存在相关时，它们在流水线中是可以重叠起来并行执行。 当一条指令，完成了“取指”操作，开始进行“译码”的时候，取指模块就可以取程序的下一条指令了，这样可以让这些模块不至于闲着没用，即指令流水线可以两个以上的指令同时执行(类似车间流水线)。一般的四层流水线架构如下图所示，不同的颜色格表示不同的指令。 指令并行（Instruction-level parallelism） 同时执行多条指令。比如，一边从 memory 读数据，一边进行 fft 处理。我们经常听到的超标量（Superscalar），超长指令字（VLIW），乱序执行（ Out-of-order execution）等等技术都是发掘指令级并行的技术。 数据并行（Data parallelism）： 一个人指令同时处理多个数据。我们常听到的向量处理器（vector procesor），张量处理器（Tensor processor）多数都是利用了 SIMD（一条指令可以处理多个数据，比如一个向量乘法）技术。 并发与并行 并发与并行的通俗理解参考知乎问答-指令级并行，线程级并行，数据级并行区别？线程的概念是什么？如下： 线程级并行(Thread-Lever Parallelism) 线程级并行主要由下面两种技术的支撑： 超线程技术：2004年，奔腾4实现了Hyper-Threading.（单核心双线程） 多核技术-物理核心: 2005年，英特尔宣布他的第一个双核心 EM64T 处理器，和 Pentium D840 超线程技术实现了单个物理核心同时两个线程，也就是别人常说的虚拟内核数。比如单物理核心实现的双线程，它同时可以处理两个线程，它的物理核心数其实是是1个，通过Hyperthreading技术实现的线程级并行(Thread Lever Parallelism)。至于技术细节的实现，这涉及到高速缓存的知识。 线程级并行的好处: 当运行多任务时，它减少了之前的操作系统模拟出来的并发，那么用户进行多任务处理时可以运行更多的程序进行并发了。 它可以使单个程序运行更快。（仅当该程序有大量线程可以并行处理时） 虽然在 1960 年代已经通过操作系统已经实现了线程级并发, 但这种频繁的上下文切换意味损失了 CPU 的处理效率。 性能 CPU 的性能和速度取决于时钟频率（一般以赫兹或十亿赫兹计算，即 hz 与 Ghz）和每周期可处理的指令（IPC），两者合并起来就是每秒可处理的指令（IPS）。IPS 值代表了 CPU 在几种人工指令序列下“高峰期”的执行率，指示和应用。 参考资料 深入理解计算机系统-第三版 专用处理器设计 https://www.zhihu.com/question/21823699/answer/111606716 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"8-high-performance_computing/1-卷积算法的优化.html":{"url":"8-high-performance_computing/1-卷积算法的优化.html","title":"1-卷积算法的优化","keywords":"","body":"前言 等待更新。 参考资料 im2col方法实现卷积算法 通用矩阵乘（GEMM）优化算法 C/C++内存对齐详解 浮点数在计算机中存储方式 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"8-high-performance_computing/2-模型编译优化.html":{"url":"8-high-performance_computing/2-模型编译优化.html","title":"2-模型编译优化","keywords":"","body":"前言 等待更新。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"8-high-performance_computing/通用矩阵乘算法从入门到实践.html":{"url":"8-high-performance_computing/通用矩阵乘算法从入门到实践.html","title":"通用矩阵乘算法从入门到实践","keywords":"","body":" 一，背景知识 1.1，Linux 查看 CPU 和 Cache 信息 1.2，Windows查看cpu和cache信息 1.3， 尝试分析 init() 函数 二，优化矩阵乘法 2.1，算法层面优化 2.1.1，Strassen 算法 2.1.2，Coppersmith–Winograd 算法 2.2，指令层面优化 2.3，访存优化 2.3.1，优化方法 1 (改进访存局部性) 2.3.2，优化方法2(分块矩阵+改进访存局部性) 三，优化方法集合的完整代码 参考资料 一，背景知识 实践作业1：如何通过工具得到自己当前使用 PC 的 Cache 信息（15分）。包括 L1/L2/L3 Cache（数据和指令）的大小，Cache Line 的大小。 1.1，Linux 查看 CPU 和 Cache 信息 1，Linux 查看 cpu 信息命令：cat /proc/cpuinfo。 (base) harley@harley-pc:/sys/devices/system/cpu/cpu0/cache/index3$ cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 158 model name : Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz stepping : 10 microcode : 0xb4 cpu MHz : 3192.005 cache size : 12288 KB physical id : 0 siblings : 1 core id : 0 cpu cores : 1 apicid : 0 initial apicid : 0 fpu : yes fpu_exception : yes cpuid level : 22 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt xsaveopt xsavec xsaves arat md_clear flush_l1d arch_capabilities bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit srbds bogomips : 6384.01 clflush size : 64 cache_alignment : 64 address sizes : 43 bits physical, 48 bits virtual power management: ... processor : 3 ... 2， Linux 查询 L1/L2/L3 cache大小：cat /sys/devices/system/cpu/cpu0/cache/index*/size(*为 0/1/2/3) (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index0/size 32K (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index0/type Data (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index1/type Instruction (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index1/size 32K (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index2/size 256K (base) harley@harley-pc:~$ cat /sys/devices/system/cpu/cpu0/cache/index3/size 12288K 现代 CPU的 L1 cache 是逻辑核私有的，L1 cache 分指令 L1 cache 和数据 L1 cache，大小相等都为 32 KB；目前，L2 cache 也是片内私有，所以每个核只有256 KB；而对于 L3 cache，一个物理核CPU 的所有逻辑核共享，所以在每个逻辑核来看，L3 cache 都为12288 KB。本机的虚拟机总共有 1 个物理核，而本机共有 6 核 12 线程，所以可推算得到**本机的 Cache 信息： L1 cache： L1 Data: 192 KB = 32 x 6 KB L1 Instruction: 192 KB = 32 x 6 KB L2 cache：1536 KB = 256 X 6 KB L3 cache：12288 KB 1.2，Windows查看cpu和cache信息 1，任务管理器->性能，即可查看 cpu 和 L1/L2/L3 Cache 大小，如下图所示。 2，或者下载安装 cpuz 软件，打开即可查看，如下图所示。 1.3， 尝试分析 init() 函数 实践作业2：尝试分析 init()函数使用 O1 和 O3 优化的profile结果差异。 O3 相对 O1，执行时间减少，尝试分析反汇编代码（O3.s 和 O1.s），给出解释（15 分） O3 相对 O1，D1 Cache 的访问次数为什么从 8409K 下降到 2125K（15 分） O3 相对 O1，D1 Cache 的 miss rate 为什么从 6.2% 上升到 24.7%（15 分） 问题分析和答案: 使用 -O3 参数优化，编译器会采取很多向量化算法，提高代码的并行执行程度，利用现代 CPU 中的流水线，Cache 等。O3 优化会提高执行代码的大小，也会降低目标代码的执行时间。 访问次数下降是因为使用了 O3 优化，使得程序会自动访问连续的内存。 高速缓存缺失（cache miss）是因为访存的内存都是不连续的。 二，优化矩阵乘法 实践作业3：优化 $A^T*A$ 的矩阵乘法，目标是尽量减少计算时间。 其中 A 的大小为 1024x8192，元素为 int类型。 需要从算法层面，指令层面和访存优化的角度联合优化。 通过文档说明自己的优化思路（20 分）。 可以选择自己熟悉的处理器平台进行代码编写，如 Intel 平台或者ARM平台（20 分） 问题分析：矩阵乘的算法优化可分为两类： 基于算法分析的方法：根据矩阵乘计算特性，从数学角度优化，典型的算法包括 Strassen 算法和 Coppersmith–Winograd 算法。 基于软件优化的方法：根据计算机存储系统的层次结构特性，选择性地调整计算顺序，主要有循环拆分向量化、内存重排等。2.1，算法层面优化 从算法层面优化，首先需要分析朴素矩阵乘法的算法复杂度，分析可知，朴素的矩阵乘算法的时间复杂度为 $O(n^3)$ 。根据矩阵乘计算特性，从数学角度（算法层面）优化，典型的算法包括 Strassen 算法和 Coppersmith–Winograd 算法。 2.1.1，Strassen 算法 Strassen 算法是 1969 年提出的复杂度为 $O(n^{log_2{7}})$ 的矩阵乘法，这是历史上第一次将矩阵乘的计算复杂度价格低到 $O(n^3)$ 以下。 基于分治（Divide and Conquer）的思想，将矩阵 $A, B, C∈R^{n^2×n^2}$ 分别拆分为更小的矩阵，根据矩阵基本的运算法则，拆分后朴素算法的计算共需要八次小矩阵乘法和四次小矩阵加法计算。Strassen 算法的核心思想是通过引入辅助计算的中间矩阵，再将中间矩阵进行组合得到最后的矩阵，这个过程使用了七次乘法和十八次加法，将矩阵乘的算法复杂度降低到了 $O(n^{log_27})$ （递归地运行该算法）。算法的详细推导过程如下： 1，基于分治（Divide and Conquer）的思想，Starssen 算法将矩阵 $A,\\ B,\\ C \\in R^{n^2 \\times n^2}$ 分别拆分为更小的矩阵： $$ \\mathbf{A} = \\begin{bmatrix} \\mathbf{A}{1,1} & \\mathbf{A}{1,2} \\ \\mathbf{A}{2,1} & \\mathbf{A}{2,2} \\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix} \\mathbf{B}{1,1} & \\mathbf{B}{1,2} \\ \\mathbf{B}{2,1} & \\mathbf{B}{2,2} \\end{bmatrix}, \\mathbf{C} = \\begin{bmatrix} \\mathbf{C}{1,1} & \\mathbf{C}{1,2} \\ \\mathbf{C}{2,1} & \\mathbf{C}{2,2} \\end{bmatrix} $$ 其中，$A{i,j},\\ B{i,j},\\ C_{i,j} \\in R^{2^{n-1} \\times 2^{n-1}}$。拆分后朴素算法的计算如下所示，共需要八次小矩阵乘法和四次小矩阵加法计算。 2，引入七个如下所示的用于辅助计算的中间矩阵。 3，将中间矩阵进行组合得到最后的结果矩阵。 2.1.2，Coppersmith–Winograd 算法 Strassen 算法尽管学术意义重大，但实际应用有限，Coppersmith–Winograd 算法(1990年)的提出将矩阵乘法的算法复杂度降低到了$O(n^2.376)$。其算法的详细推导过程可参考 Matrix multiplication via arithmetic progressions （原始论文）。 2.2，指令层面优化 改进访存局部性和利用向量指令等方法都是属于软件优化方法。软件优化方法基于对计算机体系机构和软件系统的特征分析，结合具体计算的特性，设计出针对性的优化方法。 现在的 CPU 处理器，基本上想获得高的性能，必须要用向量化指令，不管是老的 SSE2，AVX 或者 AVX 2.0 等，对于CPU 的优化，如果想达到高性能，必须要用到单指令多数据（SIMD）的向量化指令。 2.3，访存优化 程序运行环境(g++ 编译器基础上 Linux系统比 Windows 运行程序时间更少一些)： 操作系统：Ubuntu 编译器：g++，g++ --std=c++17 -O3 matrix_multiplication.cpp 编程语言：C++ CPU平台: Intel 的 I7-8700 CPU 朴素的矩阵乘算法的时间复杂度为 $O(n^3)$，以 $A^T*A$ 为例，矩阵相乘核心代码如下： vector> matrix_mul(vector> A, vector> B){ /*二维矩阵相乘函数 */ // vector> A_T = matrix_transpose(A); assert((*A.begin()).size()==B.size()); //断言，第一个矩阵的列必须等于第二个矩阵的行 int new_rows = A.size(); int new_cols = (*B.begin()).size(); int L = B.size(); vector> C(new_rows, vector(new_cols,0)); for(int i=0; i 从以上代码可以看出，B[k][j] 读取内存中的数据，是不连续的。在最底层的循环中，随着 k 不断加 1，B[k][j] 不断的在内存中跳跃。这会引起缓存命中率低，循环程序不断的把内存转移至缓存，引起效率降低。在我的台式机的虚拟机上，当A 的大小为 1024x8192时，需要用时 85.3 s（我用的编译器是g++ -O3）。下面的代码是我从访存优化的角度使用的两种优化方法。 2.3.1，优化方法 1 (改进访存局部性) 内存使用上，程序访问的内存地址之间连续性越好，程序的访问效率就越高。 充分利用计算机系统的特性可以大幅度提高程序性能，参考卡内基梅隆大学的镇校神课《深入理解计算机系统》里面，给出一种方法，仅仅改变循环的次序，就可以大幅度提高性能，修改后代码如下： vector> matrix_mul_optim(vector> A, vector> B){ /*二维矩阵相乘函数 */ // vector> A_T = matrix_transpose(A); assert((*A.begin()).size()==B.size()); //断言，第一个矩阵的列必须等于第二个矩阵的行 int new_rows = A.size(); int new_cols = (*B.begin()).size(); int L = B.size(); vector> C(new_rows, vector(new_cols,0)); for(int k=0; k 首先，最内层的循环，随着 j 加 1，C[i][j] 和 B[k][j] 都是每次只加 1，这符合空间局部性的原理，也就是说，内存每次读取都是一个接着一个的来，没有大幅度跳跃。其次，A[i][k] 在中间层循环是跳跃的，但是中间层执行的没有底层那么多，而且我们把 A[i][k] 赋给了局部变量 r，在编译器生成汇编代码的过程中，局部变量 r 应该由 CPU 寄存器存储，最底层循环程序读取寄存器的时间几乎可以忽略不计的。修改后的代码运行耗时 25.2 s。 2.3.2，优化方法2(分块矩阵+改进访存局部性) 将矩阵分块（计算拆分），每次计算一部分内容。分块的目的就是优化访存，通过分块之后让访存都集中在一定区域，能够提高了数据局部性，从而提高 Cache 利用率，性能就会更好。结合分块矩阵和改进访存局部性两种方法的代买运行耗时 18.8 s。修改后的代码如下： vector> matrix_mul_optim3(vector> A, vector> B){ /*二维矩阵相乘函数，优化方法-分块矩阵 */ // vector> A_T = matrix_transpose(A); assert((*A.begin()).size()==B.size()); //断言，第一个矩阵的列必须等于第二个矩阵的行 int M = A.size(); int N = (*B.begin()).size(); int K = B.size(); // 第二个矩阵的行 int NUM = 8; // 分块数 int MT = A.size()/NUM; // 分块矩阵的行 int NT = (*B.begin()).size()/NUM; // 分块矩阵的列 int KT = B.size()/NUM; // vector> C(M, vector(N,0)); for(int kt = 0; kt 程序输出结果如下： Done! Timing : 18.889000 s The size of result matrix is (8192, 8192) 三，优化方法集合的完整代码 完整可直接在windows/Linux 上可运行的代码如下： /* * 矩阵乘法(A^T*A)实现 */ #include #include #include #include #include\"iomanip\" #include #include using namespace std::chrono; using namespace std; typedef std::vector Row; typedef std::vector Matrix; using namespace std; int m = 1024; int n = 8192; vector> init_matrix(int mm, int nn){ /*初始化指定行和列的二维矩阵 */ int random_integer; vector> matrix(mm, vector(nn,0)); // 初始化二维数组matrix为1024*8192，所有元素为0 // cout > matrix){ /*打印二维向量（矩阵）的元素 */ cout >::iterator iter; for(auto iter=matrix.cbegin();iter != matrix.cend(); ++iter) { for(int i = 0;i> matrix_transpose(vector> A){ /*获取矩阵的转置 */ int rows = A.size(); int cols = (*A.begin()).size(); vector> A_T(cols, vector(rows,0)); for (int j=0;j A1, vector B1){ /*向量相乘函数 */ assert(A1.size()==B1.size()); //断言，两个向量的长度必须相等 vector::iterator begin; // 定义迭代器 int result; // 迭代器循环遍历元素 for(int i=0; i get_col(vector> matrix, int n){ /*获取矩阵指定列的向量 */ // vector col(matrix.size()); vector col; col.reserve(matrix.size()); for(auto row: matrix){ col.push_back(row[n]); } // cout > matrix_mul(vector> A, vector> B){ /*二维矩阵相乘函数 */ // vector> A_T = matrix_transpose(A); assert((*A.begin()).size()==B.size()); //断言，第一个矩阵的列必须等于第二个矩阵的行 int new_rows = A.size(); int new_cols = (*B.begin()).size(); int L = B.size(); vector> C(new_rows, vector(new_cols,0)); for(int i=0; i> matrix_mul_optim1(vector> A, vector> B){ /*二维矩阵相乘优化函数1-改进访存局部性 */ // vector> A_T = matrix_transpose(A); assert((*A.begin()).size()==B.size()); //断言，第一个矩阵的列必须等于第二个矩阵的行 int new_rows = A.size(); int new_cols = (*B.begin()).size(); int L = B.size(); vector> C(new_rows, vector(new_cols,0)); for(int k=0; k> matrix_mul_optim2(vector> A, vector> B){ /*二维矩阵相乘优化函数2-计算拆分 */ // vector> A_T = matrix_transpose(A); assert((*A.begin()).size()==B.size()); //断言，第一个矩阵的列必须等于第二个矩阵的行 int M = A.size(); int N = (*B.begin()).size(); int K = B.size(); // 第二个矩阵的行 vector> C(M, vector(N,0)); for(int m = 0; m > matrix_mul_optim3(vector> A, vector> B){ /*二维矩阵相乘优化函数3-（分块矩阵+改进访存局部性） */ // vector> A_T = matrix_transpose(A); assert((*A.begin()).size()==B.size()); //断言，第一个矩阵的列必须等于第二个矩阵的行 int M = A.size(); int N = (*B.begin()).size(); int K = B.size(); // 第二个矩阵的行 int NUM = 8; // 分块数 int MT = A.size()/NUM; // 分块矩阵的行 int NT = (*B.begin()).size()/NUM; // 分块矩阵的列 int KT = B.size()/NUM; // vector> C(M, vector(N,0)); for(int kt = 0; kt > A; A = init_matrix(1024,8192); auto A_T = matrix_transpose(A); // print_matrix(A); // print_matrix(A_T); auto start = std::chrono::steady_clock::now(); // 开始时间 auto B = matrix_mul_optim3(A_T, A); // print_matrix(B); auto end = std::chrono::steady_clock::now(); // 匹配结束后时间 auto tt = duration_cast (end - start); printf(\"Done! Timing : %lf s\\n\", tt.count() / 1000.0); cout 程序输出结果如下： The size of init A matrix is (1024, 8192) Done! Timing : 18.787000 s The size of result matrix is (8192, 8192) 参考资料 通用矩阵乘（GEMM）优化算法 OpenBLAS项目与矩阵乘法优化 | AI 研习社 矩阵乘法的优化 how-to-optimize-gemm Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"9-model_deploy/":{"url":"9-model_deploy/","title":"9. 模型部署","keywords":"","body":"前言 本目录旨在分享模型部署的知识，包括模型转换、模型板端推理框架、算法SDK开发等知识。 目录 推理框架 ONNX模型分析与使用 TensorRT基础 0-模型压缩部署概述 1-神经网络模型复杂度分析 2-模型转换总结 3-模型板端推理 参考资料 https://developer.nvidia.com/tensorrt https://onnx.ai/ https://pytorch.org/ https://github.com/Tencent/ncnn Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"9-model_deploy/推理框架/ONNX模型分析与使用.html":{"url":"9-model_deploy/推理框架/ONNX模型分析与使用.html","title":"ONNX模型分析与使用","keywords":"","body":" 本文大部分内容为对 ONNX 官方资料的总结和翻译，部分知识点参考网上质量高的博客。 一，ONNX 概述 深度学习算法大多通过计算数据流图来完成神经网络的深度学习过程。 一些框架（例如CNTK，Caffe2，Theano和TensorFlow）使用静态图形，而其他框架（例如 PyTorch 和 Chainer）使用动态图形。 但是这些框架都提供了接口，使开发人员可以轻松构建计算图和运行时，以优化的方式处理图。 这些图用作中间表示（IR），捕获开发人员源代码的特定意图，有助于优化和转换在特定设备（CPU，GPU，FPGA等）上运行。 ONNX 的本质只是一套开放的 ML 模型标准，模型文件存储的只是网络的拓扑结构和权重（其实每个深度学习框架最后保存的模型都是类似的），脱离开框架是没办法对模型直接进行 inference的。 1.1，为什么使用通用 IR 现在很多的深度学习框架提供的功能都是类似的，但是在 API、计算图和 runtime 方面却是独立的，这就给 AI 开发者在不同平台部署不同模型带来了很多困难和挑战，ONNX 的目的在于提供一个跨框架的模型中间表达框架，用于模型转换和部署。ONNX 提供的计算图是通用的，格式也是开源的。 二，ONNX 规范 Open Neural Network Exchange Intermediate Representation (ONNX IR) Specification. ONNX 结构的定义文件 .proto 和 .prpto3 可以在 onnx folder 目录下找到，文件遵循的是谷歌 Protobuf 协议。ONNX 是一个开放式规范，由以下组件组成： 可扩展计算图模型的定义 标准数据类型的定义 内置运算符的定义 IR6 版本的 ONNX 只能用于推理（inference），从 IR7 开始 ONNX 支持训练（training）。onnx.proto 主要的对象如下： ModelProto GraphProto NodeProto AttributeProto ValueInfoProto TensorProto 他们之间的关系：ONNX 模型 load 之后，得到的是一个 ModelProto，它包含了一些版本信息，生产者信息和一个非常重要的 GraphProto；在 GraphProto 中包含了四个关键的 repeated 数组，分别是node (NodeProto 类型)，input(ValueInfoProto 类型)，output(ValueInfoProto 类型)和 initializer (TensorProto 类型)，其中 node 中存放着模型中的所有计算节点，input 中存放着模型所有的输入节点，output 存放着模型所有的输出节点，initializer 存放着模型所有的权重；节点与节点之间的拓扑定义可以通过 input 和output 这两个 string 数组的指向关系得到，这样利用上述信息我们可以快速构建出一个深度学习模型的拓扑图。最后每个计算节点当中还包含了一个 AttributeProto 数组，用于描述该节点的属性，例如 Conv 层的属性包含 group，pads 和strides 等等，具体每个计算节点的属性、输入和输出可以参考这个 Operators.md 文档。 需要注意的是，上面所说的 GraphProto 中的 input 输入数组不仅仅包含我们一般理解中的图片输入的那个节点，还包含了模型当中所有权重。举例，Conv 层中的 W 权重实体是保存在 initializer 当中的，那么相应的会有一个同名的输入在 input 当中，其背后的逻辑应该是把权重也看作是模型的输入，并通过 initializer 中的权重实体来对这个输入做初始化(也就是把值填充进来) 2.1，Model 模型结构的主要目的是将元数据( meta data)与图形(graph)相关联，图形包含所有可执行元素。 首先，读取模型文件时使用元数据，为实现提供所需的信息，以确定它是否能够：执行模型，生成日志消息，错误报告等功能。此外元数据对工具很有用，例如IDE和模型库，它需要它来告知用户给定模型的目的和特征。 每个 model 有以下组件： |Name|Type|Description| |---|---|---| |ir_version|int64|The ONNX version assumed by the model.| |opset_import|OperatorSetId|A collection of operator set identifiers made available to the model. An implementation must support all operators in the set or reject the model.| |producer_name|string|The name of the tool used to generate the model.| |producer_version|string|The version of the generating tool.| |domain|string|A reverse-DNS name to indicate the model namespace or domain, for example, 'org.onnx'| |model_version|int64|The version of the model itself, encoded in an integer.| |doc_string|string|Human-readable documentation for this model. Markdown is allowed.| |graph|Graph|The parameterized graph that is evaluated to execute the model.| |metadata_props|map|Named metadata values; keys should be distinct.| |training_info|TrainingInfoProto[]|An optional extension that contains information for training.| 2.2，Operators Sets 每个模型必须明确命名它依赖于其功能的运算符集。 操作员集定义可用的操作符，其版本和状态。 每个模型按其域定义导入的运算符集。 所有模型都隐式导入默认的 ONNX 运算符集。 运算符集(Operators Sets)对象的属性如下： |Name|Type|Description| |---|---|---| |magic|string|T ‘ONNXOPSET’| |ir_version|int32|The ONNX version corresponding to the operators.| |ir_version_prerelease|string|The prerelease component of the SemVer of the IR. |ir_build_metadata|string|The build metadata of this version of the operator set.| |domain|string|The domain of the operator set. Must be unique among all sets.| |opset_version|int64|The version of the operator set.| |doc_string|string|Human-readable documentation for this operator set. Markdown is allowed.| |operator|Operator[]|The operators contained in this operator set.| 2.3，ONNX Operator 图( graph)中使用的每个运算符必须由模型(model)导入的一个运算符集明确声明。 运算符（Operator）对象定义的属性如下： |Name|Type|Description| |---|---|---| |op_type|string|The name of the operator, as used in graph nodes. MUST be unique within the operator set’s domain.| |since_version|int64|The version of the operator set when this operator was introduced.| |status|OperatorStatus|One of ‘EXPERIMENTAL’ or ‘STABLE.’| |doc_string|string|A human-readable documentation string for this operator. Markdown is allowed.| 2.4，ONNX Graph 序列化图由一组元数据字段(metadata)，模型参数列表(a list of model parameters,)和计算节点列表组成(a list of computation nodes)。每个计算数据流图被构造为拓扑排序的节点列表，这些节点形成图形，其必须没有周期。 每个节点代表对运营商的呼叫。 每个节点具有零个或多个输入以及一个或多个输出。 图表(Graph)对象具有以下属性： Name Type Description name string 模型计算图的名称 node Node[] 节点列表，基于输入/输出数据依存关系形成部分排序的计算图，拓扑顺序排列。 initializer Tensor[] 命名张量值的列表。 当 initializer 与计算图 graph输入名称相同，输入指定一个默认值，否则指定一个常量值。 doc_string string 用于阅读模型的文档 input ValueInfo[] 计算图 graph 的输入参数，在 ‘initializer.’ 中可能能找到默认的初始化值。 output ValueInfo[] 计算图 graph 的输出参数。 value_info ValueInfo[] 用于存储除输入、输出值之外的类型和形状信息。 2.5，ValueInfo ValueInfo 对象属性如下： Name Type Description name string The name of the value/parameter. type Type The type of the value including shape information. doc_string string Human-readable documentation for this value. Markdown is allowed. 2.6，Standard data types ONNX 标准有两个版本，主要区别在于支持的数据类型和算子不同。计算图 graphs、节点 nodes和计算图的 initializers 支持的数据类型如下。原始数字，字符串和布尔类型必须用作张量的元素。 2.6.1，Tensor Element Types Group Types Description Floating Point Types float16, float32, float64 浮点数遵循IEEE 754-2008标准。 Signed Integer Types int8, int16, int32, int64 支持 8-64 位宽的有符号整数。 Unsigned Integer Types uint8, uint16 支持 8 或 16 位的无符号整数。 Complex Types complex64, complex128 具有 32 位或 64 位实部和虚部的复数。 Other string 字符串代表的文本数据。 所有字符串均使用UTF-8编码。 Other bool 布尔值类型，表示的数据只有两个值，通常为 true 和 false。 2.6.2，Input / Output Data Types 以下类型用于定义计算图和节点输入和输出的类型。 Variant Type Description ONNX dense tensors 张量是向量和矩阵的一般化 ONNX sequence sequence (序列)是有序的稠密元素集合。 ONNX map 映射是关联表，由键类型和值类型定义。 ONNX 现阶段没有定义稀疏张量类型。 三，ONNX版本控制 四，主要算子概述 五，Python API 使用 5.1，加载模型 1，Loading an ONNX model import onnx # onnx_model is an in-mempry ModelProto onnx_model = onnx.load('path/to/the/model.onnx') # 加载 onnx 模型 2，Loading an ONNX Model with External Data 【默认加载模型方式】如果外部数据(external data)和模型文件在同一个目录下，仅使用 onnx.load() 即可加载模型，方法见上小节。 如果外部数据(external data)和模型文件不在同一个目录下，在使用 onnx_load() 函数后还需使用 load_external_data_for_model() 函数指定外部数据路径。 import onnx from onnx.external_data_helper import load_external_data_for_model onnx_model = onnx.load('path/to/the/model.onnx', load_external_data=False) load_external_data_for_model(onnx_model, 'data/directory/path/') # Then the onnx_model has loaded the external data from the specific directory 3，Converting an ONNX Model to External Data from onnx.external_data_helper import convert_model_to_external_data # onnx_model is an in-memory ModelProto onnx_model = ... convert_model_to_external_data(onnx_model, all_tensors_to_one_file=True, location='filename', size_threshold=1024, convert_attribute=False) # Then the onnx_model has converted raw data as external data # Must be followed by save 5.2，保存模型 1，Saving an ONNX Model import onnx # onnx_model is an in-memory ModelProto onnx_model = ... # Save the ONNX model onnx.save(onnx_model, 'path/to/the/model.onnx') 2，Converting and Saving an ONNX Model to External Data import onnx # onnx_model is an in-memory ModelProto onnx_model = ... onnx.save_model(onnx_model, 'path/to/save/the/model.onnx', save_as_external_data=True, all_tensors_to_one_file=True, location='filename', size_threshold=1024, convert_attribute=False) # Then the onnx_model has converted raw data as external data and saved to specific directory 5.3，Manipulating TensorProto and Numpy Array import numpy import onnx from onnx import numpy_helper # Preprocessing: create a Numpy array numpy_array = numpy.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=float) print('Original Numpy array:\\n{}\\n'.format(numpy_array)) # Convert the Numpy array to a TensorProto tensor = numpy_helper.from_array(numpy_array) print('TensorProto:\\n{}'.format(tensor)) # Convert the TensorProto to a Numpy array new_array = numpy_helper.to_array(tensor) print('After round trip, Numpy array:\\n{}\\n'.format(new_array)) # Save the TensorProto with open('tensor.pb', 'wb') as f: f.write(tensor.SerializeToString()) # Load a TensorProto new_tensor = onnx.TensorProto() with open('tensor.pb', 'rb') as f: new_tensor.ParseFromString(f.read()) print('After saving and loading, new TensorProto:\\n{}'.format(new_tensor)) 5.4，创建ONNX模型 可以通过 helper 模块提供的函数 helper.make_graph 完成创建 ONNX 格式的模型。创建 graph 之前，需要先创建相应的 NodeProto(node)，参照文档设定节点的属性，指定该节点的输入与输出，如果该节点带有权重那还需要创建相应的ValueInfoProto 和 TensorProto 分别放入 graph 中的 input 和 initializer 中，以上步骤缺一不可。 import onnx from onnx import helper from onnx import AttributeProto, TensorProto, GraphProto # The protobuf definition can be found here: # https://github.com/onnx/onnx/blob/master/onnx/onnx.proto # Create one input (ValueInfoProto) X = helper.make_tensor_value_info('X', TensorProto.FLOAT, [3, 2]) pads = helper.make_tensor_value_info('pads', TensorProto.FLOAT, [1, 4]) value = helper.make_tensor_value_info('value', AttributeProto.FLOAT, [1]) # Create one output (ValueInfoProto) Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [3, 4]) # Create a node (NodeProto) - This is based on Pad-11 node_def = helper.make_node( 'Pad', # name ['X', 'pads', 'value'], # inputs ['Y'], # outputs mode='constant', # attributes ) # Create the graph (GraphProto) graph_def = helper.make_graph( [node_def], # nodes 'test-model', # name [X, pads, value], # inputs [Y], # outputs ) # Create the model (ModelProto) model_def = helper.make_model(graph_def, producer_name='onnx-example') print('The model is:\\n{}'.format(model_def)) onnx.checker.check_model(model_def) print('The model is checked!') 5.5，检查模型 在完成 ONNX 模型加载或者创建后，有必要对模型进行检查，使用 onnx.check.check_model() 函数。 import onnx # Preprocessing: load the ONNX model model_path = 'path/to/the/model.onnx' onnx_model = onnx.load(model_path) print('The model is:\\n{}'.format(onnx_model)) # Check the model try: onnx.checker.check_model(onnx_model) except onnx.checker.ValidationError as e: print('The model is invalid: %s' % e) else: print('The model is valid!') 5.6，实用功能函数 函数 extract_model() 可以从 ONNX 模型中提取子模型，子模型由输入和输出张量的名称定义。这个功能方便我们 debug 原模型和转换后的 ONNX 模型输出结果是否一致(误差小于某个阈值)，不再需要我们手动去修改 ONNX 模型。 import onnx input_path = 'path/to/the/original/model.onnx' output_path = 'path/to/save/the/extracted/model.onnx' input_names = ['input_0', 'input_1', 'input_2'] output_names = ['output_0', 'output_1'] onnx.utils.extract_model(input_path, output_path, input_names, output_names) 5.7，工具 函数 update_inputs_outputs_dims() 可以将模型输入和输出的维度更新为参数中指定的值，可以使用 dim_param 提供静态和动态尺寸大小。 import onnx from onnx.tools import update_model_dims model = onnx.load('path/to/the/model.onnx') # Here both 'seq', 'batch' and -1 are dynamic using dim_param. variable_length_model = update_model_dims.update_inputs_outputs_dims(model, {'input_name': ['seq', 'batch', 3, -1]}, {'output_name': ['seq', 'batch', 1, -1]}) # need to check model after the input/output sizes are updated onnx.checker.check_model(variable_length_model ) 参考资料 ONNX--跨框架的模型中间表达框架 深度学习模型转换与部署那些事(含ONNX格式详细分析) onnx Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"9-model_deploy/推理框架/TensorRT基础笔记.html":{"url":"9-model_deploy/推理框架/TensorRT基础笔记.html","title":"TensorRT基础笔记","keywords":"","body":"一，概述 TensorRT 是 NVIDIA 官方推出的基于 CUDA 和 cudnn 的高性能深度学习推理加速引擎，能够使深度学习模型在 GPU 上进行低延迟、高吞吐量的部署。采用 C++ 开发，并提供了 C++ 和 Python 的 API 接口，支持 TensorFlow、Pytorch、Caffe、Mxnet 等深度学习框架，其中 Mxnet、Pytorch 的支持需要先转换为中间模型 ONNX 格式。截止到 2021.4.21 日， TensorRT 最新版本为 v7.2.3.4。 深度学习领域延迟和吞吐量的一般解释： 延迟 (Latency): 人和机器做决策或采取行动时都需要反应时间。延迟是指提出请求与收到反应之间经过的时间。大部分人性化软件系统（不只是 AI 系统），延迟都是以毫秒来计量的。 吞吐量 (Throughput): 在给定创建或部署的深度学习网络规模的情况下，可以传递多少推断结果。简单理解就是在一个时间单元（如：一秒）内网络能处理的最大输入样例数。 二，TensorRT 工作流程 在描述 TensorRT 的优化原理之前，需要先了解 TensorRT 的工作流程。首先输入一个训练好的 FP32 模型文件，并通过 parser 等方式输入到 TensorRT 中做解析，解析完成后 engin 会进行计算图优化（优化原理在下一章）。得到优化好的 engine 可以序列化到内存（buffer）或文件（file），读的时候需要反序列化，将其变成 engine以供使用。然后在执行的时候创建 context，主要是分配预先的资源，engine 加 context 就可以做推理（Inference）。 三，TensorRT 的优化原理 TensorRT 的优化主要有以下几点： 算子融合（网络层合并）：我们知道 GPU 上跑的函数叫 Kernel，TensorRT 是存在 Kernel 调用的，频繁的 Kernel 调用会带来性能开销，主要体现在：数据流图的调度开销，GPU内核函数的启动开销，以及内核函数之间的数据传输开销。大多数网络中存在连续的卷积 conv 层、偏置 bias 层和 激活 relu 层，这三层需要调用三次 cuDNN 对应的 API，但实际上这三个算子是可以进行融合（合并）的，合并成一个 CBR 结构。同时目前的网络一方面越来越深，另一方面越来越宽，可能并行做若干个相同大小的卷积，这些卷积计算其实也是可以合并到一起来做的（横向融合）。比如 GoogLeNet 网络，把结构相同，但是权值不同的层合并成一个更宽的层。 concat 层的消除。对于 channel 维度的 concat 层，TensorRT 通过非拷贝方式将层输出定向到正确的内存地址来消除 concat 层，从而减少内存访存次数。 Kernel 可以根据不同 batch size 大小和问题的复杂度，去自动选择最合适的算法，TensorRT 预先写了很多 GPU 实现，有一个自动选择的过程（没找到资料理解）。其问题包括：怎么调用 CUDA 核心、怎么分配、每个 block 里面分配多少个线程、每个 grid 里面有多少个 block。 FP32->FP16、INT8、INT4：低精度量化，模型体积更小、内存占用和延迟更低等。 不同的硬件如 P4 卡还是 V100 卡甚至是嵌入式设备的卡，TensorRT 都会做对应的优化，得到优化后的 engine。 四，参考资料 内核融合：GPU深度学习的“加速神器” 高性能深度学习支持引擎实战——TensorRT 《NVIDIA TensorRT 以及实战记录》PPT https://www.tiriasresearch.com/wp-content/uploads/2018/05/TIRIAS-Research-NVIDIA-PLASTER-Deep-Learning-Framework.pdf Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"9-model_deploy/0-模型压缩部署概述.html":{"url":"9-model_deploy/0-模型压缩部署概述.html","title":"0-模型压缩部署概述","keywords":"","body":"模型压缩部署概述 模型压缩部署概述 一，模型在线部署 1.1，深度学习项目开发流程 1.2，模型训练和推理的不同 二，手机端CPU推理框架的优化 三，参考资料 一，模型在线部署 深度学习和计算机视觉方向除了算法训练/研究，还有两个重要的方向: 模型压缩（模型优化、量化）、模型部署（模型转换、后端功能SDK开发）。所谓模型部署，即将算法研究员训练出的模型部署到具体的端边云芯片平台上，并完成特定业务的视频结构化应用开发。 现阶段的平台主要分为云平台（如英伟达 GPU）、手机移动端平台（ARM 系列芯片）和其他嵌入式端侧平台（海思 3519、安霸 CV22、地平线 X3、英伟达 jetson tx2 等芯片）。对于模型部署/移植/优化工程师来说，虽然模型优化、量化等是更有挑战性和技术性的知识，但是对于新手的我们往往是在做解决模型无法在端侧部署的问题，包括但不限于：实现新 OP、修改不兼容的属性、修改不兼容的权重形状、学习不同芯片平台的推理部署框架等。对于模型转换来说，现在行业主流是使用 Caffe 和 ONNX 模型作为中间模型。 1.1，深度学习项目开发流程 在高校做深度学习 demo 应用一般是这样一个过程，比如使用 Pytorch/TensorFlow 框架训练出一个模型，然后直接使用 Pytorch 框架做推理（test）完成功能验证，但是在工业界这是不可能的，因为这样模型推理速度很慢，一般我们必须有专门的深度学习推理加速框架去做模型推理（inference）。以 GPU 云平台推理框架 TensorRT 为例，简单描述模型训练推理过程就是：训练好网络模型（权重参数数据类型为 FP32）输入 TensorRT，然后 TensorRT 做解析优化，并进行在线推理和输出结果。两种不同的模型训练推理过程对比如下图所示: 前面的描述较为简单，实际在工业届，理想的深度学习项目开发流程应该分为三个步骤: 模型离线训练、模型压缩和模型在线部署，后面两个步骤互有交叉，具体详情如下： 模型离线训练：实时性低，数据离线且更新不频繁，batchsize 较大，消耗大量 GPU 资源。 设计开发模型网络结构; 准备数据集并进行数据预处理、EDA 等操作； 深度学习框架训练模型：数据增强、超参数调整、优化器选择、训练策略调整（多尺度训练）、TTA、模型融合等； 模型测试。 模型优化压缩：主要涉及模型优化、模型转换、模型量化和模型编译优化，这些过程很多都在高性能计算推理框架中集成了，各个芯片厂商也提供了相应的工具链和推理库来完成模型优化压缩。实际开发中，在不同的平台选择不同的推理加速引擎框架，比如 GPU 平台选择 TensorRT，手机移动端（ARM）选择 NCNN/MNN，NPU 芯片平台，如海思3519、地平线X3、安霸CV22等则直接在厂商给出的工具链进行模型的优化（optimizer）和压缩。 模型优化 Optimizer：主要指计算图优化。首先对计算图进行分析并应用一系列与硬件无关的优化策略，从而在逻辑上降低运行时的开销，常见的类似优化策略其包括：算子融合（conv、bn、relu 融合）、算子替换、常数折叠、公共子表达式消除等。 模型转换 Converter：Pytorch->Caffe、Pytorch->ONNX、ONNX模型->NCNN/NPU芯片厂商模型格式（需要踩坑非常多，Pytorch、ONNX、NPU 三者之间的算子要注意兼容）。注意 ONNX 一般用作训练框架和推理框架之间转换的中间模型格式。 模型量化 Quantizer：主要指训练后量化（Post-training quantization PTQ）；权重、激活使用不同的量化位宽，如速度最快的量化方式 w8a8、速度和精度平衡的量化方式 w8a16。 模型编译优化（编译优化+NPU 指令生成+内存优化）Compiler：模型编译针对不同的硬件平台有不同优化方法，与前面的和硬件无关的模型层面的优化不同。GPU平台存在 kernel fusion 方法；而 NPU 平台算子是通过特定二进制指令实现，其编译优化方法包括，卷积层的拆分、卷积核权重数据重排、NPU 算子调优等。 模型部署/SDK输出: 针对视频级应用需要输出功能接口的SDK。实时性要求高，数据线上且更新频繁，batchsize 为 1。主要需要完成多模型的集成、模型输入的预处理、非DL算法模块的开发、 各个模块 pipeline 的串联，以及最后 c 接口（SDK）的输出。 板端框架模型推理: Inference：C/C++。不同的 NPU 芯片/不同的公司有着不同的推理框架，但是模型的推理流程大致是一样的。包括：输入图像数据预处理、加载模型文件并解析、填充输入图像和模型权重数据到相应地址、模型推理、释放模型资源。这里主要需要学习不同的模型部署和推理框架。 pipeline 应用开发: 在实际的深度学习项目开发过程中，模型推理只是其中的基础功能，具体的我们还需要实现多模型的集成、模型输入前处理、以及非 DL 算法模块的开发: 包括检测模块、跟踪模块、选帧模块、关联模块和业务算法模块等，并将各模块串联成一个 pipeline，从而完成视频结构化应用的开发。 SDK集成: 在完成了具体业务 pipeline 的算法开发后，一般就需要输出 c 接口的 SDK 给到下层的业务侧（前后端）人员调用了。这里主要涉及 c/c++ 接口的转换、pipeline 多线程/多通道等sample的开发、以及大量的单元、性能、精度、稳定性测试。 芯片平台板端推理 Inference，不同的 NPU 芯片有着不同的 SDK 库代码，但是模型运行流程类似。 不同平台的模型的编译优化是不同的，比如 NPU 和一般 GPU 的区别在于后端模型编译上，GPU 是编译生成 kernel library(cuDNN 函数)，NPU 是编译生成二进制指令；前端的计算图优化没有本质区别，基本通用。 所以综上所述，深度学习项目开发流程可以大致总结为三个步骤: 模型离线训练、模型优化压缩和模型部署/SDK输出，后两个步骤互有交叉。前面 2 个步骤在 PC 上完成，最后一个步骤开发的代码是需要在在 AI 芯片系统上运行的。最后以视差模型在海思 3519 平台的部署为例，其模型部署工作流程如下： 1.2，模型训练和推理的不同 为了更好进行模型优化和部署的工作，需要总结一下模型推理（Inference）和训练（Training）的不同： 网络权重值固定，只有前向传播（Forward），无需反向传播，因此： 模型权值和结构固定，可以做计算图优化，比如算子融合等； 输入输出大小固定，可以做 memory 优化，比如 feature 重排和 kernel 重排。 batch_size 会很小（比如 1），存在 latency 的问题。 可以使用低精度的技术，训练阶段要进行反向传播，每次梯度的更新是很微小的，需要相对较高的精度比如 FP32 来处理数据。但是推理阶段，对精度要求没那么高，现在很多论文都表明使用低精度如 in16 或者 int8 数据类型来做推理，也不会带来很大的精度损失。 二，手机端CPU推理框架的优化 对于 HPC 和软件工程师来说，在手机 CPU 端做模型推理框架的优化，可以从上到下考虑： 算法层优化：最上面就是算法层，如可以用winograd从数学上减少乘法的数量（仅在大channel尺寸下有效）； 框架优化：推理框架可以实现内存池、多线程等策略； 硬件层优化：主要包括: 适应不同的硬件架构特性、pipeline和cache优化、内存数据重排、NEON 汇编优化等。 三，参考资料 《NVIDIA TensorRT 以及实战记录》PPT Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"9-model_deploy/1-神经网络模型复杂度分析.html":{"url":"9-model_deploy/1-神经网络模型复杂度分析.html","title":"1-神经网络模型复杂度分析","keywords":"","body":" 前言 一，模型计算量分析 卷积层 FLOPs 计算 全连接层的 FLOPs 计算 二，模型参数量分析 卷积层参数量 BN 层参数量 全连接层参数量 三，模型内存访问代价计算 卷积层 MAC 计算 四，一些概念 双精度、单精度和半精度 浮点计算能力 硬件利用率(Utilization) 五，参考资料 前言 现阶段的轻量级模型 MobileNet/ShuffleNet 系列、CSPNet、RepVGG、VoVNet 等都必须依赖于于具体的计算平台（如 CPU/GPU/ASIC 等）才能更完美的发挥网络架构。 1，计算平台主要有两个指标：算力 $\\pi $和 带宽 $\\beta $。 算力指的是计算平台每秒完成的最大浮点运算次数，单位是 FLOPS 带宽指的是计算平台一次每秒最多能搬运多少数据（每秒能完成的内存交换量），单位是 Byte/s。 计算强度上限 $I_{max}$，上面两个指标相除得到计算平台的计算强度上限。它描述了单位内存交换最多用来进行多少次计算，单位是 FLOPs/Byte。 $$I_{max} = \\frac {\\pi }{\\beta}$$ 这里所说的“内存”是广义上的内存。对于 CPU 而言指的就是真正的内存（RAM）；而对于 GPU 则指的是显存。 2，和计算平台的两个指标相呼应，模型也有两个主要的反馈速度的间接指标：计算量 FLOPs 和访存量 MAC。 计算量（FLOPs）：指的是输入单个样本（一张图像），模型完成一次前向传播所发生的浮点运算次数，即模型的时间复杂度，单位是 FLOPs。 访存量（MAC）：指的是输入单个样本（一张图像），模型完成一次前向传播所发生的内存交换总量，即模型的空间复杂度，单位是 Byte，因为数据类型通常为 float32，所以需要乘以 4。CNN 网络中每个网络层 MAC 的计算分为读输入 feature map 大小、权重大小（DDR 读）和写输出 feature map 大小（DDR 写）三部分。 模型的计算强度 $I$ ：$I = \\frac{FLOPs}{MAC}$，即计算量除以访存量后的值，表示此模型在计算过程中，每 Byte 内存交换到底用于进行多少次浮点运算。单位是 FLOPs/Byte。可以看到，模计算强度越大，其内存使用效率越高。 模型的理论性能 $P$ ：我们最关心的指标，即模型在计算平台上所能达到的每秒浮点运算次数（理论值）。单位是 FLOPS or FLOP/s。Roof-line Model 给出的就是计算这个指标的方法。 3，Roofline 模型讲的是程序在计算平台的算力和带宽这两个指标限制下，所能达到的理论性能上界，而不是实际达到的性能，因为实际计算过程中还有除算力和带宽之外的其他重要因素，它们也会影响模型的实际性能，这是 Roofline Model 未考虑到的。例如矩阵乘法，会因为 cache 大小的限制、GEMM 实现的优劣等其他限制，导致你几乎无法达到 Roofline 模型所定义的边界（屋顶）。 所谓 “Roof-line”，指的就是由计算平台的算力和带宽上限这两个参数所决定的“屋顶”形态，如下图所示。 算力决定“屋顶”的高度（绿色线段） 带宽决定“房檐”的斜率（红色线段） Roof-line 划分出的两个瓶颈区域定义如下： 个人感觉如果在给定计算平台上做模型部署工作，因为芯片的算力已定，工程师能做的主要工作应该是提升带宽。 一，模型计算量分析 终端设备上运行深度学习算法需要考虑内存和算力的需求，因此需要进行模型复杂度分析，涉及到模型计算量（时间/计算复杂度）和模型参数量（空间复杂度）分析。 为了分析模型计算复杂度，一个广泛采用的度量方式是模型推断时浮点运算的次数 （FLOPs），即模型理论计算量，但是，它是一个间接的度量，是对我们真正关心的直接度量比如速度或者时延的一种近似估计。 本文的卷积核尺寸假设为为一般情况，即正方形，长宽相等都为 K。 FLOPs：floating point operations 指的是浮点运算次数,理解为计算量，可以用来衡量算法/模型时间的复杂度。 FLOPS：（全部大写）,Floating-point Operations Per Second，每秒所执行的浮点运算次数，理解为计算速度,是一个衡量硬件性能/模型速度的指标。 MACCs：multiply-accumulate operations，乘-加操作次数，MACCs 大约是 FLOPs 的一半。将 $w[0]*x[0] + ...$ 视为一个乘法累加或 1 个 MACC。 注意相同 FLOPs 的两个模型其运行速度是会相差很多的，因为影响模型运行速度的两个重要因素只通过 FLOPs 是考虑不到的，比如 MAC（Memory Access Cost）和网络并行度；二是具有相同 FLOPs 的模型在不同的平台上可能运行速度不一样。 注意，网上很多文章将 MACCs 与 MACC 概念搞混，我猜测可能是机器翻译英文文章不准确的缘故，可以参考此链接了解更多。需要指出的是，现有很多硬件都将乘加运算作为一个单独的指令。 卷积层 FLOPs 计算 卷积操作本质上是个线性运算，假设卷积核大小相等且为 $K$。这里给出的公式写法是为了方便理解，大多数时候为了方便记忆，会写成比如 $MACCs = H \\times W \\times K^2 \\times C_i \\times C_o$。 $FLOPs=(2\\times C_i\\times K^2-1)\\times H\\times W\\times C_o$（不考虑bias） $FLOPs=(2\\times C_i\\times K^2)\\times H\\times W\\times C_o$（考虑bias） $MACCs=(C_i\\times K^2)\\times H\\times W\\times C_o$（考虑bias） $C_i$ 为输入特征图通道数，$K$ 为过卷积核尺寸，$H,W,C_o$ 为输出特征图的高，宽和通道数。二维卷积过程如下图所示： 二维卷积是一个相当简单的操作：从卷积核开始，这是一个小的权值矩阵。这个卷积核在 2 维输入数据上「滑动」，对当前输入的部分元素进行矩阵乘法，然后将结果汇为单个输出像素。 公式解释，参考这里，如下： 理解 FLOPs 的计算公式分两步。括号内是第一步，计算出output feature map 的一个 pixel，然后再乘以 $H\\times W\\times C_o$，从而拓展到整个 output feature map。括号内的部分又可以分为两步：$(2\\times C_i\\times K^2-1)=(C_i\\times K^2) + (C_i\\times K^2-1)$。第一项是乘法运算次数，第二项是加法运算次数，因为 $n$ 个数相加，要加 $n-1$次，所以不考虑 bias 的情况下，会有一个 -1，如果考虑 bias，刚好中和掉，括号内变为$(2\\times C_i\\times K^2)$。 所以卷积层的 $FLOPs=(2\\times C_{i}\\times K^2-1)\\times H\\times W\\times C_o$ ($C_i$ 为输入特征图通道数，$K$ 为过滤器尺寸，$H, W, C_o$为输出特征图的高，宽和通道数)。 全连接层的 FLOPs 计算 全连接层的 $FLOPs = (2I − 1)O$，$I$ 是输入层的维度，$O$ 是输出层的维度。 二，模型参数量分析 模型参数数量（params）：指模型含有多少参数，直接决定模型的大小，也影响推断时对内存的占用量，单位通常为 M，GPU 端通常参数用 float32 表示，所以模型大小是参数数量的 4 倍。这里考虑的卷积核长宽是相同的一般情况，都为 K。 模型参数量的分析是为了了解内存占用情况，内存带宽其实比 FLOPs 更重要。目前的计算机结构下，单次内存访问比单次运算慢得多的多。对每一层网络，端侧设备需要： 从主内存中读取输入向量 / feature map； 从主内存中读取权重并计算点积； 将输出向量或 feature map 写回主内存。 MAes：memory accesse，内存访问次数。 卷积层参数量 卷积层权重参数量 = $ C_i\\times K^2\\times C_o + C_o$。 $C_i$ 为输入特征图通道数，$K$ 为过滤器(卷积核)尺寸，$C_o$ 为输出的特征图的 channel 数(也是 filter 的数量)，算式第二项是偏置项的参数量 。(一般不写偏置项，偏置项对总参数量的数量级的影响可以忽略不记，这里为了准确起见，把偏置项的参数量也考虑进来。） 假设输入层矩阵维度是 96×96×3，第一层卷积层使用尺寸为 5×5、深度为 16 的过滤器（卷积核尺寸为 5×5、卷积核数量为 16），那么这层卷积层的参数个数为 ５×5×3×16+16=1216个。 BN 层参数量 BN 层参数量 = $2\\times C_i$。 其中 $C_i$ 为输入的 channel 数（BN层有两个需要学习的参数，平移因子和缩放因子） 全连接层参数量 全连接层参数量 = $T_i\\times T_o + T_O$。 $T_i$ 为输入向量的长度， $T_o$ 为输出向量的长度，公式的第二项为偏置项参数量。(目前全连接层已经逐渐被 Global Average Pooling 层取代了。) 注意，全连接层的权重参数量（内存占用）远远大于卷积层。 三，模型内存访问代价计算 MAC(memory access cost) 内存访问代价也叫内存使用量，指的是输入单个样本（一张图像），模型/卷积层完成一次前向传播所发生的内存交换总量，即模型的空间复杂度，单位是 Byte。 CNN 网络中每个网络层 MAC 的计算分为读输入 feature map 大小（DDR 读）、权重大小（DDR 读）和写输出 feature map 大小（DDR 写）三部分。 卷积层 MAC 计算 以卷积层为例计算 MAC，可假设某个卷积层输入 feature map 大小是 (Cin, Hin, Win)，输出 feature map 大小是 (Hout, Wout, Cout)，卷积核是 (Cout, Cin, K, K)，理论 MAC（理论 MAC 一般小于 实际 MAC）计算公式如下： # 端侧推理IN8量化后模型，单位一般为 1 byte input = Hin x Win x Cin # 输入 feature map 大小 output = Hout x Wout x Cout # 输出 feature map 大小 weights = K x K x Cin x Cout + bias # bias 是卷积层偏置 ddr_read = input + weights ddr_write = output MAC = ddr_read + ddr_write feature map 大小一般表示为 （N, C, H, W），MAC 指标一般用在端侧模型推理中，端侧模型推理模式一般都是单帧图像进行推理，即 N = 1(batch_size = 1)，不同于模型训练时的 batch_size 大小一般大于 1。 四，一些概念 双精度、单精度和半精度 CPU/GPU 的浮点计算能力得区分不同精度的浮点数，分为双精度 FP64、单精度 FP32 和半精度 FP16。因为采用不同位数的浮点数的表达精度不一样，所以造成的计算误差也不一样，对于需要处理的数字范围大而且需要精确计算的科学计算来说，就要求采用双精度浮点数，而对于常见的多媒体和图形处理计算，32 位的单精度浮点计算已经足够了，对于要求精度更低的机器学习等一些应用来说，半精度 16 位浮点数就可以甚至 8 位浮点数就已经够用了。 对于浮点计算来说， CPU 可以同时支持不同精度的浮点运算，但在 GPU 里针对单精度和双精度就需要各自独立的计算单元。 浮点计算能力 FLOPS：每秒浮点运算次数，每秒所执行的浮点运算次数，浮点运算包括了所有涉及小数的运算，比整数运算更费时间。下面几个是表示浮点运算能力的单位。我们一般常用 TFLOPS(Tops) 作为衡量 NPU/GPU 性能/算力的指标，比如海思 3519AV100 芯片的算力为 1.7Tops 神经网络运算性能。 MFLOPS（megaFLOPS）：等于每秒一佰万（=10^6）次的浮点运算。 GFLOPS（gigaFLOPS）：等于每秒拾亿（=10^9）次的浮点运算。 TFLOPS（teraFLOPS）：等于每秒万亿（=10^12）次的浮点运算。 PFLOPS（petaFLOPS）：等于每秒千万亿（=10^15）次的浮点运算。 EFLOPS（exaFLOPS）：等于每秒百亿亿（=10^18）次的浮点运算。 硬件利用率(Utilization) 在这种情况下，利用率（Utilization）是可以有效地用于实际工作负载的芯片的原始计算能力的百分比。深度学习和神经网络使用相对数量较少的计算原语（computational primitives），而这些数量很少的计算原语却占用了大部分计算时间。矩阵乘法（MM）和转置是基本操作。MM 由乘法累加（MAC）操作组成。OPs/s（每秒完成操作的数量）指标通过每秒可以完成多少个 MAC（每次乘法和累加各被认为是 1 个 operation，因此 MAC 实际上是 2 个 OP）得到。所以我们可以将利用率定义为实际使用的运算能力和原始运算能力的比值： $$ mac\\ utilization = \\frac {used\\ Ops/s}{raw\\ OPs/s} = \\frac {FLOPs/time(s)}{Raw_FLOPs}(Raw_FLOPs = 1.7T\\ at\\ 3519)$$ 五，参考资料 PRUNING CONVOLUTIONAL NEURAL NETWORKS FOR RESOURCE EFFICIENT INFERENCE 神经网络参数量的计算：以UNet为例 How fast is my model? MobileNetV1 & MobileNetV2 简介 双精度，单精度和半精度 AI硬件的Computational Capacity详解 Roofline Model与深度学习模型的性能分析 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"9-model_deploy/2-模型转换总结.html":{"url":"9-model_deploy/2-模型转换总结.html","title":"2-模型转换","keywords":"","body":"前言 等待更新。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"9-model_deploy/3-模型板端推理.html":{"url":"9-model_deploy/3-模型板端推理.html","title":"3-模型板端推理","keywords":"","body":"前言 芯片的算力不一定和模型推理速度成正比，嵌入式 AI 的另一个核心是 inference 框架。对于 CPU 架构来说，是否使用 SIMD（ ARM从v7 开始就支持 NEON 指令了）、是否使用多核多线程、是否有高效的卷积实现方式、是否有做汇编优化等等都会极大影响模型运行速度；而对 DSP/NPU 等硬件架构来说，是否对模型进行量化推理、量化的方式、访存的优化等也会有很大影响。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"interview_summary/":{"url":"interview_summary/","title":"Interview Summary","keywords":"","body":"一些面经总结 计算机视觉岗 2019 届实习面经.md 计算机视觉岗 2019 届暑期实习应聘总结 2019 届地平线机器人实习总结 计算机视觉岗 2020 届秋招面经 视觉算法岗 2021 年社招面经 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"interview_summary/1-计算机视觉岗2019届实习面经.html":{"url":"interview_summary/1-计算机视觉岗2019届实习面经.html","title":"计算机视觉岗 2019 届实习面经.md","keywords":"","body":"阿里算法工程师(计算机视觉方向) 一面(1个小时10分钟)--->简历面 自我介绍，差不多 10 分钟。 简历项目和比赛介绍，中间有问一些项目和比赛细节，问了一些延伸和开放性问题： Adam 和 SGD 优化器哪个更好，好在哪里，哪个使模型更加容易发散? FPN 作用 讲下 yolov3 的架构，和 two-stage 的 mask-rcnn 有什么区别 代码测试，求 n 个数里面前k个最大的数。 我最开始说用快排，面试说还有其他方法吗，我一紧张说了个时间复杂度更大的方法，面试官提醒我可以考虑树排序，但是我没学过，回答不上来，最后面试官说你本科没学过数据结构，那就先算了。 问了几个机器学习算法，KNN 和 SVM 的细节。 这里答的不好，太久没用传统机器学习算法，很多东西都忘了，中间一个简单的几何中常见距离计算方式(欧式距离)，我忘了居然答余弦距离。 问了我有什么想问的。一面总结： 面试官人比较友好，自己项目细节一定要熟悉，简历上的东西最好清楚掌握，数据结构和常用算法一定要掌握，这是我的第一个面试经历，不管接下来的面试能否通过，都还是值得纪念和自省的。格灵深瞳算法实习生 一面（29分钟）-->基础面/项目面/终面(4月28日晚更新，已挂) 自我介绍，差不多３分钟 自我介绍要简介些，我这里自我介绍有点太详细了 钢筋检测项目介绍和目标检测框架细节 大致介绍自己的工作和项目细节 问了 faster rcnn、Mask rcnn 的细节，faster rcnn 的 rpn 结构介绍下，rpn 的 loss 是什么，masker rcnn 和 faster rcnn 有什么区别和改进 retinanet 的结构和创新点，讲一下 ssd 和 retinanet 的区别 鲸鱼识别项目介绍和图像分类网络细节介绍 大致介绍下鲸鱼识别项目 resnet 网络的创新，为什么能解决梯度消失问题，残差模块详细介绍下，为什么能解决网络层数加深带来的梯度消失和网络退化问题。 你有什么想问的 问了去了之后我能做什么 什么时候能出面试结果 面试官给我提出建议：加强论文阅读和基础原理细节掌握、加强原理的表述和表达能力面试总结 １．格林深瞳实习生面试只有一面，所以项目和基础都在这一面都问了。这次面试官问的很多问题，给了我很多启发，自己项目虽然做的多，但是在很多理论和基础原理上细节功夫下的不够，论文看的不够多。 ２．其实自己也知道，自己在基础理论和原理方面掌握得不够深，但是由于缺乏时间，我还是没做到自己的目标，希望借这次面试反映出的自己理论缺失点，来提醒和激励自己一定要把基础理论和原理彻底掌握。 ３．经过阿里的面试，自己回去把更多的项目细节掌握了，这次格林深瞳面试之后一定要把基础理论和原理掌握，从图像分类网络: resnet等，到目标检测和图像分割网络：faster rcnn、mask rcnn、ssd、yolov3等彻底掌握基础原理和细节，多看相关论文和博客。 南京地平线机器人　智能驾驶算法实习生 笔试(１个小时)--> ５道编程题 给定两个字符串 a 和 b（长度超过100w），在字符类型上 b 是 a 的子集，求 b 在 a 字符类型上的补集; 给定正整数N, 返回小于等于N且至少有一位重复数字的正整数的个数; 电话号码组合。下图是一个手机按键的样例，每一个数字包含一些字母。比如字母“A”可以通过按一次“2”得到，字母“B”可以通过按两次“2”得到，以此类推。当给定一个数字字符串，我们也可以得到相应的映射，比如“22”, 代表字母组成的可能性是[“AA”, “B”]。要求：输入为一个数字字符串，例如”2321241499844211”。输出为可能代表的所有字母组合。 给定两矩形的左上角和右下角坐标，求两矩形的重叠区域面积（overlap），若不重叠，返回0。（其实就是计算IOU）。 实现 softmax，包括 init，forward，backward。 如果把笔试题写出来侵权，一定要联系我删除笔试题哈。 一面（48分钟） 自我介绍 面地平线的这次自我介绍，比之前的面试算是有了一些改变，不再流水线式的介绍学习经历和项目经历，而是突出性格和技术栈重点。 图像基础操作题,对图像做45度旋转，如何使图像完整不缺失，缺失和超出的部分如何处理？ 项目细节 离线过采样和在线过采样哪个更快？ 如何针对已有的网络做改进，提升速度？ 如何解决类别不平衡问题？ 训练网络的指标，除了基本的的 acc, loss，roc、auc有了解吗？ 算法细节 ROI Pooling 和 ROI Align 的区别及演进 离线图像增强与在线图像增强有什么区别 Python 和计算机常考基础 装饰器怎么用 深拷贝和浅拷贝的区别 多线程和多进程的区别 Linux 和 git 命令操作基础 linux查找、查看文件的3个常用命令：which、find、wheresis。（这里应该是查找命令，当时也没听清楚，连就说了cat查看文件、which、find） 统计文件夹下的文件个数:ls -l | grep \"^_\" | wc -l（这个操作，我之前用过很多次，但是没说的很清楚，不过意思应该表达清楚了） git 的基本操作: 如何回退版本。一面总结 Python一些基础还是要搞清楚，向迭代器、深拷贝、浅拷贝，我之前都看过面经和用法，都还是忘了，真是不应该。 地平线机器人面试真的问的很广，偏工程向，碰到不会的也不要太紧张，之后一定要去补课。 自己要加强 Python 基础的一些技术盲点。 以后面试表达要有针对性，可以引导面试官往自己熟悉的方向，但不要拓展太多。 二面（70分钟） 项目介绍 项目细节，和由项目延伸的原理问题 细粒度图像分类了解吗 目标检测框架原理问题 RPN 结构讲下，RPN 的 loss 有哪些，分类 loss 是二分类还是多分类 ROI Pooling 是在 RPN 前面还是后面，讲下原理，有什么作用 ROI Polling 和 ROI Align的区别 Mask RCNN基本结构讲下 1*1 卷积作用（降维－改变特征通道数，加入非线性） Faster RCNN 的 loss 有哪些，分别讲下 CNN 的 SOTA 模型原理 ResNet结构讲下，它解决了什么问题 InceptionV3 结构讲下 C/C++/Python 基础 Python 装饰器解释下，基本要求是什么（参数为函数，返回为函数，本质是嵌套函数） C 的结构体和 C++ 类的区别(C 结构体不能定义函数) __init__ 函数作用 Python 怎么继承父类的 __init__ 函数（super 操作） 面向对象编程和面向过程编程区别 Linux 系统基础操作 一些基本命令 管道命令解释下 统计文件夹下的文件个数:ls -l | grep \"^_\" | wc -l git 相关操作 git 熟不熟悉，平常怎么用 除了commit、pull等基本命令，还用过哪些 嵌入式 Linux 系统 tensorflow 安装是源码安装还是 pip/conda 安装，交叉编译用过吗 cmake 语法了解吗 有什么想问我的 对我此次面试评价如何，我有什么需要改进和学习的地方（在学习一定要加强系统学习专业基础，在公司很难有完整时间系统学习知识） 部门主要是做什么的，我去了之后做什么方向二面总结 此次面试时间比较长，总的来说，这次面试自己还是有点进展，面试一定要保持心态放松和良好，表达要流利、清楚，针对面试官指定的问题，尽量不要拓展太多（超出问题本身），技术上一定要系统学习自己的研究方向。 小鹏汽车－图像算法实习生 一面（30分钟） 自我介绍 项目介绍 项目细节询问 数据增强用了哪些，为什么用 拓展问题 图像分割结果，如果边缘信息本来是直线的，但是分割出来效果线确是弯的，怎么解决（有点记不清了） 你有什么想问我的没二面（27分钟） 项目介绍 你平时看过哪些论文，最新的论文看过哪些 平时几点钟回去, 代码量多少，平时用C、C++还是Python编程. 你有什么想问的面试总结 二面的面试官提到我最新的论文看的不多，其实最新的论文是一定要看的，紧跟行业发展，了解技术的最新发展动向，而且也有助于拓展自己的思维，学习下别人的idea。合心科技算法实习生（一家不尊重面试者、面试体验极其糟糕的公司）(不到10分钟) 一面（不到10分钟） 基本介绍（不确定他有在听吗） 项目介绍（这个过程，面试语气度非常不友好，我也不确定他有在听我讲项目没，反正我讲完了，他也没问我什么问题，评价了下我做的东西太简单、太 low 了（大意是这个），说我检测的项目就是套框架、没有自己实现框架，目标检测的项目虽然是用了 mask rcnn 的框架，但是我自己也做了很多其他的工作，比如测试的程序、数据过采样、数据标签生成、训练策略调整等是自己写的。这个过程体验真的极其糟糕，我深深地感受到了面试官不尊重人、看不起人的语气和态度） 你有什么想问我的吗（到这里面试官有些不耐烦了，估计就是照着流程问下，我问了这个岗位主要做什么方向的产品，被直接怼，你都不看招聘要求吗，我说招聘信息没写清楚，面试官不耐烦的讲了下是做教育产品，面试结束） 面试总结 我承认自己技术水平不够，还需要不断学习，但是这不能成为这家公司面试官不尊重、看不起起我的理由，既然我通过了你们的简历筛选，就说明我的简历和技术水平得到了你们的部分承认，但是面试过程，不仅是在浪费双方时间，我更直接地感受到了“合心科技”这家公司深深的恶意和不尊重人。 说实话，我实习面了有９家公司了，阿里、地平线机器人、格灵深瞳、小鹏汽车都面过，面试官都是很友好的，但是这家\"合心科技\"公司的面试官真的态度超级糟糕，全程一副不尊重人、看不起人的态度，面试迟到、全程一副高高在上的态度、那种看不起人、不尊重面试者的语气，对不起，我真的实在是受不了。 最后，对于合心科技，这家创业公司，公司规模（50-150）人，我在这里写出面试过程，希望以后找实习的同学尽量避免这家公司（合心科技）的坑，不要让糟糕的面试体验影响了大家找工作的心情和态度。 写下这个总结的过程，我是尽量平复了自己的内心，尽量希望自己糟糕的心情不要影响了我的文字表达能力。这个面经我也保持了客观的态度，以上内容没有任何虚假。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"interview_summary/2-计算机视觉岗2019届暑期实习应聘总结.html":{"url":"interview_summary/2-计算机视觉岗2019届暑期实习应聘总结.html","title":"计算机视觉岗 2019 届暑期实习应聘总结","keywords":"","body":" 找实习感想 找实习建议 面试过程建议 计算机视觉岗找实习心得 计算机视觉面试问题分类总结 已面试的公司和进展 实习面经 找实习感想 从4月１日开始找实习，到5月13日收到南京地平线机器人口头offer和北京小鹏汽车技术面通过通知，这40来天的时间真的过的不容易，每天早上醒来都被“找实习”这三个字压得沉甸甸的，一有空闲时间就看牛客网面经和投简历，前前后后差不多有 60 家公司，大公司、小公司、独角兽公司，只要招深度学习/计算机视觉/图像算法相关岗位，我都投，投递的渠道最开始是官网投递，后来发现太慢了，招银、平安科技这两家公司填写的简历信息太多太耗时了，后来就转 boss 直聘、拉勾网、牛客网和内推邮箱渠道，这样就省事很多，一个平台填好简历，可以投递不同公司。 这段时间自己也学到了很多东西，弥补了些自身技术盲点和加深了对一些基本原理和理论的理解，更重要的是认识到了与大佬之间的差距，对于搞技术的人来说，一定要保持终身学习的态度和激情，也要保持心态良好和身体健康。 找算法实习岗对大多数人来说是心智和体力的持久战，据我所知，大多数人都花了 1 个月以上的时间，所以要做好长期的打算，如果一时失利也别灰心，出去放松一天，放松下自己的心情，保持良好的精神和身体状况很重要。 实习平台很重要，一定要尽量投大公司、知名企业或者行业独角兽，不仅是面试正规和面试官温和亲切的态度，更重要的是自己本身面试之后经过反省总结，也能学到很多东西，小公司的面试大多都不是很正规。好的面试官，不管你技术实力是否满足他们要求，面试官一般都会很温和，让你放轻松，给你足够的尊重，有些面试官甚至会给你提些很有帮助和恳切的建议。这里，给我留下良好印象的是阿里和地平线机器人的面试官，面试官人真的超级友好和亲切。 找实习建议 心态第一，坚持为胜，找实习是一个持久战。对于技术实力一般的人来说，真的要保持良好的心态，不抛弃、不放弃，这中间也许你会经历很多失败，但是真的只要坚持下来，我不保证你能拿到很好公司的offer，但是你自己本省一定能从面试中学到很多东西，尤其是很多大公司的面试官会给你一些很恳切的建议，可以让你受益匪浅。 实习要趁早。建议有条件的 研一 或者 大三 就去实习，对于暑期实习来说，简历投递一定要趁早，最好 2 月份就开始，我很晚才投简历，很可能就会错过内推时间和岗位 hc 没有的情况，我 4 月份开始投简历已经算是很晚了，所以也直接导致我投了海康威视之后，一直是简历复选中，很有可能就是 hc 已经没了。 数据结构与算法题必须刷。虽然这是老生常谈的建议，但是我们必须记住很多大公司一定会有笔试题，就算是内推免笔试的，面试过程中也很有可能会出数据结构与算法题。这里我建议去 leetcode 或者牛客网上刷剑指offer，一般把简单和中等难度题刷会就可以，笔试题一般都不会很难，除非是谷歌、MSRA 那种公司可能会对笔试题有更高的要求。刷题的话，第一次刷不会可以去看参考解题思路和答案，看完后要自己写出代码。 常见面试题要掌握。其实关于计算机基础和计算机视觉算法原理的面试题，可以提前准备下，有些题频繁的问到，可以提前准备下，比如：1*1 卷积作用，链式求导公式，多线程与多进程区别，tcp/udp 通信原理等。这些常见面试题，都可以在牛客网找的到，当然不同岗位面试题不一样，甚至每个人的面试问题都有很大不一样，不要因为别人面经上的题不会，你就有些失去信心，但是对于同一个岗位的频繁出现的面试问题还是必须掌握。机器学习、算法工程师面试考点汇总，参考这里。 简历要有亮点。paper、算法比赛、项目、实习必须要有一个啊，博客、github 最好也要有，这真的很给简历加分！实验室没有发 paper 和项目条件的，可以考虑去 天池、Datafountain、kaggle 上打比赛，真的可以学到很多东西。 面试过程建议 不要紧张，表达要清楚流利，要记住，绝大部分面试官都是很友好和亲切的，尤其是大公司的面试官真的超级温和，这里为阿里和地平线机器人面试官点赞！对于那种不尊重人和看不起人的面试官，我个人觉得没必要去他们公司了，一般这种面试官会出现在小公司，一个面试官连对面试者基本的尊重都做不到，我难以想象这家公司的文化是什么样子。 要对自己有信心，但是千万不能撒谎和装逼，一般面试过程中不会的问题，面试官也会跳过或者安慰你没事的，不用太紧张，碰到 １、2 个不会的问题也属正常。 计算机视觉岗找实习心得 首先，我的水平真的算是很一般的，真正的大佬都是很轻松的拿到数个满意 offer。我自己是本科是双非大学自动化专业，研究生是中等 211 大学控制工程专业，本科主要搞嵌入式方向，研究生才转为计算机视觉方向和深度学习方向，这里也给后来者一个建议，如果不是真心热爱、喜欢你所从事的计算机视觉方向，只是为了钱的话那就真的没必要了，CV 岗竞争真的很激烈，nlp 和开发岗好很多，而且现在开发岗工资真的很高啊！ 其次，最开始找实习的时候，我没有刷过数据结构和算法题，导致我华为(程序写出来了，但是输入输出格式没注意)、百度、腾讯笔试统统挂了，那段时间真的超级难过，后悔没有提前刷题。4月20号之后，我开始在 boss 直聘上投简历，这里陆陆续续收到 2 家小公司面试通知和 offer，也算是给了我些信心。 最后，在这段时间一边把之前项目细节搞清楚，一边开始复习栈、队列、链表、二叉树和经典数据结构算法原理，并在 leetcode 上刷题，因为时间关系题目刷的不多，然后就是把 Faster RCNN、FPN、Mask RCNN、retinanet、ResNet、VGG、InceptionV3 等经典网络结构原理和细节部分彻底搞清楚，并去看了部分检测框架源码，然后就是深度学习算法的一些基本原理：链式求导过程、BP 反向传播、SGD 优化器原理、激活函数公式及原理、常见图像处理算法等，这里涉及一些公式，还就是 C/C++/Python 编程基础，和计算机基础等。每经过一次面试，我都会自我总结，这使得我后期对面试也开始有了些自己的经验和心得。 计算机视觉面试问题分类总结 对于 CV 实习岗，面试涉及到的知识可参考下面的部分： 目标检测算法原理和网络结构细节: two-stage 算法: Faster RCNN、FPN、Mask RCNN 等 one-stage 算法：SSD、yolo、retinanet 等 无 anchor 算法: centernet、FCOS等 CNN 的 SOTA 网络原理和细节: ResNet、VGG、InceptionV3、DenseNet 等. 深度学习算法公式理解：链式求导过程、BP 反向传播、SGD 优化器原理、激活函数公式及原理、常见图像处理算法等. C/C++/Java/Python 编程基础 C++ 构造函数与析构函数意义、指针和引用区别、new，malloc 区别、抽象类和接口的区别等 Python 浅拷贝和深拷贝区别、装饰器使用、super() 用法、高阶函数：map/reduce/filter/sorted 用法、try...except 使用等 如何用 C++/Java/Python 写多进程和多线程代码 计算机基础:计算机网络、操作系统、数据库 TCP/IP 算法, IP 寻址, socket 通信流程 大端小端存储，如何将小端存储模式转为大端存储模式 OSI 七层模型解释 数据库基本操作，及 sql 语句 多进程与多线程区别 常用工程工具使用基础 cmake、git 语法等和操作 Linux 系统开发，常见命令使用和 shell 语法 项目或者 paper 细节，根据项目细节延伸问如何做提升和改变 已面试的公司和进展 投递公司 简历投递渠道 进展 北京阿里 朋友内推 一面挂 北京格林深瞳 boss 直聘投递 一面挂 北京百度 网申 笔试挂 深圳腾讯 朋友内推 笔试挂 川渝华为 官网投递 笔试挂 北京小鹏汽车 boss直聘投递 offer，婉拒 南京地平线机器人 boss直聘投递 正式offer 深圳平安科技 boss直聘投递 简历过，拒绝面试 深圳pony.ai boss直聘投递 简历过，拒绝笔试 深圳中科类脑 boss直聘投递 简历过，婉拒面试 杭州新再灵 拉钩网 技术面过，hr面挂 康尚生物医疗 boss直聘投递 offer，婉拒 北京矩视智能 boss直聘投递 口头offer，婉拒 上海拼多多 朋友内推 简历过，没参加笔试 北京合心科技 拉勾网投递 面试官极其不尊重人，放弃 实习面经 实习面经文章。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"interview_summary/3-2019届地平线机器人实习总结.html":{"url":"interview_summary/3-2019届地平线机器人实习总结.html","title":"2019 届地平线机器人实习总结","keywords":"","body":"关于工作内容 来地平线实习差不多 3 个月了，在这边完成的工作内容，主要有以下几个方面： 抽烟检测模型的输出，包括 arm、gpu、j2定点化、mimic模型的输出及相关训练集及模型测试分析报告的撰写； ARM 端抽烟检测工程代码的优化，主要是针对数据预处理、数据可视化、模型训练、模型评测、fp/fn 定性分析等代码； 针对不同平台的模型难以统一评测的问题，编写了统一的评测程序，配置相关 yaml 文件即可完成不同平台模型的评测并输出 pr曲线； 通过对数据过采样的方式解决了 阴阳脸 误报严重的问题； 还有一些其他工作：比如数据送标、训练集/测试集制作、日常标注答疑等。 关于工作内容上，自己做的抽烟检测问题，是属于多分类问题，在学术界多分类问题不是什么大问题，早已经有可行的解决方案了，但是在实际工业中发现，即使是三分类的这样简单的问题，在实际项目中也会碰到各种各样的问题，这让我认识到 AI 要想真正落地，是真的需要一批真心喜欢技术，又踏实的人去把 AI 技术落地，这中间也许会碰到一些 dirty work 吧，对于实习生的成长也可能是不利的，但是对于业务项目来说，这些工作可能又是必须做的。 关于工作环境 地平线是 to B 型公司，因此底层技术和业务项目显得尤为重要，需要每一个地平线研发人员扎实做底层技术和踏实做好业务产品。在这边整体工作环境还不错，虽然会加班，但是公司为每个员工配备了升降桌和电竞椅，以考虑员工的身体健康，每天也有零食来补充能量。身边的同事很友好，让我感动的在于有些同事虽然平时很忙，但是有问题找他的时候，他也会很耐心的帮你解决疑惑，感谢周围每一个帮助过我的同事。 关于个人成长 有些遗憾，在这边实习个人成长方面没有达到自己的要求，虽然刚来第一个月自己成长很快，但是后期成长有限，自己的成长跟付出不成正比吧。这一点就不细说了，希望地平线以后能完善对于实习生的培养规范，不能只是一味的要求干活。值得让我注意的是，我发现光干活真的并不一定能让你得到很多成长，但是学习别人的优秀工程代码和学习新知识，然后再应用到实际项目中，那样不仅自己能得到成长，工作效率也能大大提升。自己后期就是稍微看了一个代码优秀的系统性的项目代码，短时间内就让自己收获很多，可惜后期时间有限，留给自己学习的时间不多，否则自己的成长能更多些。 关于技术收获 虽然在公司很多事，但是自己回家后偶尔也会看会书，在公式有时间也会看些基础知识和新知识，总的来说技术收获还是有些： 开发工具熟悉/了解：git/shell/tmux/cmake/hadoop命令 抽烟工程代码优化及数据处理程序编写，熟练掌握 Python 了解模型压缩知识（浅层网络设计/知识蒸馏等）及熟悉模型评测指标代码编写及分析 熟悉 mxnet 框架 展望 实习近 3 个月的时间匆匆而过，这段时间虽然过得很累，但是实习是研究生必须经历的一个阶段，不管是找工作还是对于自己以后的人生职业规划都是有用的，而且越早实习越好！！！在研三接下来的一年时间里： 我会先刷题，复习机器学习/深度学习/图像处理/C++/Python/的基础知识和项目细节，为找工作做准备，这几个月要为找工作而努力 之后，做一些之前没做的事：复现论文，faster rcnn/mask rcnn/unet/yolo/ssd 等论文，一个就可以，尽力发一个还不错的论文 掌握 C++，完整的写一个 C++ 项目 继续学习目标检测/语义分割的知识，参加 Kaggle比赛，尽力拿一个金牌吧（很难，但是当作目标） Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"interview_summary/4-计算机视觉岗2020届秋招面经.html":{"url":"interview_summary/4-计算机视觉岗2020届秋招面经.html","title":"计算机视觉岗 2020 届秋招面经","keywords":"","body":"字节跳动AI Lab-计算机视觉算法工程师 一面 （挂） 钢筋数量检测项目深挖。 Roi Pooling 和 Roi Algin区别？ F1 Score 如何计算？ Siamese 网络原理，loss 计算方式。 算法题（忘了） 地平线机器人-计算机时间算法工程师 提前批一面（挂）(1个小时) 两道算法题： 求二叉树的右视图 输入一个数，找出含 7 的数字的数的最大个数 实习项目细节 Roi pooling 和 Roi Align 怎么做的，Roi Align的反向传播公式写下。 几种优化器说下，说下区别及优缺点。 SGD 公式和带动量SGD公式写下。 元戎启行-计算机视觉算法工程师 一面（50分钟）(挂) 自我介绍。 项目细节，深挖项目（图像增强怎么做的，过采样具体怎么实现，模型融合怎么融合等）。 实习项目，具体怎么解决了问题。 Roi pooling 为什么不如 Roi Align？ Focal loss 说下。 git 如何回退版本： git log 查看历史版本 git reset -hard [版本id] 恢复到历史版本 git push -f -u origin master 把修改推送到远程服务器 多进程了解吗： Python 如何实现多进程; 多进程中如何对同一个变量进行操作; 进程之间的通信方式。 一道编程题 涂鸦移动-软件开发工程师 一面(45分钟) 实习项目介绍。 Python多进程介绍下。 面向对象讲解下。 说说有哪些排序算法，讲下你最熟悉的几种，怎么实现的。 两道编程题 二维数组矩阵 求 top k 数 二面(40分钟) 多进程与多线程区别。 两数之和（leetcode2） 三数之和（leetcode15） 排序算法: 讲下常用的排序算法，及各自时间复杂度。 快排原理过程说下，什么情况下时间复杂度最高。 其他问题 平安智慧城-图像识别算法工程师 玄学挂, 问题都答上来，反手就是挂，感觉看学校。 一面（15分钟） Faster RCNN 结构画下，讲下过程。 VGG 和 ResNet 结构画下。 ResNet 结构解决了什么问题，怎么解决的？ pooling 层作用。 Inception 结构画下。 华为海思-人工智能算法工程师 （一面体验不好，二面编程题自己状态出问题了，导致用错了方法（时间复杂度 O(1) 就能解决，我没想出来） 华为海思一面 Faster RCNN 为什么能精准定位到检测框的位置？ 知识蒸馏方法的一个问题（这个问题问的太抽象了，我始终没有搞懂面试官想要问我什么）。 你有没有对现有网络做过改进，怎么改进的（说了对压缩模型的一些设计，但是面试官不满意）。 手撕代码：指定位置反转链表， 并把代码每一行解释清楚 建议我转通用软件开发，说我不适合做算法，我不同意，导致后面二面心情有点糟糕，状态不在线。 华为海思二面 Python 的一些基础知识。 Inception 画下，以其中一个模块为例，从头到尾解释下包括： 卷积核参数的选择？ 为什么 Inception 能为了增加网络对尺度的适应性？ 感受野是什么？ 聊天， 给我挑道简单的代码题、 手撕代码，代码题不难，面试官人也很好，提醒了有更少的时间复杂度，但是自己状态不好，没想到时间复杂度可以为 O(1)。 总结 华为今年感觉不关心你的项目了， 很看中手撕代码，只要代码那关没过，就算你基础知识原理和项目做的再出色，估计也过不了， 相对，只要手撕代码过了，前面就算答得很烂，也还是能面试通过。 奥比中光-算法优化 笔试 选择题：笔试都是考一些算法优化的问题，没有专门了解过的话，真的很多人估计都不会做。 编程题：代码优化。 一面 实习项目介绍： 解决了什么问题 怎么解决的 inception 结构原理描述。 博客写了什么内容 传统图像处理方法说下 深度学习（神经网络）的一些基本结构说下 HR面 自我介绍。 实习项目介绍： 用了什么方法 为什么用inception结构 目标检测了解哪些方法？ 平时喜欢看什么书？ 大学期间有参加什么活动吗？ 手里有其他offer没？ 总结 面试官和hr人都很友好，会耐心的听你把话说完，面试体验还可以。 58同城-机器学习算法工程师 （问的很细致，公式的每一个参数都要解释清楚） 5一面 KNN 原理讲下，以一个实际问题为例，讲下 KNN 怎么做的 retinanet 网络相关： retinanet 结构讲下 FPN 网络画下，讲下原理 Focal loss 讲下，写下公式，讲下 $\\alpha{t}$ 和 $\\gamma$ 两个参数作用，为什么 $\\alpha{t}$ 可以解决正负样本的不平衡， $\\gamma$ 可以解决解决难易样本的不平衡问题。 soft-nms 和硬 nms 原理和过程各自介绍下，为什么soft-nms 能解决漏检问题（我没讲清楚）。 dropout 和 bn 在训练阶段和预测阶段有什么不同，具体原理和过程说下。 手撕 nms 代码，并讲清楚过程(10分钟)。 58同城二面 逻辑回归怎么解决过拟合问题？ dropout 原理，训练和测试阶段有什么不同。 tensorflow 一些框架的问题，有用 tensorflow 写过模型和项目吗（有） 实习项目介绍，inception结构和原理介绍下。 模型压缩都有哪些方法，说下知识蒸馏怎么做的。 防止过拟合的方法有哪些，这些方法都是怎么做的？ 58同城hr面 介绍自己。 奖学金拿过吗？ 在大学期间的一些活动。 实习项目介绍下，解决了什么问题，还有什么问题没解决，打算怎么解决。 手里有其他 offer 没和秋招情况。 瓜子二手车-机器学习算法工程师 一面（30分钟） 概率题，并写出相关代码。 实习项目介绍， inception 网络原理。 faster rcnn 网络讲下。 二面（70分钟） 中途接了个电话。 排好序的有重复数字的数组，从中找一个指定元素，并返回最小的那个索引值（同步 IDE 写代码, 要求 AC 并且尽可能最优解）。 回文整数（不能使用str）。 传统图像处理方法： opencv 图像处理基本方法 边缘检测算子有哪些，说下canny 算子具体怎么做的 HOG 特征算法过程说下 pooling 层作用？ 实习项目介绍，arm 端模型如何部署的。 总结 瓜子二手车也没怎么问项目，就是手撕代码和问一些基础知识，秋招好几家公司都是这样了，不是很关心项目，比较看中手撕代码。 卡斯柯信号-C++软件开发工程师 一面(35分钟) 项目介绍。 你是如何学习机器学习、深度学习的。 指针和引用的区别？ c++ 面向对象的三个特性：封装、继承、多态。 基类和派生类析构函数有什么区别？ 讲下继承。 二面(50分钟) 做4道编程题（38分钟）。 讲笔试题。 聊家常（为什么想来上海，有女朋友吗）。 工商银行-软件开发中心: 大数据及机器学习算法工程师 一面(15分钟)(过) 核心项目介绍(围绕项目提了一些问题)。 你的本科和研究所成绩，为什么投成都岗？ 你的本科学校是什么类型的，研究所毕业设计打算做什么？ 你的博客主要写了什么内容？ 你的比赛经历介绍下。 你有什么想问的？ 面试总结 银行感觉比较看奖学金(成绩)和比赛，技术问题问的很少，很玄学的就通过面试了，后期有事回家就没去最后的笔试(提前批先面试再笔试)。 经纬恒润-无人驾驶算法工程师 一面（20分钟） 自我介绍 实习项目介绍 解决了什么问题，怎么解决的。 类别过采样方法原理。 解决数据不平衡问题有哪些方法。 期望薪资和工作地点。 手里有哪些offer。 面试总结 挂了，虽然问题都答上来了，面试估计是为了 KPI，且经纬恒润的薪资不高。 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "},"interview_summary/5-视觉算法岗2021年社招面经.html":{"url":"interview_summary/5-视觉算法岗2021年社招面经.html","title":"视觉算法岗 2021 年社招面经","keywords":"","body":" 社招面经 一，项目 二，深度学习、模型部署 2.1，目标检测相关 2.2，深度学习相关 2.3，模型部署相关 2.4，编程语言相关 三，数据结构与算法 coding 个人背景：硕士毕业1年，面试的岗位大部分是计算机视觉算法工程师，少部分算法优化、部署岗。然后这个面经是去年写的，今天突然看到了，就发出来防止丢失。 社招面经 总的来说，大部分公司的技术面试都分为这几个部分：项目描述和细节提问、深度学习+目标检测算法、数据结构和算法代码及编程语言相关。下面是我面试当中问到的一些问题。 一，项目 主要是描述项目背景、项目实现的功能及使用的方法和流程，面试官会针对他感兴趣的点问一些技术细节，基本上只要能把项目流利的描述出来就问题不大。 二，深度学习、模型部署 2.1，目标检测相关 1，两阶段检测网络（Faster RCNN 系列）和一阶段检测网络（YOLO 系列）有什么区别？以及为什么两阶段比一阶段精度高？ 双阶段网络算法更精细，把任务分成了正负样本分类、bbox 初次回归以及类别分类和 bbox 二次回归。 而 YOLO 算法更简单粗暴，使用 backbone 对输入图像提取特征后，将特征图划分成 $S\\times S$ 的网格，物体的中心坐标落在哪个网络内，该网格（grid）就负责预测目标的置信度、类别和 bbox；YOLOv2-v5 通过 $1 \\times 1$ 卷积输出特定通道数的特征图来，特征图有 N 个通道，对应的每个 grid 都会有 N 个值，分别对应置信度、类别和 bbox 坐标。 个人感觉这种问题不好回答，也没有标准答案，可能会出现你答的点不是面试官想要的。 可参考 你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (上) 和 一文读懂Faster RCNN 文章，理解典型的双阶段检测网络和单阶段检测网络。 2，说说你对 Focal Loss 的理解，为什么能解决分类问题中的类别不平衡问题？ 作者认为一阶段检测网络的精度不高的原因主要在于：极度不平衡的正负样本比例，从而导致梯度（gradient）被容易样本（easy example）的损失主导。 作者通过 Focal Loss 公式让置信度高（即容易样本）的样本的损失衰减的更厉害，从而降低容易样本的 Loss 权重，从而让模型在后期尽量去学习那些 hard 的样本。 3，如何在模型训练的时候判断是否过拟合，及模型过拟合问题如何解决？ 将训练数据划分为训练集和验证集，80% 用于训练集，20% 用于验证集（训练集和验证集一定不能相交）；训练的时候每隔一定 Epoch 比较验证集但指标和训练集是否一致，如果不一致，并且验证集指标变差了，即意味着过拟合。 数据增强, 增加数据多样性; 正则化策略：如 Parameter Norm Penalties (参数范数惩罚), L1, L2 正则化; 模型融合, 比如 Bagging 和其他集成方法; 添加 BN（batch normalization）层或者 dropout 层（现在基本不用）; Early Stopping (提前终止训练)。 4，如何在模型训练的时候判断是否欠拟合，及模型欠拟合问题如何解决？ underfitting 欠拟合的表现就是模型不收敛，即训练过程中验证集的指标比较差，Loss 不收敛。欠拟合的原因有很多种，这里以神经网络拟合能力不足问题给出以下参考解决方法： 寻找最优的权重初始化方案：如 He 正态分布初始化 he_normal，深度学习框架都内置了很多权重初始化方法； 使用适当的激活函数：卷积层的输出使用的激活函数一般为 ReLu，循环神经网络中的循环层使用的激活函数一般为 tanh，或者 ReLu； 选择合适的优化器和学习速率：SGD 优化器速度慢但是会达到最优. 5，描述以下 YOLOv3 算法及 YOLOv4、YOLOv5 的改进点，及为什么 CIoU Loss 比 IoU Loss 效果好？ YOLOv3 相比前代主要的改进点如下： Backbone 从 DarkNet19 升级为 DarkNet53。 添加了类似 FPN 的多尺度检测网络，解决小目标检测精度低的问题。 分类预测使用多标签进行类别分类，不再使用 softmax 函数。 每个 ground truth 对象只分配一个边界框。 6，描述下 RoI Pooling 过程和作用，以及 RoI Align 的改进点。 参考这篇文章 Understanding Region of Interest — (RoI Align and RoI Warp) 7，YOLOv3 的标签编码解码过程，以及正负样本采样策略。 和 YOLOv2 一样，YOLOv3 依然使用 K-means 聚类的方法来挑选 anchor boxes 作为边界框预测的先验框。每个边界框都会预测 $4$ 个偏移坐标 $(t_x,t_y,t_w,t_h)$。假设 $(c_x, c_y)$ 为 grid 的左上角坐标，$p_w$、$p_h$ 是先验框（anchors）的宽度与高度，那么网络预测值和边界框真实位置的关系如下所示： 假设某一层的 feature map 的大小为 $13 \\times 13$， 那么 grid cell 就有 $13 \\times 13$ 个，则第 $n$ 行第 $n$ 列的 grid cell 的坐标 $(x_x, c_y)$ 就是 $(n-1,n)$。 $$ bx = \\sigma(t_x) + c_x \\\\ b_y = \\sigma(t_y) + c_y \\\\ b_w = p{w}e^{tw} \\\\ b_h = p{h}e^{t_h} $$ 正负样本的确定： 正样本：与 GT 的 IOU 最大的框。 负样本：与 GT 的 IOU 的框。 忽略的样本：与 GT 的 IOU>0.5 但不是最大的框。 使用 $t_x$ 和 $t_y$ （而不是 $b_x$ 和 $b_y$ ）来计算损失。 8，详细讲解下 Faster RCNN 和 Mask RCNN 算法过程。 参考以下两篇文章理解 Faster RCNN 和 Mask RCNN 模型: 二阶段目标检测网络-Faster RCNN论文解读 二阶段目标检测网络-Mask RCNN网络理解 9，最新的目标检测算法有哪些？ YOLOv4-v5、Scaled YOLOv4 和 Anchor-free 的算法：CenterNet。 10，手写 Soft NMS 和 Focal Loss。 2.2，深度学习相关 1，BN 的作用及 BN 工作流程，以及训练和推理的区别？ 2，普通卷积层、分组卷积、深度可分离卷积的 FLOPs 计算公式。 3，普通卷积层、分组卷积、深度可分离卷积的 MAC 计算公式。 4，详细描述下你知道的轻量级网络：MobileNetV1、ShuffleNetv1-v2。 5，何谓正则化？ 通过给模型的代价函数（损失函数）添加被称为正则化项（regularizer）的惩罚，这称为将模型（学习函数为 $f(x; θ)$）正则化。正则化是一种思想（策略），给代价函数添加惩罚只是其中一种方法。 6，L2 正则化（权重衰减）原理，为什么它能防止模型过拟合？系数 $\\lambda $ 如何取值？ L2 正则化（权重衰减）是另外一种正则化技术，通过加入的正则项对参数数值进行衰减，得到更小的权值。当 $\\lambda$ 较大时，会使得一些权重几乎衰减到零，相当于去掉了这一项特征，类似于减少特征维度。假设待正则的网络参数为 $w$，L2 正则化为各个元素平方和的 $1/2$ 次方，其形式为： $$L2 = \\frac{1}{2}\\lambda ||w||^{2}_{2}$$ 实际使用时，一般将正则项加入目标函数，通过整体目标函数的误差反向传播，从而实现正则化影响和指导模型训练的目的。 7，L1 正则化原理，系数 $\\lambda $ 如何取值？ L1 范数: 为向量 x 各个元素绝对值之和。L1 正则化可以使权值参数稀疏，方便特征提取。 8，Pytorch 的 conv2d 函数的参数有哪些？以及模型输出大小计算公式，并解释为什么公式是这样。 9，Pytorch 的 DataLoader 原理。 10，普通卷积过程描述下。 2.3，模型部署相关 1，浮点数在计算机中的表示方式？ 2，描述下你知道的模型量化知识。 3，知识蒸馏原理，及温度系数如何取值？ 4，通用矩阵乘（GEMM）优化算法有哪些？ 二维矩阵相乘的 C++ 代码如下； vector> matrix_mul(vector> A, vector> B){ /*二维矩阵相乘函数，时间复杂度 O(n^3) */ // vector> A_T = matrix_transpose(A); assert((*A.begin()).size()==B.size()); //断言，第一个矩阵的列必须等于第二个矩阵的行 int new_rows = A.size(); int new_cols = (*B.begin()).size(); int L = B.size(); vector> C(new_rows, vector(new_cols,0)); for(int i=0; i 对这样的矩阵乘的算法优化可分为两类： 基于算法分析的方法：根据矩阵乘计算特性，从数学角度优化，典型的算法包括 Strassen 算法和 Coppersmith–Winograd 算法。 基于软件优化的方法：根据计算机存储系统的层次结构特性，选择性地调整计算顺序，主要有循环拆分向量化、内存重排等。 2.4，编程语言相关 1，虚函数原理及作用？ 2，C++ 构造函数和析构函数的初始化顺序。 3，智能指针描述下？ 4，static 关键字作用？ 5，STL 库的容器有哪些，讲下你最熟悉的一种及常用函数。 6，vector 和 数组的区别？vector 扩容在内存中是怎么操作的？ 7，引用和指针的区别？ 8，C++ 中定义 int a = 2,; int b = 2 和 Python 中定义 a = 2 b=3 有什么区别？ 9，OpenCV 读取图像返回后的矩阵在内存中是怎么保存的？ 10，内存对齐原理描述，为什么需要内存对齐？ 11，散列表的实现原理？ 12，虚拟地址和物理内存的关系？ 三，数据结构与算法 coding 1，二分查找算法 + 可运行代码。 2，白板写链表反转。 3，包含 min 函数的栈 + 可运行代码（剑指 Offer 30. 包含min函数的栈） 4，最长回文子串 + 时间复杂度 5，TOP k 问题-最小的 K 个数 + 说下你知道哪几种解法，及各自时间复杂度 6，返回转置后的矩阵（逆时针） 7，冒泡排序及优化 8，求数组中比左边元素都大同时比右边元素都小的元素，返回这些元素的索引 9，手写快速排序 10，手写 softmax 算子 + 解释代码及衍生问题 12，无重复字符的最长子串 13，N 皇后问题 14，求最大的第 k 个数 Copyright © zhg5200211@outlook.com 2022 all right reserved，powered by Gitbook该文章修订时间： 2022-12-03 05:38:08 "}}